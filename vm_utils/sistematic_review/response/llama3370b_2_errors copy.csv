"Document Title","Authors","Publication Year","Title: Skin","Abstract: Skin","First Removal","Abstract: Traditional ML","Second Removal","Duplication Status","Third Removal","Accepted or not","Proposed Model","Tasks (Objectives)","Used Databases"," Proposed Methodology"," Evaluation Metrics and Results","Abstract","DOI","Author Keywords","Article Citation Count","PDF Link"
"A review on smartphone skin cancer diagnosis apps in evaluation and benchmarking: coherent taxonomy, open issues and recommendation pathway solution","Zaidan A.A.; Zaidan B.B.; Albahri O.S.; Alsalem M.A.; Albahri A.S.; Yas Q.M.; Hashim M.","2018","1","1","0","0","0","First occurrence","0","","","","","","","This research aims to review the attempts of researchers in response to the new and disruptive technology of skin cancer applications in terms of evaluation and benchmarking, in order to identify the research landscape from the literature into a cohesive taxonomy. An extensive search was conducted for articles dealing with ‘skin cancer’, ‘apps’ and ‘smartphone’ or ‘mHealth’ in different variations to find all the relevant articles in three main databases, namely, “Web of Science”, “Science Direct”, and “IEEE explore”. These databases are considered wide enough to cover medical and technical literature. The final classification scheme outcome of the dataset contained 110 articles that were classified into four classes: development and design; analytical; evaluative and comparative; and review and survey studies. Afterwards, another filtering process was achieved based on the evaluation criteria error rate within the dataset, time complicity and reliability, which are used in skin cancer applications. The final classification scheme outcome of the dataset contained 89 articles distributed in mapping and crossover with four sections concluded from 110 articles. Development and design studies, analytical studies, evaluative and comparative studies and articles of reviews and surveys comprised of 48.3146%, 22.4719%, 16.8539% (15), and 12.3595% (11) of the reviewed articles, respectively. The basic features of this evolving approach were identified in these aspects. We also determined open issues in terms of evaluation and benchmarking that hamper the utility of this technology. Furthermore, with the exception of the 89 papers reviewed, the new recommendation pathway solution was described in order to improve the measurement process for smartphone-based skin cancer diagnosis applications. © 2018, IUPESM and Springer-Verlag GmbH Germany, part of Springer Nature.","10.1007/s12553-018-0223-9","Mobile health; Real-time apps; Skin cancer diagnosis, evaluation and benchmarking, smartphone","82","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052206797&doi=10.1007%2fs12553-018-0223-9&partnerID=40&md5=2ae673fb60ac157328673d6d0cb31ad1"
"Generalizability vs. robustness: Investigating medical imaging networks using adversarial examples","Paschali M.; Conjeti S.; Navarro F.; Navab N.","2018","0","1","0","0","0","Unique","0","","","","","","","In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness. © Springer Nature Switzerland AG 2018.","10.1007/978-3-030-00928-1_56","","96","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054088746&doi=10.1007%2f978-3-030-00928-1_56&partnerID=40&md5=3c70e668d961ec47185a600f36b674b1"
"Comprehensive insights into evaluation and benchmarking of real-time skin detectors: Review, open issues & challenges, and recommended solutions","Yas Q.M.; Zaidan A.A.; Zaidan B.B.; Rahmatullah B.; Abdul Karim H.","2018","1","1","0","0","0","Unique","0","","","","","","","Evaluation and benchmarking of real-time skin detectors remain challenging because of multiple evaluation attributes that must be considered. Numerous evaluation and benchmarking techniques have been proposed, but they exhibit several limitations. Fixing multiple attributes based on benchmarking approaches by using other attributes limits reliable real-time skin detection. This paper presents comprehensive insights into the evaluation and benchmarking of real-time skin detectors on the basis of two critical directions. Current evaluation criteria highlight conflicting issues and benchmarking techniques to identify weak points, and possible solutions are discussed. The findings are as follows: (1) open issues and challenges to evaluation and benchmarking are emphasized; and (2) decision making using multiple criteria such as reliability, time complexity, and error rate within a dataset is used for evaluating and benchmarking real-time skin detectors to come up with solutions for future directions. © 2017 Elsevier Ltd","10.1016/j.measurement.2017.09.027","Evaluation and benchmarking; Multi-criteria analysis; Multi-criterion decision making; Skin detector","64","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030544821&doi=10.1016%2fj.measurement.2017.09.027&partnerID=40&md5=2b31ff96b3f340c71219cdc6eaf02b07"
"Technique standards for skin lesion imaging a delphi consensus statement","Katragadda C.; Finnane A.; Soyer H.P.; Marghoob A.A.; Halpern A.; Malvehy J.; Kittler H.; Hofmann-Wellenhof R.; Da Silva D.; Abraham I.; Curiel-Lewandrowski C.","2017","1","1","0","0","0","Unique","0","","","","","","","IMPORTANCE Variability in the metrics for image acquisition at the total body, regional, close-up, and dermoscopic levels impacts the quality and generalizability of skin images. Consensus guidelines are indicated to achieve universal imaging standards in dermatology. OBJECTIVE To achieve consensus among members of the International Skin Imaging Collaboration (ISIC) on standards for image acquisition metrics using a hybrid Delphi method. EVIDENCE REVIEW Delphi study with 5 rounds of ratings and revisions until relative consensus was achieved. The initial set of statements was developed by a core group (CG) on the basis of a literature review and clinical experience followed by 2 rounds of rating and revisions. The consensus process was validated by an extended group (EG) of ISIC members through 2 rounds of scoring and revisions. In all rounds, respondents rated the draft recommendations on a 1 (strongly agree) to 5 (strongly disagree) scale, explained ratings of less than 5, and optionally provided comments. At any stage, a recommendation was retained if both mean and median rating was 4 or higher. RESULTS The initial set of 45 items (round 1) was expanded by the CG to 56 variants in round 2, subsequently reduced to 42 items scored by the EG in round 3, yielding an EG set of 33 recommendations (rounds 4 and 5): General recommendation (1 guideline), lighting (5), background color (3), field of view (3), image orientation (8), focus/depth of field (3), resolution (4), scale (3), color calibration (2), and image storage (1). CONCLUSIONS AND RELEVANCE This iterative process of ratings and comments yielded a strong consensus on standards for skin imaging in dermatology practice. Adoption of these methods for image standardization is likely to improve clinical practice, information exchange, electronic health record documentation, harmonization of clinical studies and database development, and clinical decision support. Feasibility and validity testing under real-world clinical conditions is indicated. © 2017 American Medical Association.","10.1001/jamadermatol.2016.3949","","40","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014680612&doi=10.1001%2fjamadermatol.2016.3949&partnerID=40&md5=1fbab5a602bb93e37cc24372d256a84a"
"Radiomics improves cancer screening and early detection","Gillies R.J.; Schabath M.B.","2020","0","1","0","0","0","Unique","0","","","","","","","Imaging is a key technology in the early detection of cancers, including X-ray mammography, low-dose CT for lung cancer, or optical imaging for skin, esophageal, or colorectal cancers. Historically, imaging information in early detection schema was assessed qualitatively. However, the last decade has seen increased development of computerized tools that convert images into quantitative mineable data (radiomics), and their subsequent analyses with artificial intelligence (AI). These tools are improving diagnostic accuracy of early lesions to define risk and classify malignant/ aggressive from benign/indolent disease. The first section of this review will briefly describe the various imaging modalities and their use as primary or secondary screens in an early detection pipeline. The second section will describe specific use cases to illustrate the breadth of imaging modalities as well as the benefits of quantitative image analytics. These will include optical (skin cancer), X-ray CT (pancreatic and lung cancer), X-ray mammography (breast cancer), multiparametric MRI (breast and prostate cancer), PET (pancreatic cancer), and ultrasound elastography (liver cancer). Finally, we will discuss the inexorable improvements in radiomics to build more robust classifier models and the significant limitations to this development, including access to well-annotated databases, and biological descriptors of the imaged feature data. © 2020 American Association for Cancer Research.","10.1158/1055-9965.EPI-20-0075","","113","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098796016&doi=10.1158%2f1055-9965.EPI-20-0075&partnerID=40&md5=e6f6418aa8b70876c6bfe35d92091053"
"Machine Learning in Melanoma Diagnosis. Limitations About to be Overcome; [Uso del aprendizaje automático en el diagnóstico del melanoma. Limitaciones por superar]","González-Cruz C.; Jofre M.A.; Podlipnik S.; Combalia M.; Gareau D.; Gamboa M.; Vallone M.G.; Faride Barragán-Estudillo Z.; Tamez-Peña A.L.; Montoya J.; América Jesús-Silva M.; Carrera C.; Malvehy J.; Puig S.","2020","0","1","0","0","0","Unique","0","","","","","","","Background: Automated image classification is a promising branch of machine learning (ML) useful for skin cancer diagnosis, but little has been determined about its limitations for general usability in current clinical practice. Objective: To determine limitations in the selection of skin cancer images for ML analysis, particularly in melanoma. Methods: Retrospective cohort study design, including 2,849 consecutive high-quality dermoscopy images of skin tumors from 2010 to 2014, for evaluation by a ML system. Each dermoscopy image was assorted according to its eligibility for ML analysis. Results: Of the 2,849 images chosen from our database, 968 (34%) met the inclusion criteria for analysis by the ML system. Only 64.7% of nevi and 36.6% of melanoma met the inclusion criteria. Of the 528 melanomas, 335 (63.4%) were excluded. An absence of normal surrounding skin (40.5% of all melanomas from our database) and absence of pigmentation (14.2%) were the most common reasons for exclusion from ML analysis. Discussion: Only 36.6% of our melanomas were admissible for analysis by state-of-the-art ML systems. We conclude that future ML systems should be trained on larger datasets which include relevant non-ideal images from lesions evaluated in real clinical practice. Fortunately, many of these limitations are being overcome by the scientific community as recent works show. © 2020 AEDV","10.1016/j.ad.2019.09.002","Artificial Intelligence; Convolutional neural networks; Dermoscopy; Image classification; Machine learning; Melanoma; Skin cancer","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082808971&doi=10.1016%2fj.ad.2019.09.002&partnerID=40&md5=7bbb4a5e8546a23d9be34598da7f38ce"
"Using deep learning for dermatologist-level detection of suspicious pigmented skin lesions from wide-field images","Soenksen L.R.; Kassis T.; Conover S.T.; Marti-Fuster B.; Birkenfeld J.S.; Tucker-Schwartz J.; Naseem A.; Stavert R.R.; Kim C.C.; Senna M.M.; Avilés-Izquierdo J.; Collins J.J.; Barzilay R.; Gray M.L.","2021","1","1","0","0","0","Unique","0","","","","","","","A reported 96,480 people were diagnosed with melanoma in the United States in 2019, leading to 7230 reported deaths. Early-stage identification of suspicious pigmented lesions (SPLs) in primary care settings can lead to improved melanoma prognosis and a possible 20-fold reduction in treatment cost. Despite this clinical and economic value, efficient tools for SPL detection are mostly absent. To bridge this gap, we developed an SPL analysis system for wide-field images using deep convolutional neural networks (DCNNs) and applied it to a 38,283 dermatological dataset collected from 133 patients and publicly available images. These images were obtained from a variety of consumer-grade cameras (15,244 nondermoscopy) and classified by three board-certified dermatologists. Our system achieved more than 90.3% sensitivity (95% confidence interval, 90 to 90.6) and 89.9% specificity (89.6 to 90.2%) in distinguishing SPLs from nonsuspicious lesions, skin, and complex backgrounds, avoiding the need for cumbersome individual lesion imaging. We also present a new method to extract intrapatient lesion saliency (ugly duckling criteria) on the basis of DCNN features from detected lesions. This saliency ranking was validated against three board-certified dermatologists using a set of 135 individual wide-field images from 68 dermatological patients not included in the DCNN training set, exhibiting 82.96% (67.88 to 88.26%) agreement with at least one of the top three lesions in the dermatological consensus ranking. This method could allow for rapid and accurate assessments of pigmented lesion suspiciousness within a primary care visit and could enable improved patient triaging, utilization of resources, and earlier treatment of melanoma. Copyright © 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works","10.1126/scitranslmed.abb3652","","116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101698746&doi=10.1126%2fscitranslmed.abb3652&partnerID=40&md5=57c5efcec77b7a835206aee69138f440"
"Using feature maps to unpack the CNN ‘Black box’ theory with two medical datasets of different modality","Azam S.; Montaha S.; Fahim K.U.; Rafid A.K.M.R.H.; Mukta M.S.H.; Jonkman M.","2023","0","1","0","0","0","Unique","0","","","","","","","Convolutional neural networks (CNNs) have been established for a comprehensive range of computer vision problems across several benchmarks. Visualization and analysis of feature maps generated by convolutional layers can be an effective approach to explore the hidden and complex characteristic of a CNN model. Convolutional layers provide diverse feature maps however, the extent of this diversity needs to be explored. This research attempts to provide five insights of the ‘Black box’ mechanism of CNNs, using skin cancer dermoscopy and lung scan computed tomography (CT) Scan datasets by statistically analyzing layer by layer (three convolutional layers) feature maps using 17 geometrical and 6 intensity-based features to determine the characteristics and level of diversity. Significance and difference of the feature maps layer by layer, black feature maps analysis, difference of the feature maps to each other and to the original image, variations among the feature maps when running the model multiple times and inter-class variation among the feature maps for different iteration are explored. Various statistical methods including T-test, analysis of variance (ANOVA), mean, median, mean squared error (MSE), peak signal to noise ratio (PSNR), structural similarity index (SSIM), root mean squared error (RMSE), dice similarity score (DSC), universal image quality index (UQI) and Spectral angle mapper (SAM) are employed. Experimental results show that for the skin cancer dermoscopy dataset, a large number of black feature maps are produced (20–60%) while the proportion of black feature maps for the CT Scan dataset is comparatively low (2–20%). This demonstrates that for different datasets, feature maps with diverse characteristics can be produced. The layer by layer differences between the feature maps is evaluated using T-tests and ANOVA for seventeen geometrical features and six intensity-based features. For both datasets across most of the geometrical features and across most of the intensity-based features a significant diversity can be observed. The difference of the feature maps to each other and to the original image is quite high, with MSE values for the dermoscopy and CT Scan datasets in the range of 1860–31,399 and 171–6089, respectively, PSNR 3–15 and 10–25, SSIM values of 0.01–0.84 and 0.3–0.81, RMSE values of 0.81–1 and 0.21–1, DSC values of 0.37–0.53 and 0.47–0.75, UQI values of 0.02–0.86 and 0.01–0.88 and SAM values of 0.12–1.53 and 0.19–1.55 for the dermoscopy and CT Scan datasets respectively. When running the model multiple times (three iterations), a notable iteration by iteration diversity is found in terms of mean, median, maximum and minimum values for most of the geometrical features. The inter-class variation among the feature maps for different iterations and layers are evaluated based on the F-value of the ANOVA test. For the dermoscopy dataset, the highest mean F-value is found for layer 1 and iteration 3 while for the CT scan dataset the highest mean F-value is found for layer 3 and iteration 3 indicating that for these feature maps the highest inter-class dissimilarity is generated. The findings of this study may aid in exploring the complex mechanism of convolutional layers, kernels and feature maps. © 2023","10.1016/j.iswa.2023.200233","ANOVA test; Black box; Convolutional neural network; Feature map analysis; Geometric feature; T-test","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159372463&doi=10.1016%2fj.iswa.2023.200233&partnerID=40&md5=0a35beaca70aecf58a9a29121d71bf13"
"Fungal Skin Disease Classification Using the Convolutional Neural Network","Nigat T.D.; Sitote T.M.; Gedefaw B.M.","2023","1","1","0","0","0","Unique","0","","","","","","","Skin is the outer cover of our body, which protects vital organs from harm. This important body part is often affected by a series of infections caused by fungus, bacteria, viruses, allergies, and dust. Millions of people suffer from skin diseases. It is one of the common causes of infection in sub-Saharan Africa. Skin disease can also be the cause of stigma and discrimination. Early and accurate diagnosis of skin disease can be vital for effective treatment. Laser and photonics-based technologies are used for the diagnosis of skin disease. These technologies are expensive and not affordable, especially for resource-limited countries like Ethiopia. Hence, image-based methods can be effective in reducing cost and time. There are previous studies on image-based diagnosis for skin disease. However, there are few scientific studies on tinea pedis and tinea corporis. In this study, the convolution neural network (CNN) has been used to classify fungal skin disease. The classification was carried out on the four most common fungal skin diseases: tinea pedis, tinea capitis, tinea corporis, and tinea unguium. The dataset consisted of a total of 407 fungal skin lesions collected from Dr. Gerbi Medium Clinic, Jimma, Ethiopia. Normalization of image size, conversion of RGB to grayscale, and balancing the intensity of the image have been carried out. Images were normalized to three sizes: 120 × 120, 150 × 150, and 224 × 224. Then, augmentation was applied. The developed model classified the four common fungal skin diseases with 93.3% accuracy. Comparisons were made with similar CNN architectures: MobileNetV2 and ResNet 50, and the proposed model was superior to both. This study may be an important addition to the very limited work on the detection of fungal skin disease. It can be used to build an automated image-based screening system for dermatology at an initial stage.  © 2023 Tsedenya Debebe Nigat et al.","10.1155/2023/6370416","","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161236442&doi=10.1155%2f2023%2f6370416&partnerID=40&md5=61ca06e7b6895c78c71d529030b152fa"
"Cutting-edge Deep Learning Solutions for Precise Identification and Definition of Skin Cancer Lesions","Mohadikar R.S.; Kotadi C.; Dhule C.","2024","1","1","0","0","0","Unique","0","","","","","","","This study uses three different types of convolutional neural networks (CNNs), Inception, VGG16, and DenseNet, to look into cutting-edge deep learning methods for accurately identifying the suggested models are trained on a large and varied dataset that includes high-resolution dermatoscopic images of different types and states of skin cancer. It is known that the Inception model makes good use of computer resources, and it can also capture features very well. Because it has a deep design, VGG16 is great at picking up small patterns and features inside skin tumors, which makes diagnosis more accurate. Using dense connection patterns, DenseNet encourages information flow and gradient transmission, which helps the network pick up on small differences in the features of lesions. The rough cross-validation and tests on separate datasets are used to see how well these models work. The comparison looks at things like sensitivity, specificity, and general accuracy to see how well each model can find and describe skin cancer spots. The paper discuss these deep learning model and found the accuracy for Inception as 92.63, the VGG16 more close to with accuracy with 84.7 and out of this three model DenseNet proven better accuracy with 93.33. The study also looks into how easy it is to understand the models, using focus maps and feature graphics to show how decisions are made. The findings show that deep learning could help improve the diagnosis of skin cancer, with each design showing its own strengths. The results add to the current discussion about using deep learning for medical picture analysis and also show how these models can be used in real-life therapeutic situations. Ultimately, this study shows how to better and more accurately find skin cancer spots, which will allow for earlier treatment and better results for patients.  © 2024 IEEE.","10.1109/ICICET59348.2024.10616331","Convolutional Neural Networks; Deep Learning; Diagnostic Accuracy; Inception; Skin Cancer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201194342&doi=10.1109%2fICICET59348.2024.10616331&partnerID=40&md5=d37a6ffab86fb2a231c9ef83084e9475"
