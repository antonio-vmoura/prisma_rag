"Document Title","Authors","Publication Year","Title: Skin","Abstract: Skin","First Removal","Abstract: Traditional ML","Second Removal","Duplication Status","Third Removal","Accepted or not","Proposed Model","Tasks (Objectives)","Used Databases"," Proposed Methodology"," Evaluation Metrics and Results","Abstract","DOI","Author Keywords","Article Citation Count","PDF Link"
"Real-time classification and detection of citrus based on improved single short multibox detecter; [基于改进SSD的柑橘实时分类检测]","Li S.; Hu D.; Gao S.; Lin J.; An X.; Zhu M.","2019","0","1","0","0","0","Unique","0","","","","","","","Manually classifying citrus based on its surface defects is tedious and time-consuming and a new real-time method is proposed in this paper based on the improved SSD deep learning model. In the testing bench of the waxing machine, 2 500 images of a variety of citrus species were taken, of which 2 000 were randomly selected as training set and 500 as testing set. Among them, the method classified 19 507 as normal, 9 097 skin defects and 4 327 mechanically damaged. Considering that traditional methods using near-infrared spectra, support vector machines, HSV and RGB color space model are inefficient to detect surface defects of citrus and can only identify one, we proposed an improved method to calculate the image using the one-stage detection model - SSD-ResNet18. The method gets the feature maps through backbone first, and then predicts the number of boundary boxes from the feature maps before determining the location and category of citrus using confidence and non-maximum suppression. This can detect a batch of citrus. In the proposed method, we used the mAP (mean average precision) as the precision index and the mean detection time as the speed index. Optimization in the proposed method was solved using the SGD (stochastic gradient descent) algorithm. The learning scheduler was based on cosine decay, enabling the learning rate to drop to 0 at the end of the training period. This ensures the lost value during the training period to continuously decline. As the model was stable at the end of the training period, it can be saved at the end of the training for further use. While the VGG16 was used as the original SSD backbone, it needs a multitude of parameters and is hence computationally inefficient. We replaced it with the ResNet18, which is approximately 100 times more efficient than the VGG16. An improved feature map was obtained from the analysis of the effective sensory field of different feature maps and the size of citrus in the map, the anchor in which was obtained using the K-means clustering algorithm from the manual label box. The suitable image resolution for the proposed model was obtained by comparing images taken at five resolutions: 512×512 pixels, 640×640 pixels, 768×768 pixels, 896×896 pixels and 1024×1024 pixels. The results showed that the accuracy of the mAP of SSD-ResNet18 was 87.89%, improving 0.34 percentage points higher than the original SSD. The average detecting time of the SSD-ResNet18 was 20.72 ms, reduced by 436.90% compared to the original SSD's 108.83 ms. The accuracy of the AP of SSD-ResNet18 was 94.72%, 85.79% and 83.17%, respectively, for detecting normal, skin lesion and mechanical damage. We compared MobileNetV3, ESPNetV2, VoVNet39 and ResNet18 as backbones and did not find significant difference between their accuracy, but ResNet18 was 10.52 ms, 16.78 ms and 36.76 ms less than MobileNetV3, ESPNetV2 and VoVNet39 in detection time, respectively. The method proposed in the paper meets the requirement on detecting speed in real-time citrus production line and can effectively classify and detect a multitude of citrus simultaneously. © 2019, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.","10.11975/j.issn.1002-6819.2019.24.036","Citrus; Deep learning; Models; Nondestructive detection; Object recognition; ResNet18; SSD; Surface defects","39","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081004295&doi=10.11975%2fj.issn.1002-6819.2019.24.036&partnerID=40&md5=cb35d8dc964078117bcb93962e3e2b63"
"Attention-guided deep convolutional neural networks for skin cancer classification","Aggarwal A.; Das N.; Sreedevi I.","2019","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is a silently killing disease which commonly goes unnoticed in its primitive stage but proves to be deadly later on. Hence, it needs to be detected and classified in the early stages itself. The advent of machine learning as well as deep learning based classification techniques has made this task possible. Deep convolutional neural networks (D-CNNs) have the ability to extract universal and dataset-specific features for the image classification task. But the classification of skin cancer images remains a challenging task due to the absence of balanced class images, difference between images of the same class, similarity between inter-class images and the inefficiency in focusing on the semantically significant areas of the image. To improve the performance of these D-CNNs, we incorporate the attention mechanism that focuses on the regions of importance in an image. In that regard, we propose an attention-guided D-CNN for classification of skin cancer. It is observed from the classification results that a model with attention boosts the accuracy of a normal D-CNN architecture by approximately 12%. Our research work contributes significantly to the field of biomedical image processing by providing a mechanism to improve performance of D-CNNs and facilitating early detection of skin cancer. © 2019 IEEE.","10.1109/IPTA.2019.8936100","Attention; Computer vision; Convolutional neural networks; Deep learning; Dermoscopy; Fine tuning; Skin cancer classification; Transfer learning","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077965756&doi=10.1109%2fIPTA.2019.8936100&partnerID=40&md5=7b0b5b14c699de63fd53e337ab6ad37f"
"Label-Free Non-linear Multimodal Optical Microscopy—Basics, Development, and Applications","Mazumder N.; Balla N.K.; Zhuo G.-Y.; Kistenev Y.V.; Kumar R.; Kao F.-J.; Brasselet S.; Nikolaev V.V.; Krivova N.A.","2019","0","1","0","0","0","Unique","0","","","","","","","Non-linear optical (NLO) microscopy has proven to be a powerful tool especially for tissue imaging with sub-cellular resolution, high penetration depth, endogenous contrast specificity, pinhole-less optical sectioning capability. In this review, we discuss label-free non-linear optical microscopes including the two-photon fluorescence (TPF), fluorescence lifetime imaging microscopy (FLIM), polarization-resolved second harmonic generation (SHG) and coherent anti-Stokes Raman scattering (CARS) techniques with various samples. The non-linear signals are generated from collagen in tissue (SHG), amylopectin from starch granules (SHG), sarcomere structure of fresh muscle (SHG), elastin in skin (TPF), nicotinamide adenine dinucleotide (NADH) in cells (TPF), and lipid droplets in cells (CARS). Again, the non-linear signals are very specific to the molecular structure of the sample and its relative orientation to the polarization of the incident light. Thus, polarization-resolved non-linear optical microscopy provides high image contrast and quantitative estimate of sample orientation. An overview of the advancements on polarization-resolved SHG microscopy including Stokes vector based polarimetry, circular dichroism, and susceptibility are also presented in this review article. The working principles and corresponding implements of above-mentioned microscopy techniques are elucidated. The potential of time-resolved TPF lifetime imaging microscopy (TP-FLIM) is explored by imaging endogenous fluorescence of NAD(P)H, a key coenzyme in cellular metabolic processes. We also discuss single laser source time-resolved multimodal CARS-FLIM microscopy using time-correlated single-photon counting (TCSPC) in combination with continuum generation from photonic crystal fiber (PCF). Using examples, we demonstrate that the multimodal NLO microscopy is a powerful tool to assess the molecular specificity with high resolution. © Copyright © 2019 Mazumder, Balla, Zhuo, Kistenev, Kumar, Kao, Brasselet, Nikolaev and Krivova.","10.3389/fphy.2019.00170","coherent anti-stokes Raman scattering; collagen; fluorescence lifetime imaging; nicotinamide adenine dinucleotide; non-linear optical microscopy; second harmonic generation; two-photon fluorescence microscopy","43","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075240495&doi=10.3389%2ffphy.2019.00170&partnerID=40&md5=845188ad80e0b066bc72d8e176058646"
"Association between Surgical Skin Markings in Dermoscopic Images and Diagnostic Performance of a Deep Learning Convolutional Neural Network for Melanoma Recognition","Winkler J.K.; Fink C.; Toberer F.; Enk A.; Deinlein T.; Hofmann-Wellenhof R.; Thomas L.; Lallas A.; Blum A.; Stolz W.; Haenssle H.A.","2019","1","1","0","0","0","Unique","0","","","","","","","Importance: Deep learning convolutional neural networks (CNNs) have shown a performance at the level of dermatologists in the diagnosis of melanoma. Accordingly, further exploring the potential limitations of CNN technology before broadly applying it is of special interest. Objective: To investigate the association between gentian violet surgical skin markings in dermoscopic images and the diagnostic performance of a CNN approved for use as a medical device in the European market. Design and Setting: A cross-sectional analysis was conducted from August 1, 2018, to November 30, 2018, using a CNN architecture trained with more than 120 000 dermoscopic images of skin neoplasms and corresponding diagnoses. The association of gentian violet skin markings in dermoscopic images with the performance of the CNN was investigated in 3 image sets of 130 melanocytic lesions each (107 benign nevi, 23 melanomas). Exposures: The same lesions were sequentially imaged with and without the application of a gentian violet surgical skin marker and then evaluated by the CNN for their probability of being a melanoma. In addition, the markings were removed by manually cropping the dermoscopic images to focus on the melanocytic lesion. Main Outcomes and Measures: Sensitivity, specificity, and area under the curve (AUC) of the receiver operating characteristic (ROC) curve for the CNN's diagnostic classification in unmarked, marked, and cropped images. Results: In all, 130 melanocytic lesions (107 benign nevi and 23 melanomas) were imaged. In unmarked lesions, the CNN achieved a sensitivity of 95.7% (95% CI, 79%-99.2%) and a specificity of 84.1% (95% CI, 76.0%-89.8%). The ROC AUC was 0.969. In marked lesions, an increase in melanoma probability scores was observed that resulted in a sensitivity of 100% (95% CI, 85.7%-100%) and a significantly reduced specificity of 45.8% (95% CI, 36.7%-55.2%, P <.001). The ROC AUC was 0.922. Cropping images led to the highest sensitivity of 100% (95% CI, 85.7%-100%), specificity of 97.2% (95% CI, 92.1%-99.0%), and ROC AUC of 0.993. Heat maps created by vanilla gradient descent backpropagation indicated that the blue markings were associated with the increased false-positive rate. Conclusions and Relevance: This study's findings suggest that skin markings significantly interfered with the CNN's correct diagnosis of nevi by increasing the melanoma probability scores and consequently the false-positive rate. A predominance of skin markings in melanoma training images may have induced the CNN's association of markings with a melanoma diagnosis. Accordingly, these findings suggest that skin markings should be avoided in dermoscopic images intended for analysis by a CNN. © 2019 American Medical Association. All rights reserved.","10.1001/jamadermatol.2019.1735","","262","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070989510&doi=10.1001%2fjamadermatol.2019.1735&partnerID=40&md5=e105d1488c35e7bcd835bf2645a3ce68"
"Superior skin cancer classification by the combination of human and artificial intelligence","Hekler A.; Utikal J.S.; Enk A.H.; Hauschild A.; Weichenthal M.; Maron R.C.; Berking C.; Haferkamp S.; Klode J.; Schadendorf D.; Schilling B.; Holland-Letz T.; Izar B.; von Kalle C.; Fröhling S.; Brinker T.J.; Schmitt L.; Peitsch W.K.; Hoffmann F.; Becker J.C.; Drusio C.; Jansen P.; Lodde G.; Sammet S.; Sondermann W.; Ugurel S.; Zader J.; Salzmann M.; Schäfer S.; Schäkel K.; Winkler J.; Wölbing P.; Asper H.; Bohne A.-S.; Brown V.; Burba B.; Deffaa S.; Dietrich C.; Dietrich M.; Drerup K.A.; Egberts F.; Erkens A.-S.; Greven S.; Harde V.; Jost M.; Kaeding M.; Kosova K.; Lischner S.; Maagk M.; Messinger A.L.; Metzner M.; Motamedi R.; Rosenthal A.-C.; Seidl U.; Stemmermann J.; Torz K.; Velez J.G.; Haiduk J.; Alter M.; Bär C.; Bergenthal P.; Gerlach A.; Holtorf C.; Karoglan A.; Kindermann S.; Kraas L.; Felcht M.; Gaiser M.R.; Klemke C.-D.; Kurzen H.; Leibing T.; Müller V.; Reinhard R.R.; Winter F.; Eicher L.; Hartmann D.; Heppt M.; Kilian K.; Krammer S.; Lill D.; Niesert A.-C.; Oppel E.; Sattler E.; Senner S.; Wallmichrath J.; Wolff H.; Gesierich A.; Giner T.; Glutsch V.; Kerstan A.; Presser D.; Schrüfer P.; Schummer P.; Stolze I.; Weber J.; Drexler K.; Mickler M.; Stauner C.T.; Thiem A.","2019","1","1","0","0","0","Unique","0","","","","","","","Background: In recent studies, convolutional neural networks (CNNs) outperformed dermatologists in distinguishing dermoscopic images of melanoma and nevi. In these studies, dermatologists and artificial intelligence were considered as opponents. However, the combination of classifiers frequently yields superior results, both in machine learning and among humans. In this study, we investigated the potential benefit of combining human and artificial intelligence for skin cancer classification. Methods: Using 11,444 dermoscopic images, which were divided into five diagnostic categories, novel deep learning techniques were used to train a single CNN. Then, both 112 dermatologists of 13 German university hospitals and the trained CNN independently classified a set of 300 biopsy-verified skin lesions into those five classes. Taking into account the certainty of the decisions, the two independently determined diagnoses were combined to a new classifier with the help of a gradient boosting method. The primary end-point of the study was the correct classification of the images into five designated categories, whereas the secondary end-point was the correct classification of lesions as either benign or malignant (binary classification). Findings: Regarding the multiclass task, the combination of man and machine achieved an accuracy of 82.95%. This was 1.36% higher than the best of the two individual classifiers (81.59% achieved by the CNN). Owing to the class imbalance in the binary problem, sensitivity, but not accuracy, was examined and demonstrated to be superior (89%) to the best individual classifier (CNN with 86.1%). The specificity in the combined classifier decreased from 89.2% to 84%. However, at an equal sensitivity of 89%, the CNN achieved a specificity of only 81.5% Interpretation: Our findings indicate that the combination of human and artificial intelligence achieves superior results over the independent results of both of these systems. © 2019 The Author(s)","10.1016/j.ejca.2019.07.019","Artificial intelligence; Deep learning; Melanoma; Skin cancer","292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069037204&doi=10.1016%2fj.ejca.2019.07.019&partnerID=40&md5=aa8bc09e2f12f5764ae39331bb09cbbf"
"Systematic outperformance of 112 dermatologists in multiclass skin cancer image classification by convolutional neural networks","Maron R.C.; Weichenthal M.; Utikal J.S.; Hekler A.; Berking C.; Hauschild A.; Enk A.H.; Haferkamp S.; Klode J.; Schadendorf D.; Jansen P.; Holland-Letz T.; Schilling B.; von Kalle C.; Fröhling S.; Gaiser M.R.; Hartmann D.; Gesierich A.; Kähler K.C.; Wehkamp U.; Karoglan A.; Bär C.; Brinker T.J.; Schmitt L.; Peitsch W.K.; Hoffmann F.; Becker J.C.; Drusio C.; Lodde G.; Sammet S.; Sondermann W.; Ugurel S.; Zader J.; Salzmann M.; Schäfer S.; Schäkel K.; Winkler J.; Wölbing P.; Asper H.; Bohne A.-S.; Brown V.; Burba B.; Deffaa S.; Dietrich C.; Dietrich M.; Drerup K.A.; Egberts F.; Erkens A.-S.; Greven S.; Harde V.; Jost M.; Kaeding M.; Kosova K.; Lischner S.; Maagk M.; Messinger A.L.; Metzner M.; Motamedi R.; Rosenthal A.-C.; Seidl U.; Stemmermann J.; Torz K.; Velez J.G.; Haiduk J.; Alter M.; Bergenthal P.; Gerlach A.; Holtorf C.; Kindermann S.; Kraas L.; Felcht M.; Klemke C.-D.; Kurzen H.; Leibing T.; Müller V.; Reinhard R.R.; Winter F.; Eicher L.; Heppt M.; Kilian K.; Krammer S.; Lill D.; Niesert A.-C.; Oppel E.; Sattler E.; Senner S.; Wallmichrath J.; Wolff H.; Giner T.; Glutsch V.; Kerstan A.; Presser D.; Schrüfer P.; Schummer P.; Stolze I.; Weber J.; Drexler K.; Mickler M.; Stauner C.T.; Thiem A.","2019","1","1","0","0","0","Unique","0","","","","","","","Background: Recently, convolutional neural networks (CNNs) systematically outperformed dermatologists in distinguishing dermoscopic melanoma and nevi images. However, such a binary classification does not reflect the clinical reality of skin cancer screenings in which multiple diagnoses need to be taken into account. Methods: Using 11,444 dermoscopic images, which covered dermatologic diagnoses comprising the majority of commonly pigmented skin lesions commonly faced in skin cancer screenings, a CNN was trained through novel deep learning techniques. A test set of 300 biopsy-verified images was used to compare the classifier's performance with that of 112 dermatologists from 13 German university hospitals. The primary end-point was the correct classification of the different lesions into benign and malignant. The secondary end-point was the correct classification of the images into one of the five diagnostic categories. Findings: Sensitivity and specificity of dermatologists for the primary end-point were 74.4% (95% confidence interval [CI]: 67.0–81.8%) and 59.8% (95% CI: 49.8–69.8%), respectively. At equal sensitivity, the algorithm achieved a specificity of 91.3% (95% CI: 85.5–97.1%). For the secondary end-point, the mean sensitivity and specificity of the dermatologists were at 56.5% (95% CI: 42.8–70.2%) and 89.2% (95% CI: 85.0–93.3%), respectively. At equal sensitivity, the algorithm achieved a specificity of 98.8%. Two-sided McNemar tests revealed significance for the primary end-point (p < 0.001). For the secondary end-point, outperformance (p < 0.001) was achieved except for basal cell carcinoma (on-par performance). Interpretation: Our findings show that automated classification of dermoscopic melanoma and nevi images is extendable to a multiclass classification problem, thus better reflecting clinical differential diagnoses, while still outperforming dermatologists at a significant level (p < 0.001). © 2019 The Author(s)","10.1016/j.ejca.2019.06.013","Artificial intelligence; Melanoma; Skin cancer; Skin cancer screening","182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069038904&doi=10.1016%2fj.ejca.2019.06.013&partnerID=40&md5=a73968ddeacb3e9da8e3bd76141493b4"
"Efficient skin lesion segmentation using separable-Unet with stochastic weight averaging","Tang P.; Liang Q.; Yan X.; Xiang S.; Sun W.; Zhang D.; Coppola G.","2019","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Efficient segmentation of skin lesion in dermoscopy images can improve the classification accuracy of skin diseases, which provides a powerful approach for the dermatologists in examining pigmented skin lesions. However, the segmentation is challenging due to the low contrast of skin lesions from a captured image, fuzzy and indistinct lesion boundaries, huge variety of interclass variation of melanomas, the existence of artifacts, etc. In this work, an efficient and accurate melanoma region segmentation method is proposed for computer-aided diagnostic systems. Method: A skin lesion segmentation (SLS) method based on the separable-Unet with stochastic weight averaging is proposed in this work. Specifically, the proposed Separable-Unet framework takes advantage of the separable convolutional block and U-Net architectures, which can extremely capture the context feature channel correlation and higher semantic feature information to enhance the pixel-level discriminative representation capability of fully convolutional networks (FCN). Further, considering that the over-fitting is a local optimum (or sub-optimum) problem, a scheme based on stochastic weight averaging is introduced, which can obtain much broader optimum and better generalization. Results: The proposed method is evaluated in three publicly available datasets. The experimental results showed that the proposed approach segmented the skin lesions with an average Dice coefficient of 93.03% and Jaccard index of 89.25% for the International Skin Imaging Collaboration (ISIC) 2016 Skin Lesion Challenge (SLC) dataset, 86.93% and 79.26% for the ISIC 2017 SLC, and 94.13% and 89.40% for the PH2 dataset, respectively. The proposed approach is compared with other state-of-the-art methods, and the results demonstrate that the proposed approach outperforms them for SLS on both melanoma and non-melanoma cases. Segmentation of a potential lesion with the proposed approach in a dermoscopy image requires less than 0.05 s of processing time, which is roughly 30 times faster than the second best method (regarding the value of Jaccard index) for the ISIC 2017 dataset with the same hardware configuration. Conclusions: We concluded that using the separable convolutional block and U-Net architectures with stochastic weight averaging strategy could enable to obtain better pixel-level discriminative representation capability. Moreover, the considerably decreased computation time suggests that the proposed approach has potential for practical computer-aided diagnose systems, besides provides a segmentation for the specific analysis with improved segmentation performance. © 2019 Elsevier B.V.","10.1016/j.cmpb.2019.07.005","Real-time segmentation; Separable convolutional block; Skin lesion segmentation; Stochastic weight averaging","160","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068764142&doi=10.1016%2fj.cmpb.2019.07.005&partnerID=40&md5=73a61d2972e6941d7a2cd0006657e017"
"Automated deep learning design for medical image classification by health-care professionals with no coding experience: a feasibility study","Faes L.; Wagner S.K.; Fu D.J.; Liu X.; Korot E.; Ledsam J.R.; Back T.; Chopra R.; Pontikos N.; Kern C.; Moraes G.; Schmid M.K.; Sim D.; Balaskas K.; Bachmann L.M.; Denniston A.K.; Keane P.A.","2019","0","1","0","0","0","Unique","0","","","","","","","Background: Deep learning has the potential to transform health care; however, substantial expertise is required to train such models. We sought to evaluate the utility of automated deep learning software to develop medical image diagnostic classifiers by health-care professionals with no coding—and no deep learning—expertise. Methods: We used five publicly available open-source datasets: retinal fundus images (MESSIDOR); optical coherence tomography (OCT) images (Guangzhou Medical University and Shiley Eye Institute, version 3); images of skin lesions (Human Against Machine [HAM] 10000), and both paediatric and adult chest x-ray (CXR) images (Guangzhou Medical University and Shiley Eye Institute, version 3 and the National Institute of Health [NIH] dataset, respectively) to separately feed into a neural architecture search framework, hosted through Google Cloud AutoML, that automatically developed a deep learning architecture to classify common diseases. Sensitivity (recall), specificity, and positive predictive value (precision) were used to evaluate the diagnostic properties of the models. The discriminative performance was assessed using the area under the precision recall curve (AUPRC). In the case of the deep learning model developed on a subset of the HAM10000 dataset, we did external validation using the Edinburgh Dermofit Library dataset. Findings: Diagnostic properties and discriminative performance from internal validations were high in the binary classification tasks (sensitivity 73·3–97·0%; specificity 67–100%; AUPRC 0·87–1·00). In the multiple classification tasks, the diagnostic properties ranged from 38% to 100% for sensitivity and from 67% to 100% for specificity. The discriminative performance in terms of AUPRC ranged from 0·57 to 1·00 in the five automated deep learning models. In an external validation using the Edinburgh Dermofit Library dataset, the automated deep learning model showed an AUPRC of 0·47, with a sensitivity of 49% and a positive predictive value of 52%. Interpretation: All models, except the automated deep learning model trained on the multilabel classification task of the NIH CXR14 dataset, showed comparable discriminative performance and diagnostic properties to state-of-the-art performing deep learning algorithms. The performance in the external validation study was low. The quality of the open-access datasets (including insufficient information about patient flow and demographics) and the absence of measurement for precision, such as confidence intervals, constituted the major limitations of this study. The availability of automated deep learning platforms provide an opportunity for the medical community to enhance their understanding in model development and evaluation. Although the derivation of classification models without requiring a deep understanding of the mathematical, statistical, and programming principles is attractive, comparable performance to expertly designed models is limited to more elementary classification tasks. Furthermore, care should be placed in adhering to ethical principles when using these automated models to avoid discrimination and causing harm. Future studies should compare several application programming interfaces on thoroughly curated datasets. Funding: National Institute for Health Research and Moorfields Eye Charity. © 2019 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license.","10.1016/S2589-7500(19)30108-6","","234","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071729503&doi=10.1016%2fS2589-7500%2819%2930108-6&partnerID=40&md5=887d3a84b6ed49a7713eb05853fda8fe"
"Cancer diagnosis using deep learning: A bibliographic review","Munir K.; Elahi H.; Ayub A.; Frezza F.; Rizzi A.","2019","0","1","0","0","0","Unique","0","","","","","","","In this paper, we first describe the basics of the field of cancer diagnosis, which includes steps of cancer diagnosis followed by the typical classification methods used by doctors, providing a historical idea of cancer classification techniques to the readers. These methods include Asymmetry, Border, Color and Diameter (ABCD) method, seven-point detection method, Menzies method, and pattern analysis. They are used regularly by doctors for cancer diagnosis, although they are not considered very efficient for obtaining better performance. Moreover, considering all types of audience, the basic evaluation criteria are also discussed. The criteria include the receiver operating characteristic curve (ROC curve), Area under the ROC curve (AUC), F1 score, accuracy, specificity, sensitivity, precision, dice-coefficient, average accuracy, and Jaccard index. Previously used methods are considered inefficient, asking for better and smarter methods for cancer diagnosis. Artificial intelligence and cancer diagnosis are gaining attention as a way to define better diagnostic tools. In particular, deep neural networks can be successfully used for intelligent image analysis. The basic framework of how this machine learning works on medical imaging is provided in this study, i.e., pre-processing, image segmentation and post-processing. The second part of this manuscript describes the different deep learning techniques, such as convolutional neural networks (CNNs), generative adversarial models (GANs), deep autoencoders (DANs), restricted Boltzmann’s machine (RBM), stacked autoencoders (SAE), convolutional autoencoders (CAE), recurrent neural networks (RNNs), long short-term memory (LTSM), multi-scale convolutional neural network (M-CNN), multi-instance learning convolutional neural network (MIL-CNN). For each technique, we provide Python codes, to allow interested readers to experiment with the cited algorithms on their own diagnostic problems. The third part of this manuscript compiles the successfully applied deep learning models for different types of cancers. Considering the length of the manuscript, we restrict ourselves to the discussion of breast cancer, lung cancer, brain cancer, and skin cancer. The purpose of this bibliographic review is to provide researchers opting to work in implementing deep learning and artificial neural networks for cancer diagnosis a knowledge from scratch of the state-of-the-art achievements. © 2019 by the authors.","10.3390/cancers11091235","Convolutional neural networks (CNNs); Deep autoencoders (DANs); Deep learning; Generative adversarial models (GANs); Long short-term memory (LTSM); Recurrent neural networks (RNNs); Restricted Boltzmann’s machine (RBM)","338","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071841965&doi=10.3390%2fcancers11091235&partnerID=40&md5=5c9ef6e22291320690a9e58b6ae923a2"
"Melanoma lesion detection and segmentation using deep region based convolutional neural network and fuzzy C-means clustering","Nida N.; Irtaza A.; Javed A.; Yousaf M.H.; Mahmood M.T.","2019","0","1","0","0","0","Unique","0","","","","","","","Objective: Melanoma is a dangerous form of the skin cancer responsible for thousands of deaths every year. Early detection of melanoma is possible through visual inspection of pigmented lesions over the skin, treated with simple excision of the cancerous cells. However, due to the limited availability of dermatologists, the visual inspection alone has the limited and variable accuracy that leads the patient to undergo a series of biopsies and complicates the treatment. In this work, a deep learning method is proposed for automated Melanoma region segmentation using dermoscopic images to overcome the challenges of automated Melanoma region segmentation within dermoscopic images. Materials and methods: A deep region based convolutional neural network (RCNN) precisely detects the multiple affected regions in the form of bounding boxes that simplify localization through Fuzzy C-mean (FCM) clustering. Our method constitutes of three step process: skin refinement, localization of Melanoma region, and finally segmentation of Melanoma. We applied the proposed method on benchmark dataset ISIC-2016 by International Symposium on biomedical images (ISBI) having 900 training and 376 testing Melanoma dermatological images. Main findings: The performance is evaluated for Melanoma segmentation using various quantitative measures. Our method achieved average values of pixel level specificity (SP) as 0.9417, pixel level sensitivity (SE) as 0.9781, F1 _ s core as 0.9589, pixel level accuracy (Ac) as 0.948. In addition, average dice score (Di) of segmentation was recorded as 0.94, which represents good segmentation performance. Moreover, Jaccard coefficient (Jc) averaged value on entire testing images was 0.93. Comparative analysis with the state of art methods and the results have demonstrated the superiority of the proposed method. Conclusion: In contrast with state of the art systems, the RCNN is capable to compute deep features with amen representation of Melanoma, and hence improves the segmentation performance. The RCNN can detect features for multiple skin diseases of the same patient as well as various diseases of different patients with efficient training mechanism. Series of experiments towards Melanoma detection and segmentation validates the effectiveness of our method. © 2019 Elsevier B.V.","10.1016/j.ijmedinf.2019.01.005","CAD tool; Fuzzy C-Means; Melanoma segmentation; RCNN; Region proposal","182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060342436&doi=10.1016%2fj.ijmedinf.2019.01.005&partnerID=40&md5=c9b53545517b14eee55108dfd539fc98"
"Bio-inspired deep-CNN pipeline for skin cancer early diagnosis","Rundo F.; Banna G.L.; Conoci S.","2019","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is the most common type of cancer, as also among the riskiest in the medical oncology field. Skin cancer is more common in people who work or practice outdoor sports and those that expose themselves to the sun. It may also develop years after radiographic therapy or exposure to substances that cause cancer (e.g., arsenic ingestion). Numerous tumors can affect the skin, which is the largest organ in our body and is made up of three layers: the epidermis (superficial layer), the dermis (middle layer) and the subcutaneous tissue (deep layer). The epidermis is formed by different types of cells: melanocytes, which have the task of producing melanin (a pigment that protects against the damaging effects of sunlight), and the more numerous keratinocytes. The keratinocytes of the deepest layer are called basal cells and can give rise to basal cell carcinomas. We are interested in types of skin cancer that originate from melanocytes, i.e., the so-called melanomas, because it is the most aggressive. The dermatologist, during a complete visit, evaluates the personal and family history of the patient and carries out an accurate visual examination of the skin, thanks to the use of epi-luminescence (or dermoscopy), a special technique for enlarging and illuminating the skin. This paper mentions one of the most widely used diagnostic methods due to its simplicity and validity-the ABCDE method (Asymmetry, edge irregularity, Color Variegation, Diameter, Evolution). This methodology, based on ""visual"" investigation by the dermatologist and/or oncologist, has the advantage of not being invasive and quite easy to perform. This approach is affected by the opinion of who (physicians) applies it. For this reason, certain diagnosis of cancer is made, however, only with a biopsy, a procedure during which a portion of tissue is taken and then analyzed under a microscope. Obviously, this is particularly invasive for the patient. The authors of this article have analyzed the development of a method that obtains with good accuracy the early diagnosis of skin neoplasms using non-invasive, but at the same time, robust methodologies. To this end, the authors propose the adoption of a deep learning pipeline based on morphological analysis of the skin lesion. The results obtained and compared with previous approaches confirm the good performance of the proposed pipeline. © 2019 by the authors.","10.3390/computation7030044","Deep learning; Melanoma; Skin cancer; STM32","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072859885&doi=10.3390%2fcomputation7030044&partnerID=40&md5=7eb2e2e0309bbc8222872c045a955ae7"
"Analysis of circulating soluble programmed death 1 (PD-1), neuropilin 1 (NRP-1) and human leukocyte antigen-G (HLA-G) in psoriatic patients","Bartosińska J.; Michalak-Stoma A.; Kowal M.; Raczkiewicz D.; Krasowska D.; Chodorowska G.; Giannopoulos K.","2019","0","1","0","0","0","Unique","0","","","","","","","Introduction: Circulating soluble programmed death 1 (PD-1), neuropilin 1 (NRP-1) and human leukocyte antigen-G (HLA-G) take part in modulating immune tolerance causing disturbances in the molecular mechanisms responsible for maintenance of balance between effector and regulatory components of the immune system. Since their cell-surface expression levels were found to be changed in lesional and/or non-lesional skin of psoriatic patients, analysis of soluble PD-1, NRP-1 and HLA-G concentrations sheds more light on their role in detecting unbalanced immune tolerance in psoriasis. Aim: To assess soluble PD-1, NRP-1 and HLA-G concentrations in psoriasis. Material and methods: The study included 57 psoriatic patients and 29 controls. Duration of psoriasis was in the range 1 to 55 years; the median was 19 years. The plasma concentrations of soluble HLA-G (sHLA-G), soluble NRP-1 (sNRP-1) and soluble PD-1 (sPD-1) were examined using the ELISA method. Severity of the skin lesions was assessed by means of Psoriasis Area Severity Index (PASI), body surface area (BSA) and Physician Global Assessment (PGA). Results: Psoriasis Area Severity Index in the studied group was in the range 3 to 43; the median was 12. Body surface area was in the range 2–75%; the median was 15%. The median value of PGA was 3. Soluble NRP concentration was significantly higher in the psoriatic patients (median: 1.59 pg/ml; range: 0.67–2.62 pg/ml) than in the control group (median: 1.35 pg/ml; range: 0.05–2.61 pg/ml) (p = 0.010). Soluble PD-1 and sHLA-G concentrations were not significantly different between the studied and control groups (p = 0.094 and p = 0.482, respectively). Conclusions: Increased concentrations of sNRP-1 and unchanged values of sHLA-G and sPD-1 concentrations may be indicative of impaired immune tolerance mechanisms in psoriasis. © 2019 Termedia Publishing House Ltd.. All rights reserved.","10.5114/ada.2018.73329","Human leukocyte antigen-G; Immune tolerance; Neuropilin 1; Programmed death 1; Psoriasis","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066090033&doi=10.5114%2fada.2018.73329&partnerID=40&md5=8973d8521e05ce40ef8effa29ddb1548"
"Whole-section tumor micro-architecture analysis by a two-dimensional phasor-based approach applied to polarization-dependent second harmonic imaging","Scodellaro R.; Bouzin M.; Mingozzi F.; D'Alfonso L.; Granucci F.; Collini M.; Chirico G.; Sironi L.","2019","0","1","0","0","0","Unique","0","","","","","","","Second Harmonic Generation (SHG) microscopy has gained much interest in the histopathology field since it allows label-free imaging of tissues simultaneously providing information on their morphology and on the collagen microarchitecture, thereby highlighting the onset of pathologies and diseases. A wide request of image analysis tools is growing, with the aim to increase the reliability of the analysis of the huge amount of acquired data and to assist pathologists in a user-independent way during their diagnosis. In this light, we exploit here a set of phasor-parameters that, coupled to a 2-dimensional phasor-based approach (μMAPPS, Microscopic Multiparametric Analysis by Phasor projection of Polarization-dependent SHG signal) and a clustering algorithm, allow to automatically recover different collagen microarchitectures in the tissues extracellular matrix. The collagen fibrils microscopic parameters (orientation and anisotropy) are analyzed at a mesoscopic level by quantifying their local spatial heterogeneity in histopathology sections (few mm in size) from two cancer xenografts in mice, in order to maximally discriminate different collagen organizations, allowing in this case to identify the tumor area with respect to the surrounding skin tissue. We show that the “fibril entropy” parameter, which describes the tissue order on a selected spatial scale, is the most effective in enlightening the tumor edges, opening the possibility of their automatic segmentation. Our method, therefore, combined with tissue morphology information, has the potential to become a support to standard histopathology in diseases diagnosis. Copyright © 2019 Scodellaro, Bouzin, Mingozzi, D'Alfonso, Granucci, Collini, Chirico and Sironi. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.","10.3389/fonc.2019.00527","Cancer; Collagen; Label-free imaging; Phasor approach; Second harmonic generation; Two-photon microscopy","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068999268&doi=10.3389%2ffonc.2019.00527&partnerID=40&md5=9ce1a76a4dd0f87334809aa5692fb442"
"Computer-assisted diagnosis techniques (dermoscopy and spectroscopy-based) for diagnosing skin cancer in adults","Ferrante di Ruffano L.; Takwoingi Y.; Dinnes J.; Chuchu N.; Bayliss S.E.; Davenport C.; Matin R.N.; Godfrey K.; O'sullivan C.; Gulati A.; Chan S.A.; Durack A.; O'connell S.; Gardiner M.D.; Bamber J.; Deeks J.J.; Williams H.C.","2018","1","1","0","0","0","Unique","0","","","","","","","Background: Early accurate detection of all skin cancer types is essential to guide appropriate management and to improve morbidity and survival. Melanoma and cutaneous squamous cell carcinoma (cSCC) are high-risk skin cancers which have the potential to metastasise and ultimately lead to death, whereas basal cell carcinoma (BCC) is usually localised with potential to infiltrate and damage surrounding tissue. Anxiety around missing early curable cases needs to be balanced against inappropriate referral and unnecessary excision of benign lesions. Computer-assisted diagnosis (CAD) systems use artificial intelligence to analyse lesion data and arrive at a diagnosis of skin cancer. When used in unreferred settings ('primary care'), CAD may assist general practitioners (GPs) or other clinicians to more appropriately triage high-risk lesions to secondary care. Used alongside clinical and dermoscopic suspicion of malignancy, CAD may reduce unnecessary excisions without missing melanoma cases. Objectives: To determine the accuracy of CAD systems for diagnosing cutaneous invasive melanoma and atypical intraepidermal melanocytic variants, BCC or cSCC in adults, and to compare its accuracy with that of dermoscopy. Search methods: We undertook a comprehensive search of the following databases from inception up to August 2016: Cochrane Central Register of Controlled Trials (CENTRAL); MEDLINE; Embase; CINAHL; CPCI; Zetoc; Science Citation Index; US National Institutes of Health Ongoing Trials Register; NIHR Clinical Research Network Portfolio Database; and the World Health Organization International Clinical Trials Registry Platform. We studied reference lists and published systematic review articles. Selection criteria: Studies of any design that evaluated CAD alone, or in comparison with dermoscopy, in adults with lesions suspicious for melanoma or BCC or cSCC, and compared with a reference standard of either histological confirmation or clinical follow-up. Data collection and analysis: Two review authors independently extracted all data using a standardised data extraction and quality assessment form (based on QUADAS-2). We contacted authors of included studies where information related to the target condition or diagnostic threshold were missing. We estimated summary sensitivities and specificities separately by type of CAD system, using the bivariate hierarchical model. We compared CAD with dermoscopy using (a) all available CAD data (indirect comparisons), and (b) studies providing paired data for both tests (direct comparisons). We tested the contribution of human decision-making to the accuracy of CAD diagnoses in a sensitivity analysis by removing studies that gave CAD results to clinicians to guide diagnostic decision-making. Main results: We included 42 studies, 24 evaluating digital dermoscopy-based CAD systems (Derm-CAD) in 23 study cohorts with 9602 lesions (1220 melanomas, at least 83 BCCs, 9 cSCCs), providing 32 datasets for Derm-CAD and seven for dermoscopy. Eighteen studies evaluated spectroscopy-based CAD (Spectro-CAD) in 16 study cohorts with 6336 lesions (934 melanomas, 163 BCC, 49 cSCCs), providing 32 datasets for Spectro-CAD and six for dermoscopy. These consisted of 15 studies using multispectral imaging (MSI), two studies using electrical impedance spectroscopy (EIS) and one study using diffuse-reflectance spectroscopy. Studies were incompletely reported and at unclear to high risk of bias across all domains. Included studies inadequately address the review question, due to an abundance of low-quality studies, poor reporting, and recruitment of highly selected groups of participants. Across all CAD systems, we found considerable variation in the hardware and software technologies used, the types of classification algorithm employed, methods used to train the algorithms, and which lesion morphological features were extracted and analysed across all CAD systems, and even between studies evaluating CAD systems. Meta-analysis found CAD systems had high sensitivity for correct identification of cutaneous invasive melanoma and atypical intraepidermal melanocytic variants in highly selected populations, but with low and very variable specificity, particularly for Spectro-CAD systems. Pooled data from 22 studies estimated the sensitivity of Derm-CAD for the detection of melanoma as 90.1% (95% confidence interval (CI) 84.0% to 94.0%) and specificity as 74.3% (95% CI 63.6% to 82.7%). Pooled data from eight studies estimated the sensitivity of multispectral imaging CAD (MSI-CAD) as 92.9% (95% CI 83.7% to 97.1%) and specificity as 43.6% (95% CI 24.8% to 64.5%). When applied to a hypothetical population of 1000 lesions at the mean observed melanoma prevalence of 20%, Derm-CAD would miss 20 melanomas and would lead to 206 false-positive results for melanoma. MSI-CAD would miss 14 melanomas and would lead to 451 false diagnoses for melanoma. Preliminary findings suggest CAD systems are at least as sensitive as assessment of dermoscopic images for the diagnosis of invasive melanoma and atypical intraepidermal melanocytic variants. We are unable to make summary statements about the use of CAD in unreferred populations, or its accuracy in detecting keratinocyte cancers, or its use in any setting as a diagnostic aid, because of the paucity of studies. Authors' conclusions: In highly selected patient populations all CAD types demonstrate high sensitivity, and could prove useful as a back-up for specialist diagnosis to assist in minimising the risk of missing melanomas. However, the evidence base is currently too poor to understand whether CAD system outputs translate to different clinical decision-making in practice. Insufficient data are available on the use of CAD in community settings, or for the detection of keratinocyte cancers. The evidence base for individual systems is too limited to draw conclusions on which might be preferred for practice. Prospective comparative studies are required that evaluate the use of already evaluated CAD systems as diagnostic aids, by comparison to face-to-face dermoscopy, and in participant populations that are representative of those in which the test would be used in practice. © 2018 The Cochrane Collaboration. Published by John Wiley & Sons, Ltd.","10.1002/14651858.CD013186","","118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058005041&doi=10.1002%2f14651858.CD013186&partnerID=40&md5=fbdbe7d2004711b7dba234f8a58187e1"
"Skin lesion segmentation in dermoscopy images via deep full resolution convolutional networks","Al-masni M.A.; Al-antari M.A.; Choi M.-T.; Han S.-M.; Kim T.-S.","2018","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Automatic segmentation of skin lesions in dermoscopy images is still a challenging task due to the large shape variations and indistinct boundaries of the lesions. Accurate segmentation of skin lesions is a key prerequisite step for any computer-aided diagnostic system to recognize skin melanoma. Methods: In this paper, we propose a novel segmentation methodology via full resolution convolutional networks (FrCN). The proposed FrCN method directly learns the full resolution features of each individual pixel of the input data without the need for pre- or post-processing operations such as artifact removal, low contrast adjustment, or further enhancement of the segmented skin lesion boundaries. We evaluated the proposed method using two publicly available databases, the IEEE International Symposium on Biomedical Imaging (ISBI) 2017 Challenge and PH2 datasets. To evaluate the proposed method, we compared the segmentation performance with the latest deep learning segmentation approaches such as the fully convolutional network (FCN), U-Net, and SegNet. Results: Our results showed that the proposed FrCN method segmented the skin lesions with an average Jaccard index of 77.11% and an overall segmentation accuracy of 94.03% for the ISBI 2017 test dataset and 84.79% and 95.08%, respectively, for the PH2 dataset. In comparison to FCN, U-Net, and SegNet, the proposed FrCN outperformed them by 4.94%, 15.47%, and 7.48% for the Jaccard index and 1.31%, 3.89%, and 2.27% for the segmentation accuracy, respectively. Furthermore, the proposed FrCN achieved a segmentation accuracy of 95.62% for some representative clinical benign cases, 90.78% for the melanoma cases, and 91.29% for the seborrheic keratosis cases in the ISBI 2017 test dataset, exhibiting better performance than those of FCN, U-Net, and SegNet. Conclusions: We conclude that using the full spatial resolutions of the input image could enable to learn better specific and prominent features, leading to an improvement in the segmentation performance. © 2018 Elsevier B.V.","10.1016/j.cmpb.2018.05.027","Deep learning; Dermoscopy; Full resolution convolutional network (FrCN); Melanoma; Skin lesion segmentation","404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047600387&doi=10.1016%2fj.cmpb.2018.05.027&partnerID=40&md5=18812c13cc0b21ec98c5062d3b4aad2c"
"Experimental factors affecting the within- and between-individual variation of plantar foot surface temperatures in turkeys (Meleagris gallopovo) recorded with infrared thermography","Moe R.O.; Flø A.; Bohlin J.; Vasdal G.; Erlandsen H.; Guneriussen E.; Sjökvist E.C.; Stubsjøen S.M.","2018","0","1","0","0","0","Unique","0","","","","","","","Footpad dermatitis is a welfare concern in turkeys kept for meat production. In order to develop the basis for future standardized infrared thermography (IRT) protocols to screen for impaired foot health, this study investigated within- and between-individual temperature variation in two plantar sub-regions (Footpad, and the whole plantar Foot surface), and effects of cleaning procedures, in 80 turkey toms. A thermal camera (FLIR System AB) was used to collect IRT images. Feet were cleaned with water and dried with a paper towel. The minimum and maximum temperature (Tempmin and Tempmax) of Footpad and Foot in dirty and cleaned feet were determined. Sources of variation related to anatomical region, cleaning procedure and image analysis method were identified. Tempmax Foot was significantly higher than Tempmax Footpad both before (4.8 °C 95%CI (4.36, 5.19), t = 22.9, p < 0.001) and after cleaning (3.5 °C 95%CI (2.96, 4.04), t = 12.9, p < 0.001). Furthermore, Tempmax Foot (3.92 °C 95%CI (3.54, 4.3), t = 20.6, p < 0.001) and Tempmax Footpad (2.64 °C 95%CI (2.08, 3.2), t = 9.3, p < 0.001) were significantly higher before than after cleaning. Potential effects of e.g. evaporation and skin emissivity due to residual water, and shielding properties of dirt are discussed. In general, Tempmax variance differences were lower before cleaning than Tempmin variance differences. The variance differences between Tempmax and Tempmin Footpad before cleaning were lower for Tempmax (F = 3.38, p < 0.001), and Tempmax Footpad did not exhibit any significant variance differences before and after cleaning (F = 0.75, p = 0.2). Thus, it is necessary to create a strict protocol (i.e. specifically define the anatomical region of interest, take into account image analysis methods and cleaning procedures) for reducing errors of temperature measurements in future studies of turkey foot health. Specifically, the results indicate that Footpad Tempmax, regardless of cleaning procedures, represent an optimal anatomical region and analysis method for future studies where severity of footpad lesions and impact on animal welfare are studied. © 2018 Elsevier B.V.","10.1016/j.infrared.2018.06.035","Animal welfare; Footpad; Infrared thermography; Thermal imaging; Turkey","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049307550&doi=10.1016%2fj.infrared.2018.06.035&partnerID=40&md5=f6ccee3a0c3a9e8f6ac170c47ff7686a"
"A Systematic Review on Smartphone Skin Cancer Apps: Coherent Taxonomy, Motivations, Open Challenges and Recommendations, and New Research Direction","Yas Q.M.; Zaidan A.A.; Zaidan B.B.; Hashim M.; Lim C.K.","2018","1","1","0","0","0","Unique","0","","","","","","","Objective: This research aims to survey the efforts of researchers in response to the new and disruptive technology of skin cancer apps, map the research landscape from the literature onto coherent taxonomy, and determine the basic characteristics of this emerging field. In addition, this research looks at the motivation behind using Smartphone apps in the diagnosis of skin cancer and in health care and the open challenges that impede the utility of this technology. This study offers valuable recommendations to improve the acceptance and use of medical apps in the literature. Methods: We conducted a comprehensive survey using the keywords ""skin cancer,"" ""apps,"" and ""Smartphone"" or ""m-Health"" in different variations to find all the relevant articles in three major databases: Web of Science, Science Direct, and IEEE Xplore. These databases broadly cover medical and technical literature. Results: We found 110 articles after a comprehensive survey of the literature. Out of the 110 articles, 46 present actual attempts to develop and design medical apps or share certain experiences of doing so. Twenty-eight articles consist of analytical studies on the incidence of skin cancer, the classification of malignant cancer or benign cancer, and the methods of prevention and diagnosis. Twenty-two articles comprise studies that range from the evaluative or comparative study of apps to the exploration of the desired features for skin cancer detection. Fourteen articles consist of reviews and surveys that refer to actual apps or the literature to describe medical apps for a specific specialty, disease, or skin cancer and provide a general overview of the technology. New research direction: With the exception of the 110 papers reviewed earlier in results section, the new directions of this research were described. In state-of-the-art, no particular study presenting watermarking and stenography approaches for any type of skin cancer images based on Smartphone apps is available. Discussion: Researchers have attempted to develop and improve skin cancer apps in several ways since 2011. However, several areas or aspects require further attention. All the articles, regardless of their research focus, attempt to address the challenges that impede the full utility of skin cancer apps and offer recommendations to mitigate their drawbacks. Conclusions: Research on skin cancer apps is active and efficient. This study contributes to this area of research by providing a detailed review of the available options and problems to allow other researchers and participants to further develop skin cancer apps, and the new directions of this research were described. © 2018 World Scientific Publishing Company.","10.1142/S0218126618300039","apps; m-Health; Skin cancer; smartphone; stenography; watermarking","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029477844&doi=10.1142%2fS0218126618300039&partnerID=40&md5=0df334823f744f7328e8335eafa3afd8"
"Partial order label decomposition approaches for melanoma diagnosis","Sánchez-Monedero J.; Pérez-Ortiz M.; Sáez A.; Gutiérrez P.A.; Hervás-Martínez C.","2018","0","1","0","0","0","Unique","0","","","","","","","Melanoma is a type of cancer that develops from the pigment-containing cells known as melanocytes. Usually occurring on the skin, early detection and diagnosis is strongly related to survival rates. Melanoma recognition is a challenging task that nowadays is performed by well trained dermatologists who may produce varying diagnosis due to the task complexity. This motivates the development of automated diagnosis tools, in spite of the inherent difficulties (intra-class variation, visual similarity between melanoma and non-melanoma lesions, among others). In the present work, we propose a system combining image analysis and machine learning to detect melanoma presence and severity. The severity is assessed in terms of melanoma thickness, which is measured by the Breslow index. Previous works mainly focus on the binary problem of detecting the presence of the melanoma. However, the system proposed in this paper goes a step further by also considering the stage of the lesion in the classification task. To do so, we extract 100 features that consider the shape, colour, pigment network and texture of the benign and malignant lesions. The problem is tackled as a five-class classification problem, where the first class represents benign lesions, and the remaining four classes represent the different stages of the melanoma (via the Breslow index). Based on the problem definition, we identify the learning setting as a partial order problem, in which the patterns belonging to the different melanoma stages present an order relationship, but where there is no order arrangement with respect to the benign lesions. Under this assumption about the class topology, we design several proposals to exploit this structure and improve data preprocessing. In this sense, we experimentally demonstrate that those proposals exploiting the partial order assumption achieve better performance than 12 baseline nominal and ordinal classifiers (including a deep learning model) which do not consider this partial order. To deal with class imbalance, we additionally propose specific over-sampling techniques that consider the structure of the problem for the creation of synthetic patterns. The experimental study is carried out with clinician-curated images from the Interactive Atlas of Dermoscopy, which eases reproducibility of experiments. Concerning the results obtained, in spite of having augmented the complexity of the classification problem with more classes, the performance of our proposals in the binary problem is similar to the one reported in the literature. © 2017 Elsevier B.V.","10.1016/j.asoc.2017.11.042","Computer vision; Machine learning; Melanoma; Ordinal classification; Partial order; Skin cancer","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039720658&doi=10.1016%2fj.asoc.2017.11.042&partnerID=40&md5=940866f5f70d345d0805e716b15a3225"
"Accurate segmentation of psoriasis diseases images using k-means algorithm based on cielab (L*A*B) color space","Jarad T.S.; Dawood A.J.","2017","0","1","0","0","0","Unique","0","","","","","","","Context: Psoriasis turned out to be one of the debilitating and enduring inflammatory skin diseases. Often misinterpreted as a casual skin disease, it is estimated that approximately 125 million people worldwide suffers due to this infection. The case is made worse when there is no known cure in the status quo. The communal category of psoriasis has been considered as abruptly demarcated scaly and erythematous plaque at patient’s skin. This disease could ensue anywhere on the human body. Problem: Diagnosis of psoriasis requires an experienced specialist in the field of dermatology because of the presence of other skin diseases similar to a large extent which lead to majority cases of an error in diagnostic. As doctors are still mere human and depends on factors such as eye and physical touch that is not error free. In addition, the drugs for psoriasis disease contain quantities of Chemical materials dangerous to other body organs that may put the functionality of critical organs such as the liver and spleen in jeopardy. Meanwhile, over-treatment leads to loss of life of the patient so it must be re-diagnosis multiple times until the confirmation of a high proportion of the dangerous disease. Time is not the greatest threat for this disease rather the accuracy of diagnosis is much crucial and the accuracy of diagnostic plays a pivotal role in combating this atrocious disease. Regular re-diagnosis is considered a must in order to ensure the survivability of patients from the threat it poses. However, re-diagnosis often consumed a great amount of financial expenditure just to ensure that it is indeed a disease of psoriasis and that the appropriate treatment is given may only lead to another issue which is a financial deficiency. Approach: In this paper, the researcher is interested in separating the image and concentrate on the lesion region and extricating disease district. The process itself is an enormous challenge in light of the fact that there is no discovery of this minute segmentation algorithms division executes and all in all dataset. The proposed strategy is based on K-Means clustering as initial segmentation and gets a divided region, including areas of diseased and the proposed K-Means based on CIE Lab L*a*b color spaces instead of using Red, Green and Blue (RGB) color space. Post segmentation based on color feature will be filtered out as non-interesting objects. Finding: The findings from this study have shown that: Firstly the method is depending on the L*a*b color spaces instead of using RGB color spaces, secondly, the method is based on color feature to select disease region of psoriasis or the correct object. The results of this research confirmed that this method works effectively where we have been implementing this method on a database containing 80 medical images of RGB psoriasis diseases image and shows the accuracy of this method was at 95% when we did a comparison between our method and other ways to find that the proposed strategy gives more effective results in the segmentation. The researcher compared accurate segmentation of K-Means cluster formation with color spaces L*a*b on medical imaging and K-Means cluster formation with color spaces RGB on the same images. © 2005 - Ongoing JATIT & LLS.","","Color image segmentation; K-Means algorithm; Psoriasis disease diagnosis","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029716627&partnerID=40&md5=5be2d4ef031546a108182833d059894b"
"Design and implementation of high speed Gabor filter with variable thresholding process for disease detection","Radhakrishnan K.; Sri D.; Dhanalakshmi R.; Elakkiya M.; Gayathiri G.","2017","0","1","0","0","0","First occurrence","0","","","","","","","In current scenario, there are number of skin related diseases such as tonsillitis, tumors, skin cancers, etc... which can be detected at an early stage and can be cured. For this a new idea is proposed which aims at designing a Gabor filter that makes accurate detection of diseases. We use an efficient pipelined architecture for input. Image segmentation for the detection of diseases is done using Gabor filter that in turn uses shift-add CORDIC algorithm for making the designing of Gabor filter easier. In addition to this algorithm, we use a new algorithm called Variable Thresholding algorithm that sets a threshold value during the decision making period. The algorithm is named variable because, the threshold value can be varied depending upon the diseases we are going to detect. All these algorithms are written in VHDL Language in MODELSIM software, which makes the designing and decision making process more effective. This technique is also implemented on FPGA/CPLD kit that shows us the presence of disease manually through lighting of LED's. Thus the decision of disease is made more accurate and faster through this design. © 2017 IEEE.","10.1109/SSPS.2017.8071634","","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039922876&doi=10.1109%2fSSPS.2017.8071634&partnerID=40&md5=79ff01c9b32cfd17ba2536e26af4bafc"
"Modelling and analysis of skin pigmentation","Mohamad Hani A.F.; Nugroho H.; Shamsudin N.; Hussein S.H.","2017","1","1","0","0","0","Unique","0","","","","","","","Skin pigmentation disorder is an abnormal melanin production condition that causes skin to appear lighter or darker. To assess disease severity, the concentrations of melanin pigment types namely eumelanin (dark brown-black pigment) and pheomelanin (red-yellow pigment) need to be determined objectively for assessing treatment efficacy of skin pigmentation disorders and effects of whitening cream. At present, the assessment of melanin types and is invasive; skin biopsy is conducted for chemical analysis of skin samples. Based on the Dichromatic Reflection Model, a system has been developed to measure the melanin pigments types from light reflectance of skin allowing non-invasive objective measurements. According to the Dichromatic Reflection Model, the skin reflectance (the diffuse reflectance) has been absorbed within skin layers before it reflects back to the surface. The skin reflectance is basically a function of scattering and absorption of skin optical parameters within various skin layers. The system consists of a spectrophotometer and a Graphic Processing Unit (GPU) workstation is developed. The spectrophotometer provides the skin spectral information (skin reflectance). The reflectance is then analysed using an inverse procedure; Monte Carlo simulation of light transport in multi-layered tissue or known simply as MCML, running on GPU workstation. An inverse procedure is applied to determine skin optical parameters (i.e., melanin types) by fitting reflectance measured by spectrophotometer with reflectance simulated by MCML. The system is validated with the Realistic Skin Model (RSM) of the Advanced Systems Analyses Program (ASAP) software. The system can estimate correctly with absolute error of 8.82% only. An observational study involving 110 participants having different skin tones based on Fitzpatrick classification conducted in Universiti Teknologi PETRONAS, Malaysia and University of Burgundy, France are found in line with those reported in the literatures indicating a correct estimation. Data parameters from a second observational study conducted at Hospital Kuala Lumpur and Hospital Serdang with 43 patients suffering from melasma (hyper-pigmented disorder) and vitiligo (hypo-pigmented disorder correspond to the spectral reflectance data of vitiligo and melasma). The developed system can accelerate execution of Monte Carlo for multi-layered skin tissues (MCML) to analyse skin pigmentation and determine melanin types to address the need for a non-invasive and objective evaluation of pigmented lesion and skin-whitening treatments.110 © 2018 by Taylor and Francis Group, LLC.","10.1201/9781315368351","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054217174&doi=10.1201%2f9781315368351&partnerID=40&md5=c3b172489bdbfc1ddac58c60e98370bb"
"Assessment of deep neural networks for the diagnosis of benign and malignant skin neoplasms in comparison with dermatologists: A retrospective validation study","Han S.S.; Moon I.J.; Kim S.H.; Na J.-I.; Kim M.S.; Park G.H.; Park I.; Kim K.; Lim W.; Lee J.H.; Chang S.E.","2020","1","1","0","0","0","Unique","0","","","","","","","Background The diagnostic performance of convolutional neural networks (CNNs) for diagnosing several types of skin neoplasms has been demonstrated as comparable with that of dermatologists using clinical photography. However, the generalizability should be demonstrated using a large-scale external dataset that includes most types of skin neoplasms. In this study, the performance of a neural network algorithm was compared with that of dermatologists in both real-world practice and experimental settings. Methods and findings To demonstrate generalizability, the skin cancer detection algorithm (https://rcnn. modelderm.com) developed in our previous study was used without modification. We conducted a retrospective study with all single lesion biopsied cases (43 disorders; 40,331 clinical images from 10,426 cases: 1,222 malignant cases and 9,204 benign cases); mean age (standard deviation [SD], 52.1 [18.3]; 4,701 men [45.1%]) were obtained from the Department of Dermatology, Severance Hospital in Seoul, Korea between January 1, 2008 and March 31, 2019. Using the external validation dataset, the predictions of the algorithm were compared with the clinical diagnoses of 65 attending physicians who had recorded the clinical diagnoses with thorough examinations in real-world practice. In addition, the results obtained by the algorithm for the data of randomly selected batches of 30 patients were compared with those obtained by 44 dermatologists in experimental settings; the dermatologists were only provided with multiple images of each lesion, without clinical information. With regard to the determination of malignancy, the area under the curve (AUC) achieved by the algorithm was 0.863 (95% confidence interval [CI] 0.852–0.875), when unprocessed clinical photographs were used. The sensitivity and specificity of the algorithm at the predefined high-specificity threshold were 62.7% (95% CI 59.9–65.1) and 90.0% (95% CI 89.4–90.6), respectively. Furthermore, the sensitivity and specificity of the first clinical impression of 65 attending physicians were 70.2% and 95.6%, respectively, which were superior to those of the algorithm (McNemar test; p < 0.0001). The positive and negative predictive values of the algorithm were 45.4% (CI 43.7–47.3) and 94.8% (CI 94.4–95.2), respectively, whereas those of the first clinical impression were 68.1% and 96.0%, respectively. In the reader test conducted using images corresponding to batches of 30 patients, the sensitivity and specificity of the algorithm at the predefined threshold were 66.9% (95% CI 57.7–76.0) and 87.4% (95% CI 82.5–92.2), respectively. Furthermore, the sensitivity and specificity derived from the first impression of 44 of the participants were 65.8% (95% CI 55.7–75.9) and 85.7% (95% CI 82.4–88.9), respectively, which are values comparable with those of the algorithm (Wilcoxon signed-rank test; p = 0.607 and 0.097). Limitations of this study include the exclusive use of high-quality clinical photographs taken in hospitals and the lack of ethnic diversity in the study population. Conclusions Our algorithm could diagnose skin tumors with nearly the same accuracy as a dermatologist when the diagnosis was performed solely with photographs. However, as a result of limited data relevancy, the performance was inferior to that of actual medical examination. To achieve more accurate predictive diagnoses, clinical information should be integrated with imaging information. © 2020 Han et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.","10.1371/journal.pmed.1003381","","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096809332&doi=10.1371%2fjournal.pmed.1003381&partnerID=40&md5=51d0dfa1bb4630abf473fcf38f77eaa4"
"Progress of residual neural network optimization algorithm for medical imaging disease diagnosis; [医学影像疾病诊断的残差神经网络优化算法研究进展]","Zhou T.; Huo B.; Lu H.; Shi H.","2020","0","1","0","0","0","Unique","0","","","","","","","Residual neural network (ResNet) has gained considerable attention in deep learning research over the last few years and has made great achievements in computer vision. The deep convolutional network represented by ResNet is increasingly used in the field of medical imaging and has achieved good results in the clinical diagnosis, staging, metastasis, treatment decision, and target area delineation of major diseases, such as tumors, cardiovascular and cerebrovascular diseases, and nervous system diseases. The optimization of the ResNet algorithm is an important part of the ResNet research. It largely determines model performance, such as generalization and convergence. This article summarizes the learning optimization of ResNet. First, the optimization of the learning algorithm of ResNet is elaborated, and the six aspects of activation function, loss function, parameter optimization algorithm, learning decay rate algorithm, normalization, and regularization are summarized. Nine improvement methods exist for the activation function; they are sigmoid, tanh, ReLU, PReLU, randomized ReLU, exponential linear units(ELU), softplus function, noisy softplus function, and maxout. The loss function includes 12 types: cross-entropy, mean square, Euclidean distance, contrast, hinge, softmax, L-softmax, A-softmax, L2 softmax, cosine, center, and focus losses. Eight learning rate decay methods, namely, piecewise constant, polynomial, exponential, inverse time, natural exponential, cosine, linear cosine, and noise linear cosine, are summarized. The normalization algorithms include batch normalization and renormalization. The regularization technologies include seven types: input data, data enhancement, early stop method, L1 regularization, L2 regularization, dropout, and dropout connect. Second, the application study of the residual network model in the diagnosis of medical imaging diseases is reviewed. ResNet is used to diagnose six types of diseases: lung tumor, skin disease, breast cancer, brain disease, diabetes, and hematological disease. 1) Lung cancer. Considerable data show that the incidence of lung cancer is increasing yearly, which is a serious threat to human health. Early diagnosis and detection are essential for the treatment of lung cancer. The main contributions of ResNet in lung tumor research are particle swarm optimization(PSo)+convolutional neural network(CNN), intermediate dense projection method+DenseNet, DenseNet+fully convolutional network(FCN), attention mechanism+ResNet, dense network+U-Net, and 3D+CNN. 2) Skin cancer. Malignant melanoma is one of the most common and deadly skin cancers. Melanoma can be cured if it is properly treated in the early stage. The main contributions of ResNet in the diagnosis and research of skin diseases are integrated learning+CNN, multichannel ResNet, ResNet+support vector machine(SVM), integrated learning+ResNet, and whale optimization algorithm+CNN. 3) Breast cancer. It is a malignant tumor in women, which seriously affects their physical and mental health. The main contributions of ResNet in the diagnosis and research of breast cancer include transfer learning+ResNet, decision tree+ResNet, CNN+SVM, DesneNet-II, and SE-attention+CNN. 4) Alzheimer's disease (AD). It is an irreversible brain disease accompanied with progressive impairment of memory and cognitive functions. No effective cure method exists for AD. Early diagnosis of AD is particularly important for patient care and treatment. The main contributions of ResNet in the diagnosis and research of brain diseases are DenseNet, multiscale ResNet, 3DResNet, and multitask CNN+3D DenseNet. 5) Diabetic retinopathy (glycemic retinopathy). It is an eye disease induced by long-term diabetes, which will cause the patient to lose sight and eventually lead to blindness. The main contributions of ResNet in relevant diagnosis and research are migration learning+CNN, FCN+ResNet, multicategory ResNet, and ResNet+SVM. 6) Blood diseases. The proportion of white blood cells in liquid is usually an indicator of diseases. The classification and counting of white blood cells are used in the process of diagnosing diseases. White blood cell test plays a vital role in the detection and treatment of leukemia, anemia, and other diseases. The main contributions of ResNet in the diagnosis and research of blood diseases are as follows: deep supervision ResNet, fine-tuned ResNet, deep cross ResNet, and deep supervision FCN. Lastly, the future development of deep learning in medical imaging is summarized. In this paper, the algorithms of ResNet are systematically summarized, which has a positive significance to the research and development of ResNet. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","10.11834/jig.200207","Deep learning; Disease diagnosis; Medical image; Optimization algorithm; Residual neural network(ResNet)","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093941157&doi=10.11834%2fjig.200207&partnerID=40&md5=cb8379e5f979f659be73048ecb7a7c36"
"Explainable fully connected visual words for the classification of skin cancer confocal images: Interpreting the influence of visual words in classifying benign vs malignant pattern","Kallipolitis A.; Stratigos A.; Zarras A.; Maglogiannis I.","2020","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is affecting the lives of million people worldwide. Early detection and treatment of the cause can reduce drastically morbidity. Although the main workflow in dermatology clinics includes invasive skin removal procedures for diagnostic purposes, Reflectance Confocal Microscopy (RCM) provides an ancillary, non-invasive methodology for reviewing areas of interest of the human skin at a high resolution. In this paper, we propose a method for the classification and the interpretation of visual patterns in skin cancer confocal images. Both tasks are based on the formation of a visual vocabulary from Speeded up Robust Features (SURF) and the utilization of simple shallow artificial neural network with fully connected layers. Interpretability of the predictive models is also quite important, since it improves their reliability, accountability, transparency and provides useful insight of how to evolve the predictive model towards better performance. The paper discusses the technical details of both approaches along with some initial results.  © 2020 ACM.","10.1145/3411408.3411435","Bag of Visual Words; Interpretability; Reflectance Confocal Microscopy; Skin Cancer","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091116151&doi=10.1145%2f3411408.3411435&partnerID=40&md5=df7be13b5f8ceda4b015e7045cd10f41"
"Transfer learning using a multi-scale and multi-network ensemble for skin lesion classification","Mahbod A.; Schaefer G.; Wang C.; Dorffner G.; Ecker R.; Ellinger I.","2020","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Skin cancer is among the most common cancer types in the white population and consequently computer aided methods for skin lesion classification based on dermoscopic images are of great interest. A promising approach for this uses transfer learning to adapt pre-trained convolutional neural networks (CNNs) for skin lesion diagnosis. Since pre-training commonly occurs with natural images of a fixed image resolution and these training images are usually significantly smaller than dermoscopic images, downsampling or cropping of skin lesion images is required. This however may result in a loss of useful medical information, while the ideal resizing or cropping factor of dermoscopic images for the fine-tuning process remains unknown. Methods: We investigate the effect of image size for skin lesion classification based on pre-trained CNNs and transfer learning. Dermoscopic images from the International Skin Imaging Collaboration (ISIC) skin lesion classification challenge datasets are either resized to or cropped at six different sizes ranging from 224 × 224 to 450 × 450. The resulting classification performance of three well established CNNs, namely EfficientNetB0, EfficientNetB1 and SeReNeXt-50 is explored. We also propose and evaluate a multi-scale multi-CNN (MSM-CNN) fusion approach based on a three-level ensemble strategy that utilises the three network architectures trained on cropped dermoscopic images of various scales. Results: Our results show that image cropping is a better strategy compared to image resizing delivering superior classification performance at all explored image scales. Moreover, fusing the results of all three fine-tuned networks using cropped images at all six scales in the proposed MSM-CNN approach boosts the classification performance compared to a single network or a single image scale. On the ISIC 2018 skin lesion classification challenge test set, our MSM-CNN algorithm yields a balanced multi-class accuracy of 86.2% making it the currently second ranked algorithm on the live leaderboard. Conclusions: We confirm that the image size has an effect on skin lesion classification performance when employing transfer learning of CNNs. We also show that image cropping results in better performance compared to image resizing. Finally, a straightforward ensembling approach that fuses the results from images cropped at six scales and three fine-tuned CNNs is shown to lead to the best classification performance. © 2020 Elsevier B.V.","10.1016/j.cmpb.2020.105475","Deep learning; Dermoscopy; Image cropping; Image resolution; Medical image analysis; Skin cancer; Transfer learning","239","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082848696&doi=10.1016%2fj.cmpb.2020.105475&partnerID=40&md5=99428194d1bb64de5ead87f20f85a31d"
"Artificial intelligence and its effect on dermatologists' accuracy in dermoscopic melanoma image classification: Web-based survey study","Maron R.C.; Utikal J.S.; Hekler A.; Hauschild A.; Sattler E.; Sondermann W.; Haferkamp S.; Schilling B.; Heppt M.V.; Jansen P.; Reinholz M.; Franklin C.; Schmitt L.; Hartmann D.; Krieghoff-Henning E.; Schmitt M.; Weichenthal M.; von Kalle C.; Fröhling S.; Brinker T.J.","2020","0","1","0","0","0","Unique","0","","","","","","","Background: Early detection of melanoma can be lifesaving but this remains a challenge. Recent diagnostic studies have revealed the superiority of artificial intelligence (AI) in classifying dermoscopic images of melanoma and nevi, concluding that these algorithms should assist a dermatologist's diagnoses. Objective: The aim of this study was to investigate whether AI support improves the accuracy and overall diagnostic performance of dermatologists in the dichotomous image-based discrimination between melanoma and nevus. Methods: Twelve board-certified dermatologists were presented disjoint sets of 100 unique dermoscopic images of melanomas and nevi (total of 1200 unique images), and they had to classify the images based on personal experience alone (part I) and with the support of a trained convolutional neural network (CNN, part II). Additionally, dermatologists were asked to rate their confidence in their final decision for each image. Results: While the mean specificity of the dermatologists based on personal experience alone remained almost unchanged (70.6% vs 72.4%; P=.54) with AI support, the mean sensitivity and mean accuracy increased significantly (59.4% vs 74.6%; P=.003 and 65.0% vs 73.6%; P=.002, respectively) with AI support. Out of the 10% (10/94; 95% CI 8.4%-11.8%) of cases where dermatologists were correct and AI was incorrect, dermatologists on average changed to the incorrect answer for 39% (4/10; 95% CI 23.2%-55.6%) of cases. When dermatologists were incorrect and AI was correct (25/94, 27%; 95% CI 24.0%-30.1%), dermatologists changed their answers to the correct answer for 46% (11/25; 95% CI 33.1%-58.4%) of cases. Additionally, the dermatologists' average confidence in their decisions increased when the CNN confirmed their decision and decreased when the CNN disagreed, even when the dermatologists were correct. Reported values are based on the mean of all participants. Whenever absolute values are shown, the denominator and numerator are approximations as every dermatologist ended up rating a varying number of images due to a quality control step. Conclusions: The findings of our study show that AI support can improve the overall accuracy of the dermatologists in the dichotomous image-based discrimination between melanoma and nevus. This supports the argument for AI-based tools to aid clinicians in skin lesion classification and provides a rationale for studies of such classifiers in real-life settings, wherein clinicians can integrate additional information such as patient age and medical history into their decisions. © Roman C Maron, Jochen S Utikal, Achim Hekler, Axel Hauschild, Elke Sattler, Wiebke Sondermann, Sebastian Haferkamp, Bastian Schilling, Markus V Heppt, Philipp Jansen, Markus Reinholz, Cindy Franklin, Laurenz Schmitt, Daniela Hartmann, Eva Krieghoff-Henning, Max Schmitt, Michael Weichenthal, Christof von Kalle, Stefan Fröhling, Titus J Brinker. Originally published in the Journal of Medical Internet Research (http://www.jmir.org), 11.09.2020. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.","10.2196/18091","Artificial intelligence; Deep learning; Dermatology; Diagnosis; Machine learning; Melanoma; Neural network; Nevi; Skin neoplasm","57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090921013&doi=10.2196%2f18091&partnerID=40&md5=45a560cb0690eff37f7f6495e715fb83"
"Predicting bovine tuberculosis status of dairy cows from mid-infrared spectral data of milk using deep learning","Denholm S.J.; Brand W.; Mitchell A.P.; Wells A.T.; Krzyzelewski T.; Smith S.L.; Wall E.; Coffey M.P.","2020","0","1","0","0","0","Unique","0","","","","","","","Bovine tuberculosis (bTB) is a zoonotic disease in cattle that is transmissible to humans, distributed worldwide, and considered endemic throughout much of England and Wales. Mid-infrared (MIR) analysis of milk is used routinely to predict fat and protein concentration, and is also a robust predictor of several other economically important traits including individual fatty acids and body energy. This study predicted bTB status of UK dairy cows using their MIR spectral profiles collected as part of routine milk recording. Bovine tuberculosis data were collected as part of the national bTB testing program for Scotland, England, and Wales; these data provided information from over 40,500 bTB herd breakdowns. Corresponding individual cow life–history data were also available and provided information on births, movements, and deaths of all cows in the study. Data relating to single intradermal comparative cervical tuberculin (SICCT) skin-test results, culture, slaughter status, and presence of lesions were combined to create a binary bTB phenotype labeled 0 to represent nonresponders (i.e., healthy cows) and 1 to represent responders (i.e., bTB-affected cows). Contemporaneous individual milk MIR spectral data were collected as part of monthly routine milk recording and matched to bTB status of individual animals on the single intradermal comparative cervical tuberculin test date (±15 d). Deep learning, a sub-branch of machine learning, was used to train artificial neural networks and develop a prediction pipeline for subsequent use in national herds as part of routine milk recording. Spectra were first converted to 53 × 20-pixel PNG images, then used to train a deep convolutional neural network. Deep convolutional neural networks resulted in a bTB prediction accuracy (i.e., the number of correct predictions divided by the total number of predictions) of 71% after training for 278 epochs. This was accompanied by both a low validation loss (0.71) and moderate sensitivity and specificity (0.79 and 0.65, respectively). To balance data in each class, additional training data were synthesized using the synthetic minority over sampling technique. Accuracy was further increased to 95% (after 295 epochs), with corresponding validation loss minimized (0.26), when synthesized data were included during training of the network. Sensitivity and specificity also saw a 1.22- and 1.45-fold increase to 0.96 and 0.94, respectively, when synthesized data were included during training. We believe this study to be the first of its kind to predict bTB status from milk MIR spectral data. We also believe it to be the first study to use milk MIR spectral data to predict a disease phenotype, and posit that the automated prediction of bTB status at routine milk recording could provide farmers with a robust tool that enables them to make early management decisions on potential reactor cows, and thus help slow the spread of bTB. © 2020 American Dairy Science Association","10.3168/jds.2020-18328","bovine tuberculosis; dairy cow; deep learning; mid-infrared spectroscopy; noninvasive","46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089558967&doi=10.3168%2fjds.2020-18328&partnerID=40&md5=a1ae45ee6212b96ef079c4e0d69875f9"
"Diagnostic capacity of skin tumor artificial intelligence-assisted decision-making software in real-world clinical settings","Li C.-X.; Fei W.-M.; Shen C.-B.; Wang Z.-Y.; Jing Y.; Meng R.-S.; Cui Y.","2020","1","1","0","0","0","Unique","0","","","","","","","BackgroundYouzhi artificial intelligence (AI) software is the AI-assisted decision-making system for diagnosing skin tumors. The high diagnostic accuracy of Youzhi AI software was previously validated in specific datasets. The objective of this study was to compare the performance of diagnostic capacity between Youzhi AI software and dermatologists in real-world clinical settings.MethodsA total of 106 patients who underwent skin tumor resection in the Dermatology Department of China-Japan Friendship Hospital from July 2017 to June 2019 and were confirmed as skin tumors by pathological biopsy were selected. Dermoscopy and clinical images of 106 patients were diagnosed by Youzhi AI software and dermatologists at different dermoscopy diagnostic levels. The primary outcome was to compare the diagnostic accuracy of the Youzhi AI software with that of dermatologists and that measured in the laboratory using specific data sets. The secondary results included the sensitivity, specificity, positive predictive value, negative predictive value, F-measure, and Matthews correlation coefficient of Youzhi AI software in the real-world.ResultsThe diagnostic accuracy of Youzhi AI software in real-world clinical settings was lower than that of the laboratory data (P < 0.001). The output result of Youzhi AI software has good stability after several tests. Youzhi AI software diagnosed benign and malignant diseases by recognizing dermoscopic images and diagnosed disease types with higher diagnostic accuracy than by recognizing clinical images (P = 0.008, P = 0.016, respectively). Compared with dermatologists, Youzhi AI software was more accurate in the diagnosis of skin tumor types through the recognition of dermoscopic images (P = 0.01). By evaluating the diagnostic performance of dermatologists under different modes, the diagnostic accuracy of dermatologists in diagnosing disease types by matching dermoscopic and clinical images was significantly higher than that by identifying dermoscopic and clinical images in random sequence (P = 0.022). The diagnostic accuracy of dermatologists in the diagnosis of benign and malignant diseases by recognizing dermoscopic images was significantly higher than that by recognizing clinical images (P = 0.010).ConclusionThe diagnostic accuracy of Youzhi AI software for skin tumors in real-world clinical settings was not as high as that of using special data sets in the laboratory. However, there was no significant difference between the diagnostic capacity of Youzhi AI software and the average diagnostic capacity of dermatologists. It can provide assistant diagnostic decisions for dermatologists in the current state. © 2020 Lippincott Williams and Wilkins. All rights reserved.","10.1097/CM9.0000000000001002","Artificial intelligence; Diagnostic accuracy; Skin tumor","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090505753&doi=10.1097%2fCM9.0000000000001002&partnerID=40&md5=f8d95add256f713f81b299ad034d6723"
"Augmented Intelligence Dermatology: Deep Neural Networks Empower Medical Professionals in Diagnosing Skin Cancer and Predicting Treatment Options for 134 Skin Disorders","Han S.S.; Park I.; Eun Chang S.; Lim W.; Kim M.S.; Park G.H.; Chae J.B.; Huh C.H.; Na J.-I.","2020","1","0","0","0","0","Unique","0","","","","","","","Although deep learning algorithms have demonstrated expert-level performance, previous efforts were mostly binary classifications of limited disorders. We trained an algorithm with 220,680 images of 174 disorders and validated it using Edinburgh (1,300 images; 10 disorders) and SNU datasets (2,201 images; 134 disorders). The algorithm could accurately predict malignancy, suggest primary treatment options, render multi-class classification among 134 disorders, and improve the performance of medical professionals. The area under the curves for malignancy detection were 0.928 ± 0.002 (Edinburgh) and 0.937 ± 0.004 (SNU). The area under the curves of primary treatment suggestion (SNU) were 0.828 ± 0.012, 0.885 ± 0.006, 0.885 ± 0.006, and 0.918 ± 0.006 for steroids, antibiotics, antivirals, and antifungals, respectively. For multi-class classification, the mean top-1 and top-5 accuracies were 56.7 ± 1.6% and 92.0 ± 1.1% (Edinburgh) and 44.8 ± 1.2% and 78.1 ± 0.3% (SNU), respectively. With the assistance of our algorithm, the sensitivity and specificity of 47 clinicians (21 dermatologists and 26 dermatology residents) for malignancy prediction (SNU; 240 images) were improved by 12.1% (P < 0.0001) and 1.1% (P < 0.0001), respectively. The malignancy prediction sensitivity of 23 non-medical professionals was significantly increased by 83.8% (P < 0.0001). The top-1 and top-3 accuracies of four doctors in the multi-class classification of 134 diseases (SNU; 2,201 images) were increased by 7.0% (P = 0.045) and 10.1% (P = 0.0020), respectively. The results suggest that our algorithm may serve as augmented intelligence that can empower medical professionals in diagnostic dermatology. © 2020 The Authors","10.1016/j.jid.2020.01.019","","169","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086703571&doi=10.1016%2fj.jid.2020.01.019&partnerID=40&md5=a86ba9e32b711a8d95189bb875e88121"
"A deep dive into understanding tumor foci classification using multiparametric MRI based on convolutional neural network","Zong W.; Lee J.K.; Liu C.; Carver E.N.; Feldman A.M.; Janic B.; Elshaikh M.A.; Pantelic M.V.; Hearshen D.; Chetty I.J.; Movsas B.; Wen N.","2020","0","1","0","0","0","Unique","0","","","","","","","Purpose: Deep learning models have had a great success in disease classifications using large data pools of skin cancer images or lung X-rays. However, data scarcity has been the roadblock of applying deep learning models directly on prostate multiparametric MRI (mpMRI). Although model interpretation has been heavily studied for natural images for the past few years, there has been a lack of interpretation of deep learning models trained on medical images. In this paper, an efficient convolutional neural network (CNN) was developed and the model interpretation at various convolutional layers was systematically analyzed to improve the understanding of how CNN interprets multimodality medical images and the predictive powers of features at each layer. The problem of small sample size was addressed by feeding the intermediate features into a traditional classification algorithm known as weighted extreme learning machine (wELM), with imbalanced distribution among output categories taken into consideration. Methods: The training data collection used a retrospective set of prostate MR studies, from SPIE-AAPM-NCI PROSTATEx Challenges held in 2017. Three hundred twenty biopsy samples of lesions from 201 prostate cancer patients were diagnosed and identified as clinically significant (malignant) or not significant (benign). All studies included T2-weighted (T2W), proton density-weighted (PD-W), dynamic contrast enhanced (DCE) and diffusion-weighted (DW) imaging. After registration and lesion-based normalization, a CNN with four convolutional layers were developed and trained on tenfold cross validation. The features from intermediate layers were then extracted as input to wELM to test the discriminative power of each individual layer. The best performing model from the tenfolds was chosen to be tested on the holdout cohort from two sources. Feature maps after each convolutional layer were then visualized to monitor the trend, as the layer propagated. Scatter plotting was used to visualize the transformation of data distribution. Finally, a class activation map was generated to highlight the region of interest based on the model perspective. Results: Experimental trials indicated that the best input for CNN was a modality combination of T2W, apparent diffusion coefficient (ADC) and DWIb50. The convolutional features from CNN paired with a weighted extreme learning classifier showed substantial performance compared to a CNN end-to-end training model. The feature map visualization reveals similar findings on natural images where lower layers tend to learn lower level features such as edges, intensity changes, etc, while higher layers learn more abstract and task-related concept such as the lesion region. The generated saliency map revealed that the model was able to focus on the region of interest where the lesion resided and filter out background information, including prostate boundary, rectum, etc. Conclusions: This work designs a customized workflow for the small and imbalanced dataset of prostate mpMRI where features were extracted from a deep learning model and then analyzed by a traditional machine learning classifier. In addition, this work contributes to revealing how deep learning models interpret mpMRI for prostate cancer patient stratification. © 2020 American Association of Physicists in Medicine","10.1002/mp.14255","convolutional neural network; model interpretation; prostate cancer mpMRI lesion classification; saliency map; small sample size","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086222948&doi=10.1002%2fmp.14255&partnerID=40&md5=b0b70efa4fa4cb95545fb7a4f9aeab43"
"Comparison of multiplexed immunofluorescence imaging to chromogenic immunohistochemistry of skin biomarkers in response to monkeypox virus infection","Sood A.; Sui Y.; McDonough E.; Santamaría-Pang A.; Al-Kofahi Y.; Pang Z.; Jahrling P.B.; Kuhn J.H.; Ginty F.","2020","1","1","0","0","0","Unique","0","","","","","","","Over the last 15 years, advances in immunofluorescence-imaging based cycling methods, antibody conjugation methods, and automated image processing have facilitated the development of a high-resolution, multiplexed tissue immunofluorescence (MxIF) method with single cell-level quantitation termed Cell DIVETM. Originally developed for fixed oncology samples, here it was evaluated in highly fixed (up to 30 days), archived monkeypox virus-induced inflammatory skin lesions from a retrospective study in 11 rhesus monkeys to determine whether MxIF was comparable to manual H-scoring of chromogenic stains. Six protein markers related to immune and cellular response (CD68, CD3, Hsp70, Hsp90, ERK1/2, ERK1/2 pT202_pY204) were manually quantified (H-scores) by a pathologist from chromogenic IHC double stains on serial sections and compared to MxIF automated single cell quantification of the same markers that were multiplexed on a single tissue section. Overall, there was directional consistency between the H-score and the MxIF results for all markers except phosphorylated ERK1/2 (ERK1/2 pT202_pY204), which showed a decrease in the lesion compared to the adjacent non-lesioned skin by MxIF vs an increase via H-score. Improvements to automated segmentation using machine learning and adding additional cell markers for cell viability are future options for improvement. This method could be useful in infectious disease research as it conserves tissue, provides marker colocalization data on thousands of cells, allowing further cell level data mining as well as a reduction in user bias. © 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).","10.3390/v12080787","IHC; Immunohistochemistry; Monkeypox; Monkeypox virus; MPXV; Multiplexed immunofluorescence; MxIF; Orthopoxvirus; Poxviridae; Poxvirus","30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088812183&doi=10.3390%2fv12080787&partnerID=40&md5=59bf6476c8bf190a3988f296b635ae97"
"The development of a skin cancer classification system for pigmented skin lesions using deep learning","Jinnai S.; Yamazaki N.; Hirano Y.; Sugawara Y.; Ohe Y.; Hamamoto R.","2020","1","1","0","0","0","Unique","0","","","","","","","Recent studies have demonstrated the usefulness of convolutional neural networks (CNNs) to classify images of melanoma, with accuracies comparable to those achieved by dermatologists. However, the performance of a CNN trained with only clinical images of a pigmented skin lesion in a clinical image classification task, in competition with dermatologists, has not been reported to date. In this study, we extracted 5846 clinical images of pigmented skin lesions from 3551 patients. Pigmented skin lesions included malignant tumors (malignant melanoma and basal cell carcinoma) and benign tumors (nevus, seborrhoeic keratosis, senile lentigo, and hematoma/hemangioma). We created the test dataset by randomly selecting 666 patients out of them and picking one image per patient, and created the training dataset by giving bounding-box annotations to the rest of the images (4732 images, 2885 patients). Subsequently, we trained a faster, region-based CNN (FRCNN) with the training dataset and checked the performance of the model on the test dataset. In addition, ten board-certified dermatologists (BCDs) and ten dermatologic trainees (TRNs) took the same tests, and we compared their diagnostic accuracy with FRCNN. For six-class classification, the accuracy of FRCNN was 86.2%, and that of the BCDs and TRNs was 79.5% (p = 0.0081) and 75.1% (p < 0.00001), respectively. For two-class classification (benign or malignant), the accuracy, sensitivity, and specificity were 91.5%, 83.3%, and 94.5% by FRCNN; 86.6%, 86.3%, and 86.6% by BCD; and 85.3%, 83.5%, and 85.9% by TRN, respectively. False positive rates and positive predictive values were 5.5% and 84.7% by FRCNN, 13.4% and 70.5% by BCD, and 14.1% and 68.5% by TRN, respectively. We compared the classification performance of FRCNN with 20 dermatologists. As a result, the classification accuracy of FRCNN was better than that of the dermatologists. In the future, we plan to implement this system in society and have it used by the general public, in order to improve the prognosis of skin cancer. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","10.3390/biom10081123","Artificial intelligence (AI); Deep learning; Melanoma; Neural network; Skin cancer","206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088984605&doi=10.3390%2fbiom10081123&partnerID=40&md5=9f0e69918c04a366670b253533b3e954"
"Past and present of computer-assisted dermoscopic diagnosis: performance of a conventional image analyser versus a convolutional neural network in a prospective data set of 1,981 skin lesions","Sies K.; Winkler J.K.; Fink C.; Bardehle F.; Toberer F.; Buhl T.; Enk A.; Blum A.; Rosenberger A.; Haenssle H.A.","2020","1","1","0","0","0","Unique","0","","","","","","","Background: Convolutional neural networks (CNNs) have shown a dermatologist-level performance in the classification of skin lesions. We aimed to deliver a head-to-head comparison of a conventional image analyser (CIA), which depends on segmentation and weighting of handcrafted features, to a CNN trained by deep learning. Methods: Cross-sectional study using a real-world, prospectively acquired, dermoscopic dataset of 1981 skin lesions to compare the diagnostic performance of a market-approved CNN (Moleanalyzer-Pro™, developed in 2018) to a CIA (Moleanalyzer-3™/Dynamole™; developed in 2004, all FotoFinder Systems Inc, Germany). As a reference standard, we used histopathological diagnoses (n = 785) or, in non-excised benign lesions (n = 1196), expert consensus plus an uneventful follow-up by sequential digital dermoscopy for at least 2 years. Results: A total of 281 malignant lesions and 1700 benign lesions from 435 patients (62.2% male, mean age: 52 years) were prospectively imaged. The CNN showed a sensitivity of 77.6% (95% confidence interval [CI]: [72.4%–82.1%]), specificity of 95.3% (95% CI: [94.2%–96.2%]), and receiver operating characteristic (ROC)-area under the curve (AUC) of 0.945 (95% CI: [0.930–0.961]). In contrast, the CIA achieved a sensitivity of 53.4% (95% CI: [47.5%–59.1%]), specificity of 86.6% (95% CI: [84.9%–88.1%]) and ROC-AUC of 0.738 (95% CI: [0.701–0.774]). The data set included melanomas originally diagnosed by dynamic changes during sequential digital dermoscopy (52 of 201, 20.6%), which reduced the sensitivities of both classifiers. Pairwise comparisons of sensitivities, specificities, and ROC-AUCs indicated a clear outperformance by the CNN (all p < 0.001). Conclusions: The superior diagnostic performance of the CNN argues against a continued application of former CIAs as an aide to physicians’ clinical management decisions. © 2020 Elsevier Ltd","10.1016/j.ejca.2020.04.043","Automated melanoma detection; Computer-assisted diagnosis; Convolutional neural network; Deep learning; Dermoscopy; Skin cancer; Skin lesions","27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086133129&doi=10.1016%2fj.ejca.2020.04.043&partnerID=40&md5=c17ff113bf3d78c9e39a3eb42bbbcf7a"
"Multiple skin lesions diagnostics via integrated deep convolutional networks for segmentation and classification","Al-masni M.A.; Kim D.-H.; Kim T.-S.","2020","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Computer automated diagnosis of various skin lesions through medical dermoscopy images remains a challenging task. Methods: In this work, we propose an integrated diagnostic framework that combines a skin lesion boundary segmentation stage and a multiple skin lesions classification stage. Firstly, we segment the skin lesion boundaries from the entire dermoscopy images using deep learning full resolution convolutional network (FrCN). Then, a convolutional neural network classifier (i.e., Inception-v3, ResNet-50, Inception-ResNet-v2, and DenseNet-201) is applied on the segmented skin lesions for classification. The former stage is a critical prerequisite step for skin lesion diagnosis since it extracts prominent features of various types of skin lesions. A promising classifier is selected by testing well-established classification convolutional neural networks. The proposed integrated deep learning model has been evaluated using three independent datasets (i.e., International Skin Imaging Collaboration (ISIC) 2016, 2017, and 2018, which contain two, three, and seven types of skin lesions, respectively) with proper balancing, segmentation, and augmentation. Results: In the integrated diagnostic system, segmented lesions improve the classification performance of Inception-ResNet-v2 by 2.72% and 4.71% in terms of the F1-score for benign and malignant cases of the ISIC 2016 test dataset, respectively. The classifiers of Inception-v3, ResNet-50, Inception-ResNet-v2, and DenseNet-201 exhibit their capability with overall weighted prediction accuracies of 77.04%, 79.95%, 81.79%, and 81.27% for two classes of ISIC 2016, 81.29%, 81.57%, 81.34%, and 73.44% for three classes of ISIC 2017, and 88.05%, 89.28%, 87.74%, and 88.70% for seven classes of ISIC 2018, respectively, demonstrating the superior performance of ResNet-50. Conclusions: The proposed integrated diagnostic networks could be used to support and aid dermatologists for further improvement in skin cancer diagnosis. © 2020","10.1016/j.cmpb.2020.105351","CAD; Classification; CNN; Deep learning; ISIC; Melanoma; Segmentation; Skin lesion","311","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078780551&doi=10.1016%2fj.cmpb.2020.105351&partnerID=40&md5=b232d473289572e7864d37e6af4b1eb3"
"Dense-unet: A novel multiphoton in vivo cellular image segmentation model based on a convolutional neural network","Cai S.; Tian Y.; Lui H.; Zeng H.; Wu Y.; Chen G.","2020","0","1","0","0","0","Unique","0","","","","","","","Background: Multiphoton microscopy (MPM) offers a feasible approach for the biopsy in clinical medicine, but it has not been used in clinical applications due to the lack of efficient image processing methods, especially the automatic segmentation technology. Segmentation technology is still one of the most challenging assignments of the MPM imaging technique. Methods: The MPM imaging segmentation model based on deep learning is one of the most effective methods to address this problem. In this paper, the practicability of using a convolutional neural network (CNN) model to segment the MPM image of skin cells in vivo was explored. A set of MPM in vivo skin cells images with a resolution of 128×128 was successfully segmented under the Python environment with TensorFlow. A novel deep-learning segmentation model named Dense-UNet was proposed. The Dense-UNet, which is based on U-net structure, employed the dense concatenation to deepen the depth of the network architecture and achieve feature reuse. This model included four expansion modules (each module consisted of four down-sampling layers) to extract features. Results: Sixty training images were taken from the dorsal forearm using a femtosecond Ti:Sa laser running at 735 nm. The resolution of the images is 128×128 pixels. Experimental results confirmed that the accuracy of Dense-UNet (92.54%) was higher than that of U-Net (88.59%), with a significantly lower loss value of 0.1681. The 90.60% Dice coefficient value of Dense-UNet outperformed U-Net by 11.07%. The F1-Score of Dense-UNet, U-Net, and Seg-Net was 93.35%, 90.02%, and 85.04%, respectively. Conclusions: The deepened down-sampling path improved the ability of the model to capture cellular fined-detailed boundary features, while the symmetrical up-sampling path provided a more accurate location based on the test result. These results were the first time that the segmentation of MPM in vivo images had been adopted by introducing a deep CNN to bridge this gap in Dense-UNet technology. Dense-UNet has reached ultramodern performance for MPM images, especially for in vivo images with low resolution. This implementation supplies an automatic segmentation model based on deep learning for high-precision segmentation of MPM images in vivo. © Quantitative Imaging in Medicine and Surgery. All rights reserved.","10.21037/QIMS-19-1090","Dense-UNet; Image segmentation; Multiphoton microscopy (MPM); Skin in vivo; U-Net","237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087514814&doi=10.21037%2fQIMS-19-1090&partnerID=40&md5=b4854550d1ad5b86058e191ee53118f1"
"Artificial Intelligence in Skin Cancer Diagnostics: The Patients' Perspective","Jutzi T.B.; Krieghoff-Henning E.I.; Holland-Letz T.; Utikal J.S.; Hauschild A.; Schadendorf D.; Sondermann W.; Fröhling S.; Hekler A.; Schmitt M.; Maron R.C.; Brinker T.J.","2020","1","1","0","0","0","Unique","0","","","","","","","Background: Artificial intelligence (AI) has shown promise in numerous experimental studies, particularly in skin cancer diagnostics. Translation of these findings into the clinic is the logical next step. This translation can only be successful if patients' concerns and questions are addressed suitably. We therefore conducted a survey to evaluate the patients' view of artificial intelligence in melanoma diagnostics in Germany, with a particular focus on patients with a history of melanoma. Participants and Methods: A web-based questionnaire was designed using LimeSurvey, sent by e-mail to university hospitals and melanoma support groups and advertised on social media. The anonymous questionnaire evaluated patients' expectations and concerns toward artificial intelligence in general as well as their attitudes toward different application scenarios. Descriptive analysis was performed with expression of categorical variables as percentages and 95% confidence intervals. Statistical tests were performed to investigate associations between sociodemographic data and selected items of the questionnaire. Results: 298 individuals (154 with a melanoma diagnosis, 143 without) responded to the questionnaire. About 94% [95% CI = 0.91–0.97] of respondents supported the use of artificial intelligence in medical approaches. 88% [95% CI = 0.85–0.92] would even make their own health data anonymously available for the further development of AI-based applications in medicine. Only 41% [95% CI = 0.35–0.46] of respondents were amenable to the use of artificial intelligence as stand-alone system, 94% [95% CI = 0.92–0.97] to its use as assistance system for physicians. In sub-group analyses, only minor differences were detectable. Respondents with a previous history of melanoma were more amenable to the use of AI applications for early detection even at home. They would prefer an application scenario where physician and AI classify the lesions independently. With respect to AI-based applications in medicine, patients were concerned about insufficient data protection, impersonality and susceptibility to errors, but expected faster, more precise and unbiased diagnostics, less diagnostic errors and support for physicians. Conclusions: The vast majority of participants exhibited a positive attitude toward the use of artificial intelligence in melanoma diagnostics, especially as an assistance system. © Copyright © 2020 Jutzi, Krieghoff-Henning, Holland-Letz, Utikal, Hauschild, Schadendorf, Sondermann, Fröhling, Hekler, Schmitt, Maron and Brinker.","10.3389/fmed.2020.00233","acceptance; artificial intelligence; diagnostics; melanoma; online survey; patients view; skin cancer","117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086511919&doi=10.3389%2ffmed.2020.00233&partnerID=40&md5=d82112a0add75e6f7c9809cc794b456d"
"Multi-agent learning neural network and Bayesian model for real-time IoT skin detectors: a new evaluation and benchmarking methodology","Zaidan A.A.; Zaidan B.B.; Alsalem M.A.; Albahri O.S.; Albahri A.S.; Qahtan M.Y.","2020","1","1","0","0","0","First occurrence","0","","","","","","","This study aimed to develop a new methodology for evaluating and benchmarking a multi-agent learning neural network and Bayesian model for real-time skin detectors based on Internet of things (IoT) by using multi-criteria decision-making (MCDM). The novelty of this work is in the use of an evaluation matrix for the performance evaluation of real-time skin detectors that are based on IoT. Nevertheless, an issue with the performance evaluation of real-time skin detector approaches is the determination of sensible criteria for performance metrics and the trade-off amongst them on the basis of different colour spaces. An experiment was conducted on the basis of three phases. In the first phase, a real-time camera based on cloud IoT was used to gather different caption images. The second phase could be divided into two stages. In the first stage, a skin detection approach was developed by applying multi-agent learning based on different colour spaces. This stage aimed to create a decision matrix of various colour spaces and three groups of criteria (i.e. reliability, time complexity and error rate within a dataset) for testing and evaluating the developed skin detection approaches. In the second stage, Pearson rules were utilised to calculate the correlation between the criteria in order to make sure, either needs to use all of the criteria in decision matrix and the criteria facts that affect the behaviour of each criterion, in order to make sure that use all the criteria in evaluation as multidimensional measurements or not. In the third phase, the MCDM method was used by integrating between a technique in order of preference by similarity to the ideal solution and multi-layer analytic hierarchy process to benchmark numerous real-time IoT skin detection approaches based on the performed decision matrix from the second phase. Three groups of findings were obtained. Firstly, (1) statistically significant differences were found between the criteria that emphasise the need to use all of the criteria in evaluation. (2) The behaviour of the criteria in all scenarios was affected by the distribution of threshold values for each criterion based on the different colour spaces used. Therefore, the differences in the behaviour of criteria that highlight the use of the criteria in evaluation were included as multidimensional measurements. Secondly, an overall comparison of external and internal aggregation values in selecting the best colour space, namely the normalised RGB at the sixth threshold, was discussed. Thirdly, (1) the YIQ colour space had the lowest value and was the worst case, whereas the normalised RGB had the highest value and was the most recommended of all spaces. (2) The lowest threshold was obtained at 0.5, whereas the best value was 0.9. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","10.1007/s00521-019-04325-3","Evaluation and benchmarking multi-criteria analysis; Multi-criteria decision-making techniques; Skin detector within IoT","60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066298799&doi=10.1007%2fs00521-019-04325-3&partnerID=40&md5=93a5af3f140950ac2f66c1f657e4ff5d"
"Effects of Label Noise on Deep Learning-Based Skin Cancer Classification","Hekler A.; Kather J.N.; Krieghoff-Henning E.; Utikal J.S.; Meier F.; Gellrich F.F.; Upmeier zu Belzen J.; French L.; Schlager J.G.; Ghoreschi K.; Wilhelm T.; Kutzner H.; Berking C.; Heppt M.V.; Haferkamp S.; Sondermann W.; Schadendorf D.; Schilling B.; Izar B.; Maron R.; Schmitt M.; Fröhling S.; Lipka D.B.; Brinker T.J.","2020","1","1","0","0","0","Unique","0","","","","","","","Recent studies have shown that deep learning is capable of classifying dermatoscopic images at least as well as dermatologists. However, many studies in skin cancer classification utilize non-biopsy-verified training images. This imperfect ground truth introduces a systematic error, but the effects on classifier performance are currently unknown. Here, we systematically examine the effects of label noise by training and evaluating convolutional neural networks (CNN) with 804 images of melanoma and nevi labeled either by dermatologists or by biopsy. The CNNs are evaluated on a test set of 384 images by means of 4-fold cross validation comparing the outputs with either the corresponding dermatological or the biopsy-verified diagnosis. With identical ground truths of training and test labels, high accuracies with 75.03% (95% CI: 74.39–75.66%) for dermatological and 73.80% (95% CI: 73.10–74.51%) for biopsy-verified labels can be achieved. However, if the CNN is trained and tested with different ground truths, accuracy drops significantly to 64.53% (95% CI: 63.12–65.94%, p < 0.01) on a non-biopsy-verified and to 64.24% (95% CI: 62.66–65.83%, p < 0.01) on a biopsy-verified test set. In conclusion, deep learning methods for skin cancer classification are highly sensitive to label noise and future work should use biopsy-verified training images to mitigate this problem. © Copyright © 2020 Hekler, Kather, Krieghoff-Henning, Utikal, Meier, Gellrich, Upmeier zu Belzen, French, Schlager, Ghoreschi, Wilhelm, Kutzner, Berking, Heppt, Haferkamp, Sondermann, Schadendorf, Schilling, Izar, Maron, Schmitt, Fröhling, Lipka and Brinker.","10.3389/fmed.2020.00177","artificial intelligence; dermatology; label noise; melanoma; nevi; skin cancer","50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085189217&doi=10.3389%2ffmed.2020.00177&partnerID=40&md5=762b76577ac8ae6ae6b1c664dab1a686"
"High-throughput quantitative histology in systemic sclerosis skin disease using computer vision","Correia C.; Mawe S.; Lofgren S.; Marangoni R.G.; Lee J.; Saber R.; Aren K.; Cheng M.; Teaw S.; Hoffmann A.; Goldberg I.; Cowper S.E.; Khatri P.; Hinchcliff M.; Mahoney J.M.","2020","1","1","0","0","0","First occurrence","0","","","","","","","Background: Skin fibrosis is the clinical hallmark of systemic sclerosis (SSc), where collagen deposition and remodeling of the dermis occur over time. The most widely used outcome measure in SSc clinical trials is the modified Rodnan skin score (mRSS), which is a semi-quantitative assessment of skin stiffness at seventeen body sites. However, the mRSS is confounded by obesity, edema, and high inter-rater variability. In order to develop a new histopathological outcome measure for SSc, we applied a computer vision technology called a deep neural network (DNN) to stained sections of SSc skin. We tested the hypotheses that DNN analysis could reliably assess mRSS and discriminate SSc from normal skin. Methods: We analyzed biopsies from two independent (primary and secondary) cohorts. One investigator performed mRSS assessments and forearm biopsies, and trichrome-stained biopsy sections were photomicrographed. We used the AlexNet DNN to generate a numerical signature of 4096 quantitative image features (QIFs) for 100 randomly selected dermal image patches/biopsy. In the primary cohort, we used principal components analysis (PCA) to summarize the QIFs into a Biopsy Score for comparison with mRSS. In the secondary cohort, using QIF signatures as the input, we fit a logistic regression model to discriminate between SSc vs. control biopsy, and a linear regression model to estimate mRSS, yielding Diagnostic Scores and Fibrosis Scores, respectively. We determined the correlation between Fibrosis Scores and the published Scleroderma Skin Severity Score (4S) and between Fibrosis Scores and longitudinal changes in mRSS on a per patient basis. Results: In the primary cohort (n = 6, 26 SSc biopsies), Biopsy Scores significantly correlated with mRSS (R = 0.55, p = 0.01). In the secondary cohort (n = 60 SSc and 16 controls, 164 biopsies; divided into 70% training and 30% test sets), the Diagnostic Score was significantly associated with SSc-status (misclassification rate = 1.9% [training], 6.6% [test]), and the Fibrosis Score significantly correlated with mRSS (R = 0.70 [training], 0.55 [test]). The DNN-derived Fibrosis Score significantly correlated with 4S (R = 0.69, p = 3 × 10- 17). Conclusions: DNN analysis of SSc biopsies is an unbiased, quantitative, and reproducible outcome that is associated with validated SSc outcomes. © 2020 The Author(s).","10.1186/s13075-020-2127-0","AlexNet; Computer vision; Deep neural network; Histology; Modified Rodnan skin score; Outcome measures; Outcomes; Quantitative image features; Scleroderma; Systemic sclerosis","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081739030&doi=10.1186%2fs13075-020-2127-0&partnerID=40&md5=c62461db5ea7124dcc2a5ade4bff2bed"
"Development and clinical implementation of semi-automated treatment planning including 3D printable applicator holders in complex skin brachytherapy","Guthier C.V.; Devlin P.M.; Harris T.C.; O’Farrell D.A.; Cormack R.A.; Buzurovic I.","2020","1","1","0","0","0","Unique","0","","","","","","","Purpose: High-dose-rate brachytherapy (HDR-BT) is a treatment option for malignant skin diseases compared to external beam radiation therapy, HDR-BT provides improved target coverage, better organ sparing, and has comparable treatment times. This is especially true for large clinical targets with complex topologies. To standardize and improve the quality and efficacy of the treatments, a novel streamlined treatment approach in complex skin HDR-BT was developed and implemented. This approach consists of auto generated treatment plans and a 3D printable applicator holder (3D-AH). Materials and methods: The in-house developed planning system automatically segments computed tomography simulation images (a), optimizes a treatment plan (b), and generates a model of the 3D-AH (c). The 3D-AH is used as an immobilization device for the flexible Freiburg flap applicator used to deliver treatment. The developed, automated planning is compared against the standard clinical plan generation process for a flat 10 × 10 cm2 field, curved fields with radii of 4, 6, and 8 cm, and a representative clinical case. The quality of the 3D print is verified via an additional CT of the flap applicator latched into the holder, followed by an automated rigid registration with the original planning CT. Finally, the methodology is implemented and tested clinically under an IRB approval. Results: All automatically generated plans were reviewed and accepted for clinical use. For the clinical workflow, the coverage achieved at a prescription depth for the flat 4, 6, and 8 cm applicator was (100.0 ± 4.9)%, (100.0 ± 4.9)%, (96.0 ± 0.3)%, and (98.4 ± 0.3)%, respectively. For auto planning, the coverage was (99.9 ± 0.3)%, (100.0 ± 0.2)%, (100.0 ± 0.3)%, and (100.1 ± 0.2)%. For the clinical test case, the D90 for the clinical workflow and auto planning was found to be 93.5% and 100.29% of the prescribed dose, respectively. Processing of the patient's CT to generate trajectories and positions as well as the 3D model of the applicator took <5 min. Conclusion: This workflow automates time intensive catheter digitizing and treatment planning. Compared to printing full applicators, the use of 3D-AH reduces the complexity of the 3D prints, the amount of the material to be used, the time of 3D printing, and amount of quality assurance required. The proposed methodology improves the overall treatment plan quality in complex HDR-BT and impact patient treatment outcomes potentially. © 2019 American Association of Physicists in Medicine","10.1002/mp.13975","3D-printing; automated treatment planning; brachytherapy; inverse planning","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077979903&doi=10.1002%2fmp.13975&partnerID=40&md5=4337c57abfe965b2ecb79bfa8b6108fa"
"Deep learning approaches towards skin lesion segmentation and classification from dermoscopic images - a review","Baig R.; Bibi M.; Hamid A.; Kausar S.; Khalid S.","2020","1","1","0","0","0","Unique","0","","","","","","","Background: Automated intelligent systems for unbiased diagnosis are primary requirement for the pigment lesion analysis. It has gained the attention of researchers in the last few decades. These systems involve multiple phases such as pre-processing, feature extraction, segmentation, classification and post processing. It is crucial to accurately localize and segment the skin lesion. It is observed that recent enhancements in machine learning algorithms and dermoscopic techniques reduced the misclassification rate therefore, the focus towards computer aided systems increased exponentially in recent years. Computer aided diagnostic systems are reliable source for dermatologists to analyze the type of cancer, but it is widely acknowledged that even higher accuracy is needed for computer aided diagnostic systems to be adopted practically in the diagnostic process of life threatening diseases. Introduction: Skin cancer is one of the most threatening cancers. It occurs by the abnormal multiplication of cells. The core three types of skin cells are: Squamous, Basal and Melanocytes. There are two wide classes of skin cancer; Melanocytic and non-Melanocytic. It is difficult to differentiate between benign and malignant melanoma, therefore dermatologists sometimes misclassify the benign and malignant melanoma. Melanoma is estimated as 19th most frequent cancer, it is riskier than the Basel and Squamous carcinoma because it rapidly spreads throughout the body. Hence, to lower the death risk, it is critical to diagnose the correct type of cancer in early rudimentary phases. It can occur on any part of body, but it has higher probability to occur on chest, back and legsMethods: The paper. presents a systematic review of segmentation and classification techniques for skin lesion detection. Dermoscopy and its features are discussed briefly. After that Image preprocessing techniques are described. A thorough review of Segmentation and Classification phases of skin lesion detection using deep learning techniques is presented Literature is discussed and a comparative analysis of discussed methods is presented. Conclusion: In this paper, we have presented the survey of more than 100 papers and comparative analysis of state of the art techniques, model and methodologies. Malignant melanoma is one of the most threating and deadliest cancers. Since the last few decades, researchers are putting extra attention and effort in accurate diagnosis of melanoma. The main challenges of dermoscopic skin lesion images are: low contrasts, multiple lesions, irregular and fuzzy borders, blood vessels, regression, hairs, bubbles, variegated coloring and other kinds of distortions. The lack of large training dataset makes these problems even more challenging. Due to recent advancement in the paradigm of deep learning, and specially the outstanding performance in medical imaging, it has become important to review the deep learning algorithms performance in skin lesion segmentation. Here, we have discussed the results of different techniques on the basis of different evaluation parameters such as Jaccard coefficient, sensitivity, specificity and accuracy. And the paper listed down the major achievements in this domain with the detailed discussion of the techniques. In future, it is expected to improve results by utilizing the capabilities of deep learning frameworks with other pre and post processing techniques so reliable and accurate diagnostic systems can be built. © 2020, Bentham Science Publishers. All rights reserved.","10.2174/1573405615666190129120449","Convolution neural network; Deep learning; Melanoma; Segmentation; Skin cancer; Skin lesion","48","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083425087&doi=10.2174%2f1573405615666190129120449&partnerID=40&md5=100108e98a785b1aa5e3abf7b345d220"
"Deep learning-based, computer-aided classifier developed with dermoscopic images shows comparable performance to 164 dermatologists in cutaneous disease diagnosis in the Chinese population","Wang S.-Q.; Zhang X.-Y.; Liu J.; Tao C.; Zhu C.-Y.; Shu C.; Xu T.; Jin H.-Z.","2020","0","1","0","0","0","Unique","0","","","","","","","Background: Diagnoses of Skin diseases are frequently delayed in China due to lack of dermatologists. A deep learning-based diagnosis supporting system can facilitate pre-screening patients to prioritize dermatologists’ efforts. We aimed to evaluate the classification sensitivity and specificity of deep learning models to classify skin tumors and psoriasis for Chinese population with a modest number of dermoscopic images. Methods: We developed a convolutional neural network (CNN) based on two datasets from a consecutive series of patients who underwent the dermoscopy in the clinic of the Department of Dermatology, Peking Union Medical College Hospital, between 2016 and 2018, prospectively. In order to evaluate the feasibility of the algorithm, we used two datasets. Dataset I consisted of 7192 dermoscopic images for a multi-class model to differentiate three most common skin tumors and other diseases. Dataset II consisted of 3115 dermoscopic images for a two-class model to classify psoriasis from other inflammatory diseases. We compared the performance of CNN with 164 dermatologists in a reader study with 130 dermoscopic images. The experts’ consensus was used as the reference standard except for the cases of basal cell carcinoma (BCC), which were all confirmed by histopathology. Results: The accuracies of multi-class and two-class models were 81.49% ± 0.88% and 77.02% ± 1.81%, respectively. In the reader study, for the multi-class tasks, the diagnosis sensitivity and specificity of 164 dermatologists were 0.770 and 0.962 for BCC, 0.807 and 0.897 for melanocytic nevus, 0.624 and 0.976 for seborrheic keratosis, 0.939 and 0.875 for the “others” group, respectively; the diagnosis sensitivity and specificity of multi-class CNN were 0.800 and 1.000 for BCC, 0.800 and 0.840 for melanocytic nevus, 0.850 and 0.940 for seborrheic keratosis, 0.750 and 0.940 for the “others” group, respectively. For the two-class tasks, the sensitivity and specificity of dermatologists and CNN for classifying psoriasis were 0.872 and 0.838, 1.000 and 0.605, respectively. Both the dermatologists and CNN achieved at least moderate consistency with the reference standard, and there was no significant difference in Kappa coefficients between them (P > 0.05). Conclusions: The performance of CNN developed with relatively modest number of dermoscopic images of skin tumors and psoriasis for Chinese population is comparable with 164 dermatologists. These two models could be used for screening in patients suspected with skin tumors and psoriasis respectively in primary care hospital. Copyright © 2020 The Chinese Medical Association, produced by Wolters Kluwer, Inc. under the CC-BY-NC-ND license. This is an open access article distributed under the terms of the Creative Commons Attribution-Non Commercial-No Derivatives License 4.0 (CCBY-NC-ND), where it is permissible to download and share the work provided it is properly cited. The work cannot be changed in any way or used commercially without permission from the journal.","10.1097/CM9.0000000000001023","Artificial intelligence; Convolutional neural network; Dermoscopy; Psoriasis; Skin tumor","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090509706&doi=10.1097%2fCM9.0000000000001023&partnerID=40&md5=b9a93851cbec7a9d0b03900b3b578cc3"
"Blue-White Veil Classification in Dermoscopy Images of the Skin Lesions Using Convolutional Neural Networks","Milczarski P.; Wąs Ł.","2020","1","1","0","0","0","Unique","0","","","","","","","In the dermatology, Three-Point Checklist of Dermatology is defined and it is proved to be a sufficient screening method in the skin lesions assessments during the checking by dermatology expert. In the method there is a criterion of blue-whitish veil appearance within the lesion defined and it can be classified using a binary classifier. In the paper, we show the results of CNN application to the problem of the assessment of whether the blue-white veil is present or absent within the lesion using the pre-trained VGG19 CNN network, trained and tested on the prepared images taken from the PH2 dataset. © 2020, Springer Nature Switzerland AG.","10.1007/978-3-030-61401-0_59","CNN; Dermoscopy; Pre-trained convolutional neural networks; Skin lesion asymmetry; VGG19","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096533262&doi=10.1007%2f978-3-030-61401-0_59&partnerID=40&md5=26e263073d7a344c75616605ca6f2f99"
"Keratinocytic Skin Cancer Detection on the Face Using Region-Based Convolutional Neural Network","Han S.S.; Moon I.J.; Lim W.; Suh I.S.; Lee S.Y.; Na J.-I.; Kim S.H.; Chang S.E.","2020","1","1","0","0","0","Unique","0","","","","","","","Importance: Detection of cutaneous cancer on the face using deep-learning algorithms has been challenging because various anatomic structures create curves and shades that confuse the algorithm and can potentially lead to false-positive results. Objective: To evaluate whether an algorithm can automatically locate suspected areas and predict the probability of a lesion being malignant. Design, Setting, and Participants: Region-based convolutional neural network technology was used to create 924538 possible lesions by extracting nodular benign lesions from 182348 clinical photographs. After manually or automatically annotating these possible lesions based on image findings, convolutional neural networks were trained with 1106886 image crops to locate and diagnose cancer. Validation data sets (2844 images from 673 patients; mean [SD] age, 58.2 [19.9] years; 308 men [45.8%]; 185 patients with malignant tumors, 305 with benign tumors, and 183 free of tumor) were obtained from 3 hospitals between January 1, 2010, and September 30, 2018. Main Outcomes and Measures: The area under the receiver operating characteristic curve, F1 score (mean of precision and recall; range, 0.000-1.000), and Youden index score (sensitivity + specificity-1; 0%-100%) were used to compare the performance of the algorithm with that of the participants. Results: The algorithm analyzed a mean (SD) of 4.2 (2.4) photographs per patient and reported the malignancy score according to the highest malignancy output. The area under the receiver operating characteristic curve for the validation data set (673 patients) was 0.910. At a high-sensitivity cutoff threshold, the sensitivity and specificity of the model with the 673 patients were 76.8% and 90.6%, respectively. With the test partition (325 images; 80 patients), the performance of the algorithm was compared with the performance of 13 board-certified dermatologists, 34 dermatology residents, 20 nondermatologic physicians, and 52 members of the general public with no medical background. When the disease screening performance was evaluated at high sensitivity areas using the F1 score and Youden index score, the algorithm showed a higher F1 score (0.831 vs 0.653 [0.126], P <.001) and Youden index score (0.675 vs 0.417 [0.124], P <.001) than that of nondermatologic physicians. The accuracy of the algorithm was comparable with that of dermatologists (F1 score, 0.831 vs 0.835 [0.040]; Youden index score, 0.675 vs 0.671 [0.100]). Conclusions and Relevance: The results of the study suggest that the algorithm could localize and diagnose skin cancer without preselection of suspicious lesions by dermatologists. © 2019 American Medical Association. All rights reserved.","10.1001/jamadermatol.2019.3807","","106","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076142978&doi=10.1001%2fjamadermatol.2019.3807&partnerID=40&md5=d95abb79ee610384cdf1877303c2c726"
"A convolutional neural network trained with dermoscopic images of psoriasis performed on par with 230 dermatologists","Yang Y.; Wang J.; Xie F.; Liu J.; Shu C.; Wang Y.; Zheng Y.; Zhang H.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Psoriasis is a common chronic inflammatory skin disease that causes physical and psychological burden to patients. A Convolutional Neural Network (CNN) focused on dermoscopic images would substantially aid the classification and increase the accuracy of diagnosis of psoriasis. Objectives: This study aimed to train an efficient deep-learning network to recognize dermoscopic images of psoriasis (and other papulosquamous diseases), improving the accuracy of the diagnosis of psoriasis. Methods: EfficientNet-B4 architecture was trained with 7033 dermoscopic images from 1166 patients collected from the Department of Dermatology, Peking Union Medical College Hospital (China). We performed a five-fold cross-validation on the training set to compare the classification performance of EfficientNet-B4 over different networks commonly used in previous studies. From the test set, 90 images were used to compare the performance between our four-class model and that of board-certified dermatologists, whose diagnoses and information (e.g., age, titles) were obtained through an online questionnaire. Results: The mean sensitivity and specificity of EfficientNet-B4 on the training set was 0.927± 0.028 and 0.827 ± 0.043 for the two-class task, and 0.889 ± 0.014 and 0.968 ± 0.004 four-class task. The diagnostic sensitivity and specificity of the 230 dermatologists were 0.688 and 0.903 for psoriasis, 0.677 and 0.838 for eczema, 0.669 and 0.953 for lichen planus, and 0.832 and 0.932 for the “others” group, respectively; the diagnostic sensitivity and specificity of our four-class CNN was 0.929 and 0.952 for psoriasis, 0.773 and 0.926 for eczema, 0.933 and 0.960 for lichen planus, and 0.840 and 0.985 for the “others” group, respectively. Both the 230 dermatologists and CNN achieved at least moderate consistency with the reference standard, and there was no significant difference between them (P > 0.05). Conclusions: The two-classification and four-classification models of psoriasis established in our study could accurately classify papulosquamous skin diseases. They showed generally comparable performances to the average level of dermatologists and would provide a strong support for the diagnosis of psoriasis. © 2021 Elsevier Ltd","10.1016/j.compbiomed.2021.104924","Convolutional neural networks; Deep-learning; Dermoscopic images; Papulosquamous skin diseases; Psoriasis","38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117397089&doi=10.1016%2fj.compbiomed.2021.104924&partnerID=40&md5=5f7a1c21aec382322171dd49df340fea"
"The mathematics of erythema: Development of machine learning models for artificial intelligence assisted measurement and severity scoring of radiation induced dermatitis","Ranjan R.; Partl R.; Erhart R.; Kurup N.; Schnidar H.","2021","0","1","0","0","0","Unique","0","","","","","","","Although significant advancements in computer-aided diagnostics using artificial intelligence (AI) have been made, to date, no viable method for radiation-induced skin reaction (RISR) analysis and classification is available. The objective of this single-center study was to develop machine learning and deep learning approaches using deep convolutional neural networks (CNNs) for automatic classification of RISRs according to the Common Terminology Criteria for Adverse Events (CTCAE) grading system. ScarletredⓇ Vision, a novel and state-of-the-art digital skin imaging method capable of remote monitoring and objective assessment of acute RISRs was used to convert 2D digital skin images using the CIELAB color space and conduct SEV* measurements. A set of different machine learning and deep convolutional neural network-based algorithms has been explored for the automatic classification of RISRs. A total of 2263 distinct images from 209 patients were analyzed for training and testing the machine learning and CNN algorithms. For a 2-class problem of healthy skin (grade 0) versus erythema (grade ≥ 1), all machine learning models produced an accuracy of above 70%, and the sensitivity and specificity of erythema recognition were 67–72% and 72–83%, respectively. The CNN produced a test accuracy of 74%, sensitivity of 66%, and specificity of 83% for predicting healthy and erythema cases. For the severity grade prediction of a 3-class problem (grade 0 versus 1 versus 2), the overall test accuracy was 60–67%, and the sensitivities were 56–82%, 35–59%, and 65–72%, respectively. For estimating the severity grade of each class, the CNN obtained an accuracy of 73%, 66%, and 82%, respectively. Ensemble learning combines several individual predictions to obtain a better generalization performance. Furthermore, we exploited ensemble learning by deploying a CNN model as a meta-learner. The ensemble CNN based on bagging and majority voting shows an accuracy, sensitivity and specificity of 87%, 90%, and 82% for a 2-class problem, respectively. For a 3-class problem, the ensemble CNN shows an overall accuracy of 66%, while for each grade (0, 1, and 2) accuracies were 76%, 69%, and 87%, sensitivities were 70%, 57%, and 71%, and specificities were 78%, 75%, and 95%, respectively. This study is the first to focus on erythema in radiation-dermatitis and produces benchmark results using machine learning models. The outcome of this study validates that the proposed system can act as a pre-screening and decision support tool for oncologists or patients to provide fast, reliable, and efficient assessment of erythema grading. © 2021 The Authors","10.1016/j.compbiomed.2021.104952","Artificial intelligence; Deep learning; eHealth; Erythema; Machine learning; Radiation dermatitis; Scarletred®Vision; Standardized erythema value (SEV)","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118495447&doi=10.1016%2fj.compbiomed.2021.104952&partnerID=40&md5=f90e674471ca72c649d9a1d6ca79c860"
"Artificial intelligence for skin cancer detection: Scoping review","Takiddin A.; Schneider J.; Yang Y.; Abd-Alrazaq A.; Househ M.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Skin cancer is the most common cancer type affecting humans. Traditional skin cancer diagnosis methods are costly, require a professional physician, and take time. Hence, to aid in diagnosing skin cancer, artificial intelligence (AI) tools are being used, including shallow and deep machine learning-based methodologies that are trained to detect and classify skin cancer using computer algorithms and deep neural networks. Objective: The aim of this study was to identify and group the different types of AI-based technologies used to detect and classify skin cancer. The study also examined the reliability of the selected papers by studying the correlation between the data set size and the number of diagnostic classes with the performance metrics used to evaluate the models. Methods: We conducted a systematic search for papers using Institute of Electrical and Electronics Engineers (IEEE) Xplore, Association for Computing Machinery Digital Library (ACM DL), and Ovid MEDLINE databases following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews (PRISMA-ScR) guidelines. The studies included in this scoping review had to fulfill several selection criteria: being specifically about skin cancer, detecting or classifying skin cancer, and using AI technologies. Study selection and data extraction were independently conducted by two reviewers. Extracted data were narratively synthesized, where studies were grouped based on the diagnostic AI techniques and their evaluation metrics. Results: We retrieved 906 papers from the 3 databases, of which 53 were eligible for this review. Shallow AI-based techniques were used in 14 studies, and deep AI-based techniques were used in 39 studies. The studies used up to 11 evaluation metrics to assess the proposed models, where 39 studies used accuracy as the primary evaluation metric. Overall, studies that used smaller data sets reported higher accuracy. Conclusions: This paper examined multiple AI-based skin cancer detection models. However, a direct comparison between methods was hindered by the varied use of different evaluation metrics and image types. Performance scores were affected by factors such as data set size, number of diagnostic classes, and techniques. Hence, the reliability of shallow and deep models with higher accuracy scores was questionable since they were trained and tested on relatively small data sets of a few diagnostic classes. © Abdulrahman Takiddin, Jens Schneider, Yin Yang, Alaa Abd-Alrazaq, Mowafa Househ. Originally published in the Journal of Medical Internet Research (https://www.jmir.org), 24.11.2021. This is an open-access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in the Journal of Medical Internet Research, is properly cited. The complete bibliographic information, a link to the original publication on https://www.jmir.org/, as well as this copyright and license information must be included.","10.2196/22934","Artificial intelligence; Deep neural networks; Machine learning; Skin cancer; Skin lesion","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120402925&doi=10.2196%2f22934&partnerID=40&md5=7958bbff216fe98140428ce7da108aca"
"Design and Assessment of Convolutional Neural Network Based Methods for Vitiligo Diagnosis","Zhang L.; Mishra S.; Zhang T.; Zhang Y.; Zhang D.; Lv Y.; Lv M.; Guan N.; Hu X.S.; Chen D.Z.; Han X.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Today's machine-learning based dermatologic research has largely focused on pigmented/non-pigmented lesions concerning skin cancers. However, studies on machine-learning-aided diagnosis of depigmented non-melanocytic lesions, which are more difficult to diagnose by unaided eye, are very few. Objective: We aim to assess the performance of deep learning methods for diagnosing vitiligo by deploying Convolutional Neural Networks (CNNs) and comparing their diagnosis accuracy with that of human raters with different levels of experience. Methods: A Chinese in-house dataset (2,876 images) and a world-wide public dataset (1,341 images) containing vitiligo and other depigmented/hypopigmented lesions were constructed. Three CNN models were trained on close-up images in both datasets. The results by the CNNs were compared with those by 14 human raters from four groups: expert raters (>10 years of experience), intermediate raters (5–10 years), dermatology residents, and general practitioners. F1 score, the area under the receiver operating characteristic curve (AUC), specificity, and sensitivity metrics were used to compare the performance of the CNNs with that of the raters. Results: For the in-house dataset, CNNs achieved a comparable F1 score (mean [standard deviation]) with expert raters (0.8864 [0.005] vs. 0.8933 [0.044]) and outperformed intermediate raters (0.7603 [0.029]), dermatology residents (0.6161 [0.068]) and general practitioners (0.4964 [0.139]). For the public dataset, CNNs achieved a higher F1 score (0.9684 [0.005]) compared to the diagnosis of expert raters (0.9221 [0.031]). Conclusion: Properly designed and trained CNNs are able to diagnose vitiligo without the aid of Wood's lamp images and outperform human raters in an experimental setting. © Copyright © 2021 Zhang, Mishra, Zhang, Zhang, Zhang, Lv, Lv, Guan, Hu, Chen and Han.","10.3389/fmed.2021.754202","deep learning; diagnosis; machine learning; skin pigmentation; vitiligo","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118346579&doi=10.3389%2ffmed.2021.754202&partnerID=40&md5=d5a1f69c0de26df9026526c200a25b74"
"Skin cancer classification via convolutional neural networks: systematic review of studies involving human experts","Haggenmüller S.; Maron R.C.; Hekler A.; Utikal J.S.; Barata C.; Barnhill R.L.; Beltraminelli H.; Berking C.; Betz-Stablein B.; Blum A.; Braun S.A.; Carr R.; Combalia M.; Fernandez-Figueras M.-T.; Ferrara G.; Fraitag S.; French L.E.; Gellrich F.F.; Ghoreschi K.; Goebeler M.; Guitera P.; Haenssle H.A.; Haferkamp S.; Heinzerling L.; Heppt M.V.; Hilke F.J.; Hobelsberger S.; Krahl D.; Kutzner H.; Lallas A.; Liopyris K.; Llamas-Velasco M.; Malvehy J.; Meier F.; Müller C.S.L.; Navarini A.A.; Navarrete-Dechent C.; Perasole A.; Poch G.; Podlipnik S.; Requena L.; Rotemberg V.M.; Saggini A.; Sangueza O.P.; Santonja C.; Schadendorf D.; Schilling B.; Schlaak M.; Schlager J.G.; Sergon M.; Sondermann W.; Soyer H.P.; Starz H.; Stolz W.; Vale E.; Weyers W.; Zink A.; Krieghoff-Henning E.; Kather J.N.; von Kalle C.; Lipka D.B.; Fröhling S.; Hauschild A.; Kittler H.; Brinker T.J.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Multiple studies have compared the performance of artificial intelligence (AI)–based models for automated skin cancer classification to human experts, thus setting the cornerstone for a successful translation of AI-based tools into clinicopathological practice. Objective: The objective of the study was to systematically analyse the current state of research on reader studies involving melanoma and to assess their potential clinical relevance by evaluating three main aspects: test set characteristics (holdout/out-of-distribution data set, composition), test setting (experimental/clinical, inclusion of metadata) and representativeness of participating clinicians. Methods: PubMed, Medline and ScienceDirect were screened for peer-reviewed studies published between 2017 and 2021 and dealing with AI-based skin cancer classification involving melanoma. The search terms skin cancer classification, deep learning, convolutional neural network (CNN), melanoma (detection), digital biomarkers, histopathology and whole slide imaging were combined. Based on the search results, only studies that considered direct comparison of AI results with clinicians and had a diagnostic classification as their main objective were included. Results: A total of 19 reader studies fulfilled the inclusion criteria. Of these, 11 CNN-based approaches addressed the classification of dermoscopic images; 6 concentrated on the classification of clinical images, whereas 2 dermatopathological studies utilised digitised histopathological whole slide images. Conclusions: All 19 included studies demonstrated superior or at least equivalent performance of CNN-based classifiers compared with clinicians. However, almost all studies were conducted in highly artificial settings based exclusively on single images of the suspicious lesions. Moreover, test sets mainly consisted of holdout images and did not represent the full range of patient populations and melanoma subtypes encountered in clinical practice. © 2021 The Author(s)","10.1016/j.ejca.2021.06.049","Artificial intelligence; Convolutional neural network(s); Deep learning; Dermatology; Digital biomarkers; Machine learning; Malignant melanoma; Skin cancer classification","200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110598997&doi=10.1016%2fj.ejca.2021.06.049&partnerID=40&md5=51cbe62048608fbf4401d8bd2c32d61a"
"Automated Extraction of Skin Wound Healing Biomarkers From In Vivo Label-Free Multiphoton Microscopy Using Convolutional Neural Networks","Jones J.D.; Rodriguez M.R.; Quinn K.P.","2021","1","1","0","0","0","Unique","0","","","","","","","Background and Objectives: Histological analysis is a gold standard technique for studying impaired skin wound healing. Label-free multiphoton microscopy (MPM) can provide natural image contrast similar to histological sections and quantitative metabolic information using NADH and FAD autofluorescence. However, MPM analysis requires time-intensive manual segmentation of specific wound tissue regions limiting the practicality and usage of the technology for monitoring wounds. The goal of this study was to train a series of convolutional neural networks (CNNs) to segment MPM images of skin wounds to automate image processing and quantification of wound geometry and metabolism. Study Design/Materials and Methods: Two CNNs with a 4-layer U-Net architecture were trained to segment unstained skin wound tissue sections and in vivo z-stacks of the wound edge. The wound section CNN used 380 distinct MPM images while the in vivo CNN used 5,848 with both image sets being randomly distributed to training, validation, and test sets following a 70%, 20%, and 10% split. The accuracy of each network was evaluated on the test set of images, and the effectiveness of automated measurement of wound geometry and optical redox ratio were compared with hand traced outputs of six unstained wound sections and 69 wound edge z-stacks from eight mice. Results: The MPM wound section CNN had an overall accuracy of 92.83%. Measurements of epidermal/dermal thickness, wound depth, wound width, and % re-epithelialization were within 10% error when evaluated on six full wound sections from days 3, 5, and 10 post-wounding that were not included in the training set. The in vivo wound z-stack CNN had an overall accuracy of 89.66% and was able to isolate the wound edge epithelium in z-stacks from eight mice across post-wound time points to quantify the optical redox ratio within 5% of what was recorded by manual segmentations. Conclusion: The CNNs trained and presented in this study can accurately segment MPM imaged wound sections and in vivo z-stacks to enable automated and rapid calculation of wound geometry and metabolism. Although MPM is a noninvasive imaging modality well suited to imaging living wound tissue, its use has been limited by time-intensive user segmentation. The use of CNNs for automated image segmentation demonstrate that it is possible for MPM to deliver near real-time quantitative readouts of tissue structure and function. Lasers Surg. Med. © 2021 Wiley Periodicals LLC. © 2021 Wiley Periodicals LLC","10.1002/lsm.23375","convolutional neural network; deep learning; in vivo; multiphoton microscopy; optical redox ratio; segmentation; wound healing","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099371131&doi=10.1002%2flsm.23375&partnerID=40&md5=e4192b726d186ccab44e511eeb85ab75"
"A benchmark for neural network robustness in skin cancer classification","Maron R.C.; Schlager J.G.; Haggenmüller S.; von Kalle C.; Utikal J.S.; Meier F.; Gellrich F.F.; Hobelsberger S.; Hauschild A.; French L.; Heinzerling L.; Schlaak M.; Ghoreschi K.; Hilke F.J.; Poch G.; Heppt M.V.; Berking C.; Haferkamp S.; Sondermann W.; Schadendorf D.; Schilling B.; Goebeler M.; Krieghoff-Henning E.; Hekler A.; Fröhling S.; Lipka D.B.; Kather J.N.; Brinker T.J.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: One prominent application for deep learning–based classifiers is skin cancer classification on dermoscopic images. However, classifier evaluation is often limited to holdout data which can mask common shortcomings such as susceptibility to confounding factors. To increase clinical applicability, it is necessary to thoroughly evaluate such classifiers on out-of-distribution (OOD) data. Objective: The objective of the study was to establish a dermoscopic skin cancer benchmark in which classifier robustness to OOD data can be measured. Methods: Using a proprietary dermoscopic image database and a set of image transformations, we create an OOD robustness benchmark and evaluate the robustness of four different convolutional neural network (CNN) architectures on it. Results: The benchmark contains three data sets—Skin Archive Munich (SAM), SAM-corrupted (SAM-C) and SAM-perturbed (SAM-P)—and is publicly available for download. To maintain the benchmark's OOD status, ground truth labels are not provided and test results should be sent to us for assessment. The SAM data set contains 319 unmodified and biopsy-verified dermoscopic melanoma (n = 194) and nevus (n = 125) images. SAM-C and SAM-P contain images from SAM which were artificially modified to test a classifier against low-quality inputs and to measure its prediction stability over small image changes, respectively. All four CNNs showed susceptibility to corruptions and perturbations. Conclusions: This benchmark provides three data sets which allow for OOD testing of binary skin cancer classifiers. Our classifier performance confirms the shortcomings of CNNs and provides a frame of reference. Altogether, this benchmark should facilitate a more thorough evaluation process and thereby enable the development of more robust skin cancer classifiers. © 2021 The Author(s)","10.1016/j.ejca.2021.06.047","Artificial intelligence; Benchmarking; Deep learning; Dermatology; Melanoma; Nevus; Skin neoplasms","55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110706464&doi=10.1016%2fj.ejca.2021.06.047&partnerID=40&md5=131225d3cb4e1d1b9e6635215ae8ced1"
"The classification of six common skin diseases based on xiangya-derm: Development of a chinese database for artificial intelligence","Huang K.; Jiang Z.; Li Y.; Wu Z.; Wu X.; Zhu W.; Chen M.; Zhang Y.; Zuo K.; Li Y.; Yu N.; Liu S.; Huang X.; Su J.; Yin M.; Qian B.; Wang X.; Chen X.; Zhao S.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Skin and subcutaneous disease is the fourth-leading cause of the nonfatal disease burden worldwide and constitutes one of the most common burdens in primary care. However, there is a severe lack of dermatologists, particularly in rural Chinese areas. Furthermore, although artificial intelligence (AI) tools can assist in diagnosing skin disorders from images, the database for the Chinese population is limited. Objective: This study aims to establish a database for AI based on the Chinese population and presents an initial study on six common skin diseases. Methods: Each image was captured with either a digital camera or a smartphone, verified by at least three experienced dermatologists and corresponding pathology information, and finally added to the Xiangya-Derm database. Based on this database, we conducted AI-assisted classification research on six common skin diseases and then proposed a network called Xy-SkinNet. Xy-SkinNet applies a two-step strategy to identify skin diseases. First, given an input image, we segmented the regions of the skin lesion. Second, we introduced an information fusion block to combine the output of all segmented regions. We compared the performance with 31 dermatologists of varied experiences. Results: Xiangya-Derm, as a new database that consists of over 150,000 clinical images of 571 different skin diseases in the Chinese population, is the largest and most diverse dermatological data set of the Chinese population. The AI-based six-category classification achieved a top 3 accuracy of 84.77%, which exceeded the average accuracy of dermatologists (78.15%). Conclusions: Xiangya-Derm, the largest database for the Chinese population, was created. The classification of six common skin conditions was conducted based on Xiangya-Derm to lay a foundation for product research. © 2021 Kai Huang, Zixi Jiang, Yixin Li, Zhe Wu, Xian Wu, Wu Zhu, Mingliang Chen, Yu Zhang, Ke Zuo, Yi Li, Nianzhou Yu, Siliang Liu, Xing Huang, Juan Su, Mingzhu Yin, Buyue Qian, Xianggui Wang, Xiang Chen, Shuang Zhao.","10.2196/26025","Artificial intelligence; Automatic auxiliary diagnoses; China; Classification; Convolutional neural network; Dermatology; Medical image processing; Skin; Skin disease","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115910640&doi=10.2196%2f26025&partnerID=40&md5=8e02909767fe242e0ae7c9d80c10701a"
"Quantitative Assessment of Age-dependent Changes in Porphyrins from Fluorescence Images of Ultraviolet Photography by Image Processing","Wu Y.; Akimoto M.; Igarashi H.; Shibagaki Y.; Tanaka T.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Skin is the first line of defense against harmful external environmental factors. Skin flora living on the skin surface impact skin health and skin disease. Bacteria, form part of the unique and complex skin micro-ecological system. For example, Propionibacterium acnes (P. acnes) is a member of the anaerobic organisms and is involved in the induction of skin acne. It produces porphyrins that absorb ultraviolet light and emit red fluorescence in response. As a result, fluorescence surveillance of the skin can be important in both the diagnosis of skin acne and the evaluation of therapeutic effects. Many different measurement methods for single skin biophysical properties have been reported. This study focused on the age-dependent changes in porphyrins for normal skin, and developed a novel algorithm to evaluate porphyrins using the fluorescence images by image processing quantitatively. Materials and methods: An extraction algorithm was proposed for the segmentation of porphyrin fluorescence images in OpenCV. The algorithm consisted primarily of preprocessing, conversion from RGB color space to HSV color space, and classification of fluorescence. There are 3595 healthy Japanese aged 16–85 years enrolled in the study and fluorescence images were acquired from their cheek sites under 375 nm UV-LED excitation. Age-related fluorescence variation was conducted applying the algorithm implemented. Results: A new extraction algorithm has been proposed with fluorescence image input and three indexes output, including the number of fluorescence, area of fluorescence, and mean intensity of fluorescence. Proposed algorithm was verified by three parameters, the accuracy, sensitivity, and precision, which refer to the ability of algorithm to detect the number of fluorescence correctly and repeatedly. The verification results were 71%, 72%, and 88% respectively, taking a validly fundamental step for skin health record and analysis. Furthermore, large-scale fluorescence image segmentation results revealed that similar trends were coming out for all three indexes in cheek as people get older. All the fluorescence number, area and mean intensity arrived at the highest at 30 years old and fell off since then. Conclusion: The number, area, and fluorescence intensity of porphyrins can be extracted well from fluorescence images with the proposed algorithm in the study, which has the potential to aid in thediagnosis of skin acne and predict skin conditions as an assisted tool. It is implicated that fluorescence status is influenced by age, which rises to the peak around 30 years old for normal cheek's skin. © 2021 The Author(s)","10.1016/j.pdpdt.2021.102388","Age; HSV color model; Image processing; P. acnes; Porphyrin; Portable device","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108418635&doi=10.1016%2fj.pdpdt.2021.102388&partnerID=40&md5=60cea2107e66eab6178f5a120ba2e7ba"
"Digital natives' preferences on mobile artificial intelligence apps for skin cancer diagnostics: Survey study","Haggenmüller S.; Krieghoff-Henning E.; Jutzi T.; Trapp N.; Kiehl L.; Utikal O.S.; Fabian S.; Brinker T.J.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Artificial intelligence (AI) has shown potential to improve diagnostics of various diseases, especially for early detection of skin cancer. Studies have yet to investigate the clear application of AI technology in clinical practice or determine the added value for younger user groups. Translation of AI-based diagnostic tools can only be successful if they are accepted by potential users. Young adults as digital natives may offer the greatest potential for successful implementation of AI into clinical practice, while at the same time, representing the future generation of skin cancer screening participants. Objective: We conducted an anonymous online survey to examine how and to what extent individuals are willing to accept AI-based mobile apps for skin cancer diagnostics. We evaluated preferences and relative influences of concerns, with a focus on younger age groups. Methods: We recruited participants below 35 years of age using three social media channels. Facebook, LinkedIn, and Xing. Descriptive analysis and statistical tests were performed to evaluate participants' attitudes toward mobile apps for skin examination. We integrated an adaptive choice-based conjoint to assess participants' preferences. We evaluated potential concerns using maximum difference scaling. Results: We included 728 participants in the analysis. The majority of participants (66.5%, 484/728; 95% CI 0.631-0.699) expressed a positive attitude toward the use of AI-based apps. In particular, participants residing in big cities or small towns (P=.02) and individuals that were familiar with the use of health or fitness apps (P=.02) were significantly more open to mobile diagnostic systems. Hierarchical Bayes estimation of the preferences of participants with a positive attitude (n=484) revealed that the use of mobile apps as an assistance system was preferred. Participants ruled out app versions with an accuracy of ≤65%, apps using data storage without encryption, and systems that did not provide background information about the decision-making process. However, participants did not mind their data being used anonymously for research purposes, nor did they object to the inclusion of clinical patient information in the decision-making process. Maximum difference scaling analysis for the negative-minded participant group (n=244) showed that data security, insufficient trust in the app, and lack of personal interaction represented the dominant concerns with respect to app use. Conclusions: The majority of potential future users below 35 years of age were ready to accept AI-based diagnostic solutions for early detection of skin cancer. However, for translation into clinical practice, the participants' demands for increased transparency and explainability of AI-based tools seem to be critical. Altogether, digital natives between 18 and 24 years and between 25 and 34 years of age expressed similar preferences and concerns when compared both to each other and to results obtained by previous studies that included other age groups. © 2021 Sarah Haggenmüller, Eva Krieghoff-Henning, Tanja Jutzi, Nicole Trapp, Lennard Kiehl, Jochen Sven Utikal, Sascha Fabian, Titus Josef Brinker.","10.2196/22909","Acceptance; Artificial intelligence; Concerns; Diagnostics; Digital natives; Online survey; Preferences; Skin cancer; Skin cancer screening","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114033935&doi=10.2196%2f22909&partnerID=40&md5=07d87dc78f6ccf576dda76c727a83b7c"
"Integrating patient data into skin cancer classification using convolutional neural networks: Systematic review","Höhn J.; Hekler A.; Krieghoff-Henning E.; Kather J.N.; Utikal J.S.; Meier F.; Gellrich F.F.; Hauschild A.; French L.; Schlager J.G.; Ghoreschi K.; Wilhelm T.; Kutzner H.; Heppt M.; Haferkamp S.; Sondermann W.; Schadendorf D.; Schilling B.; Maron R.C.; Schmitt M.; Jutzi T.; Fröhling S.; Lipka D.B.; Brinker T.J.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Recent years have been witnessing a substantial improvement in the accuracy of skin cancer classification using convolutional neural networks (CNNs). CNNs perform on par with or better than dermatologists with respect to the classification tasks of single images. However, in clinical practice, dermatologists also use other patient data beyond the visual aspects present in a digitized image, further increasing their diagnostic accuracy. Several pilot studies have recently investigated the effects of integrating different subtypes of patient data into CNN-based skin cancer classifiers. Objective: This systematic review focuses on the current research investigating the impact of merging information from image features and patient data on the performance of CNN-based skin cancer image classification. This study aims to explore the potential in this field of research by evaluating the types of patient data used, the ways in which the nonimage data are encoded and merged with the image features, and the impact of the integration on the classifier performance. Methods: Google Scholar, PubMed, MEDLINE, and ScienceDirect were screened for peer-reviewed studies published in English that dealt with the integration of patient data within a CNN-based skin cancer classification. The search terms skin cancer classification, convolutional neural network(s), deep learning, lesions, melanoma, metadata, clinical information, and patient data were combined. Results: A total of 11 publications fulfilled the inclusion criteria. All of them reported an overall improvement in different skin lesion classification tasks with patient data integration. The most commonly used patient data were age, sex, and lesion location. The patient data were mostly one-hot encoded. There were differences in the complexity that the encoded patient data were processed with regarding deep learning methods before and after fusing them with the image features for a combined classifier. Conclusions: This study indicates the potential benefits of integrating patient data into CNN-based diagnostic algorithms. However, how exactly the individual patient data enhance classification performance, especially in the case of multiclass classification problems, is still unclear. Moreover, a substantial fraction of patient data used by dermatologists remains to be analyzed in the context of CNN-based skin cancer classification. Further exploratory analyses in this promising field may optimize patient data integration into CNN-based skin cancer diagnostics for patients' benefits. © 2021 Journal of Medical Internet Research. All rights reserved.","10.2196/20708","Convolutional neural networks; Patient data; Skin cancer classification","55","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109168544&doi=10.2196%2f20708&partnerID=40&md5=43b3de2e55a95fa681383bb841674666"
"Combining CNN-based histologic whole slide image analysis and patient data to improve skin cancer classification","Höhn J.; Krieghoff-Henning E.; Jutzi T.B.; von Kalle C.; Utikal J.S.; Meier F.; Gellrich F.F.; Hobelsberger S.; Hauschild A.; Schlager J.G.; French L.; Heinzerling L.; Schlaak M.; Ghoreschi K.; Hilke F.J.; Poch G.; Kutzner H.; Heppt M.V.; Haferkamp S.; Sondermann W.; Schadendorf D.; Schilling B.; Goebeler M.; Hekler A.; Fröhling S.; Lipka D.B.; Kather J.N.; Krahl D.; Ferrara G.; Haggenmüller S.; Brinker T.J.","2021","1","0","0","0","0","Unique","0","","","","","","","Background: Clinicians and pathologists traditionally use patient data in addition to clinical examination to support their diagnoses. Objectives: We investigated whether a combination of histologic whole slides image (WSI) analysis based on convolutional neural networks (CNNs) and commonly available patient data (age, sex and anatomical site of the lesion) in a binary melanoma/nevus classification task could increase the performance compared with CNNs alone. Methods: We used 431 WSIs from two different laboratories and analysed the performance of classifiers that used the image or patient data individually or three common fusion techniques. Furthermore, we tested a naive combination of patient data and an image classifier: for cases interpreted as ‘uncertain’ (CNN output score <0.7), the decision of the CNN was replaced by the decision of the patient data classifier. Results: The CNN on its own achieved the best performance (mean ± standard deviation of five individual runs) with AUROC of 92.30% ± 0.23% and balanced accuracy of 83.17% ± 0.38%. While the classification performance was not significantly improved in general by any of the tested fusions, naive strategy of replacing the image classifier with the patient data classifier on slides with low output scores improved balanced accuracy to 86.72% ± 0.36%. Conclusion: In most cases, the CNN on its own was so accurate that patient data integration did not provide any benefit. However, incorporating patient data for lesions that were classified by the CNN with low ‘confidence’ improved balanced accuracy. © 2021 The Author(s)","10.1016/j.ejca.2021.02.032","Convolutional neural networks; Data fusion; Histologic whole slide images; Patient data; Skin cancer classification","79","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103699179&doi=10.1016%2fj.ejca.2021.02.032&partnerID=40&md5=e6202084346f1a5f134ccc77f9b64c3f"
"Development and Assessment of an Artificial Intelligence-Based Tool for Skin Condition Diagnosis by Primary Care Physicians and Nurse Practitioners in Teledermatology Practices","Jain A.; Way D.; Gupta V.; Gao Y.; De Oliveira Marinho G.; Hartford J.; Sayres R.; Kanada K.; Eng C.; Nagpal K.; Desalvo K.B.; Corrado G.S.; Peng L.; Webster D.R.; Dunn R.C.; Coz D.; Huang S.J.; Liu Y.; Bui P.; Liu Y.","2021","1","1","0","0","0","Unique","0","","","","","","","Importance: Most dermatologic cases are initially evaluated by nondermatologists such as primary care physicians (PCPs) or nurse practitioners (NPs). Objective: To evaluate an artificial intelligence (AI)-based tool that assists with diagnoses of dermatologic conditions. Design, Setting, and Participants: This multiple-reader, multiple-case diagnostic study developed an AI-based tool and evaluated its utility. Primary care physicians and NPs retrospectively reviewed an enriched set of cases representing 120 different skin conditions. Randomization was used to ensure each clinician reviewed each case either with or without AI assistance; each clinician alternated between batches of 50 cases in each modality. The reviews occurred from February 21 to April 28, 2020. Data were analyzed from May 26, 2020, to January 27, 2021. Exposures: An AI-based assistive tool for interpreting clinical images and associated medical history. Main Outcomes and Measures: The primary analysis evaluated agreement with reference diagnoses provided by a panel of 3 dermatologists for PCPs and NPs. Secondary analyses included diagnostic accuracy for biopsy-confirmed cases, biopsy and referral rates, review time, and diagnostic confidence. Results: Forty board-certified clinicians, including 20 PCPs (14 women [70.0%]; mean experience, 11.3 [range, 2-32] years) and 20 NPs (18 women [90.0%]; mean experience, 13.1 [range, 2-34] years) reviewed 1048 retrospective cases (672 female [64.2%]; median age, 43 [interquartile range, 30-56] years; 41920 total reviews) from a teledermatology practice serving 11 sites and provided 0 to 5 differential diagnoses per case (mean [SD], 1.6 [0.7]). The PCPs were located across 12 states, and the NPs practiced in primary care without physician supervision across 9 states. The NPs had a mean of 13.1 (range, 2-34) years of experience and practiced in primary care without physician supervision across 9 states. Artificial intelligence assistance was significantly associated with higher agreement with reference diagnoses. For PCPs, the increase in diagnostic agreement was 10% (95% CI, 8%-11%; P <.001), from 48% to 58%; for NPs, the increase was 12% (95% CI, 10%-14%; P <.001), from 46% to 58%. In secondary analyses, agreement with biopsy-obtained diagnosis categories of maglignant, precancerous, or benign increased by 3% (95% CI, -1% to 7%) for PCPs and by 8% (95% CI, 3%-13%) for NPs. Rates of desire for biopsies decreased by 1% (95% CI, 0-3%) for PCPs and 2% (95% CI, 1%-3%) for NPs; the rate of desire for referrals decreased by 3% (95% CI, 1%-4%) for PCPs and NPs. Diagnostic agreement on cases not indicated for a dermatologist referral increased by 10% (95% CI, 8%-12%) for PCPs and 12% (95% CI, 10%-14%) for NPs, and median review time increased slightly by 5 (95% CI, 0-8) seconds for PCPs and 7 (95% CI, 5-10) seconds for NPs per case. Conclusions and Relevance: Artificial intelligence assistance was associated with improved diagnoses by PCPs and NPs for 1 in every 8 to 10 cases, indicating potential for improving the quality of dermatologic care.. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.","10.1001/jamanetworkopen.2021.7249","","110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105042261&doi=10.1001%2fjamanetworkopen.2021.7249&partnerID=40&md5=79c5774a686c4e31ee9982ed849d8b7e"
"A Deep Learning Based Framework for Diagnosing Multiple Skin Diseases in a Clinical Environment","Zhu C.-Y.; Wang Y.-K.; Chen H.-P.; Gao K.-L.; Shu C.; Wang J.-C.; Yan L.-F.; Yang Y.-G.; Xie F.-Y.; Liu J.","2021","1","0","0","0","0","Unique","0","","","","","","","Background: Numerous studies have attempted to apply artificial intelligence (AI) in the dermatological field, mainly on the classification and segmentation of various dermatoses. However, researches under real clinical settings are scarce. Objectives: This study was aimed to construct a novel framework based on deep learning trained by a dataset that represented the real clinical environment in a tertiary class hospital in China, for better adaptation of the AI application in clinical practice among Asian patients. Methods: Our dataset was composed of 13,603 dermatologist-labeled dermoscopic images, containing 14 categories of diseases, namely lichen planus (LP), rosacea (Rosa), viral warts (VW), acne vulgaris (AV), keloid and hypertrophic scar (KAHS), eczema and dermatitis (EAD), dermatofibroma (DF), seborrheic dermatitis (SD), seborrheic keratosis (SK), melanocytic nevus (MN), hemangioma (Hem), psoriasis (Pso), port wine stain (PWS), and basal cell carcinoma (BCC). In this study, we applied Google's EfficientNet-b4 with pre-trained weights on ImageNet as the backbone of our CNN architecture. The final fully-connected classification layer was replaced with 14 output neurons. We added seven auxiliary classifiers to each of the intermediate layer groups. The modified model was retrained with our dataset and implemented using Pytorch. We constructed saliency maps to visualize our network's attention area of input images for its prediction. To explore the visual characteristics of different clinical classes, we also examined the internal image features learned by the proposed framework using t-SNE (t-distributed Stochastic Neighbor Embedding). Results: Test results showed that the proposed framework achieved a high level of classification performance with an overall accuracy of 0.948, a sensitivity of 0.934 and a specificity of 0.950. We also compared the performance of our algorithm with three most widely used CNN models which showed our model outperformed existing models with the highest area under curve (AUC) of 0.985. We further compared this model with 280 board-certificated dermatologists, and results showed a comparable performance level in an 8-class diagnostic task. Conclusions: The proposed framework retrained by the dataset that represented the real clinical environment in our department could accurately classify most common dermatoses that we encountered during outpatient practice including infectious and inflammatory dermatoses, benign and malignant cutaneous tumors. © Copyright © 2021 Zhu, Wang, Chen, Gao, Shu, Wang, Yan, Yang, Xie and Liu.","10.3389/fmed.2021.626369","artificial intelligence; convolutional neural networks; deep learning; dermatology; dermoscopy; skin diseases; skin imaging","68","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105134210&doi=10.3389%2ffmed.2021.626369&partnerID=40&md5=96255c5f847a8a0a5a62f02281f1deff"
"Novel transfer learning approach for medical imaging with limited labeled data","Alzubaidi L.; Al-Amidie M.; Al-Asadi A.; Humaidi A.J.; Al-Shamma O.; Fadhel M.A.; Zhang J.; Santamaría J.; Duan Y.","2021","0","1","0","0","0","Unique","0","","","","","","","Deep learning requires a large amount of data to perform well. However, the field of medical image analysis suffers from a lack of sufficient data for training deep learning models. Moreover, medical images require manual labeling, usually provided by human annotators coming from various backgrounds. More importantly, the annotation process is time-consuming, expensive, and prone to errors. Transfer learning was introduced to reduce the need for the annotation process by transferring the deep learning models with knowledge from a previous task and then by fine-tuning them on a relatively small dataset of the current task. Most of the methods of medical image classification employ transfer learning from pretrained models, e.g., ImageNet, which has been proven to be ineffective. This is due to the mismatch in learned features between the natural image, e.g., ImageNet, and medical images. Additionally, it results in the utilization of deeply elaborated models. In this paper, we propose a novel transfer learning approach to overcome the previous drawbacks by means of training the deep learning model on large unlabeled medical image datasets and by next transferring the knowledge to train the deep learning model on the small amount of labeled medical images. Additionally, we propose a new deep convolutional neural network (DCNN) model that combines recent advancements in the field. We conducted several experiments on two challenging medical imaging scenarios dealing with skin and breast cancer classification tasks. According to the reported results, it has been empirically proven that the proposed approach can significantly improve the performance of both classification scenarios. In terms of skin cancer, the proposed model achieved an F1-score value of 89.09% when trained from scratch and 98.53% with the proposed approach. Secondly, it achieved an accuracy value of 85.29% and 97.51%, respectively, when trained from scratch and using the proposed approach in the case of the breast cancer scenario. Finally, we concluded that our method can possibly be applied to many medical imaging problems in which a substantial amount of unlabeled image data is available and the labeled image data is limited. Moreover, it can be utilized to improve the performance of medical imaging tasks in the same domain. To do so, we used the pretrained skin cancer model to train on feet skin to classify them into two classes—either normal or abnormal (diabetic foot ulcer (DFU)). It achieved an F1-score value of 86.0% when trained from scratch, 96.25% using transfer learning, and 99.25% using double-transfer learning. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","10.3390/cancers13071590","Convolution neural network (CNN); Deep learning; Machine learning; Medical image analysis; Transfer learning","210","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103279104&doi=10.3390%2fcancers13071590&partnerID=40&md5=25963483590bdb8c52933af04fde4b15"
"A novel convolutional neural network for the diagnosis and classification of rosacea: Usability study","Zhao Z.; Wu C.-M.; Zhang S.; He F.; Liu F.; Wang B.; Huang Y.; Shi W.; Jian D.; Xie H.; Yeh C.-Y.; Li J.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Rosacea is a chronic inflammatory disease with variable clinical presentations, including transient flushing, fixed erythema, papules, pustules, and phymatous changes on the central face. Owing to the diversity in the clinical manifestations of rosacea, the lack of objective biochemical examinations, and nonspecificity in histopathological findings, accurate identification of rosacea is a big challenge. Artificial intelligence has emerged as a potential tool in the identification and evaluation of some skin diseases such as melanoma, basal cell carcinoma, and psoriasis. Objective: The objective of our study was to utilize a convolutional neural network (CNN) to differentiate the clinical photos of patients with rosacea (taken from 3 different angles) from those of patients with other skin diseases such as acne, seborrheic dermatitis, and eczema that could be easily confused with rosacea. Methods: In this study, 24,736 photos comprising of 18,647 photos of patients with rosacea and 6089 photos of patients with other skin diseases such as acne, facial seborrheic dermatitis, and eczema were included and analyzed by our CNN model based on ResNet-50. Results: The CNN in our study achieved an overall accuracy and precision of 0.914 and 0.898, with an area under the receiver operating characteristic curve of 0.972 for the detection of rosacea. The accuracy of classifying 3 subtypes of rosacea, that is, erythematotelangiectatic rosacea, papulopustular rosacea, and phymatous rosacea was 83.9%, 74.3%, and 80.0%, respectively. Moreover, the accuracy and precision of our CNN to distinguish rosacea from acne reached 0.931 and 0.893, respectively. For the differentiation between rosacea, seborrheic dermatitis, and eczema, the overall accuracy of our CNN was 0.757 and the precision was 0.667. Finally, by comparing the CNN diagnosis with the diagnoses by dermatologists of different expertise levels, we found that our CNN system is capable of identifying rosacea with a performance superior to that of resident doctors or attending physicians and comparable to that of experienced dermatologists. Conclusions: The findings of our study showed that by assessing clinical images, the CNN system in our study could identify rosacea with accuracy and precision comparable to that of an experienced dermatologist. © Zhixiang Zhao, Che-Ming Wu, Shuping Zhang, Fanping He, Fangfen Liu, Ben Wang, Yingxue Huang, Wei Shi, Dan Jian, Hongfu Xie, Chao-Yuan Yeh, Ji Li.","10.2196/23415","Artificial intelligence; Convolutional neural networks; Rosacea","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103617697&doi=10.2196%2f23415&partnerID=40&md5=373f42661f774f7a00100d582f2a7ef5"
"Reducing the impact of confounding factors on skin cancer classification via image segmentation: Technical Model Study","Maron R.C.; Hekler A.; Krieghoff-Henning E.; Schmitt M.; Schlager J.G.; Utikal J.S.; Brinker T.J.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: Studies have shown that artificial intelligence achieves similar or better performance than dermatologists in specific dermoscopic image classification tasks. However, artificial intelligence is susceptible to the influence of confounding factors within images (eg, skin markings), which can lead to false diagnoses of cancerous skin lesions. Image segmentation can remove lesion-adjacent confounding factors but greatly change the image representation. Objective: The aim of this study was to compare the performance of 2 image classification workflows where images were either segmented or left unprocessed before the subsequent training and evaluation of a binary skin lesion classifier. Methods: Separate binary skin lesion classifiers (nevus vs melanoma) were trained and evaluated on segmented and unsegmented dermoscopic images. For a more informative result, separate classifiers were trained on 2 distinct training data sets (human against machine [HAM] and International Skin Imaging Collaboration [ISIC]). Each training run was repeated 5 times. The mean performance of the 5 runs was evaluated on a multi-source test set (n=688) consisting of a holdout and an external component. Results: Our findings showed that when trained on HAM, the segmented classifiers showed a higher overall balanced accuracy (75.6% [SD 1.1%]) than the unsegmented classifiers (66.7% [SD 3.2%]), which was significant in 4 out of 5 runs (P<.001). The overall balanced accuracy was numerically higher for the unsegmented ISIC classifiers (78.3% [SD 1.8%]) than for the segmented ISIC classifiers (77.4% [SD 1.5%]), which was significantly different in 1 out of 5 runs (P=.004). Conclusions: Image segmentation does not result in overall performance decrease but it causes the beneficial removal of lesion-adjacent confounding factors. Thus, it is a viable option to address the negative impact that confounding factors have on deep learning models in dermatology. However, the segmentation step might introduce new pitfalls, which require further investigations. © 2021 Journal of Medical Internet Research. All rights reserved.","10.2196/21695","Artifacts; Artificial intelligence; Confounding factors; Deep learning; Dermatology; Diagnosis; Image segmentation; Melanoma; Neural networks; Nevus","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103512986&doi=10.2196%2f21695&partnerID=40&md5=8353ab2d737f03a4bcc4d741ec0147f2"
"Data augmentation for skin lesion using self-attention based progressive generative adversarial network","Abdelhalim I.S.A.; Mohamed M.F.; Mahdy Y.B.","2021","1","1","0","0","0","Unique","0","","","","","","","While recent years have witnessed the remarkable success of deep learning methods in automated skin lesion detection systems, there still exists a gap between manual assessment of experts and automated evaluation of computers. The reason behind such a gap is the deep learning models demand considerable amounts of data, while the availability of annotated images is often limited. Data Augmentation (DA) is one way to mitigate the lack of labeled data; however, the augmented images intrinsically have a similar distribution to the original ones, leading to limited performance improvement. To satisfy the data lack in the real image distribution, we synthesize skin lesion images – realistic but completely different from the original ones – using Generative Adversarial Networks (GANs). In this paper, we propose the Self-attention Progressive Growing of GANs (SPGGANs) to generate fine-grained 256 × 256 skin lesion images for Convolutional Neural Network-based melanoma detection, which is challenging via conventional GANs; difficulties arise due to unstable GAN training with high resolution and a variety of skin lesions in size, shape, and location. In SPGGAN, details can be generated using aggregated information from all feature locations. Moreover, the discriminator can monitor that highly detailed features in distant portions of the image are consistent with each other. Furthermore, the Two-Timescale Update Rule (TTUR) is applied to SPGGAN (SPGGAN-TTUR) to improve stability while generating 256 × 256 skin lesion images. SPGGAN-TTUR is evaluated on data generation and classification tasks using the HAM10000 dataset. Our results confirm the importance of our proposed GAN-based DA approach for training skin lesion classifiers and indicate that it can lead to statistically significant improvements (p-value <0.05) in the sensitivity (recall) over non-augmented and augmented, with classical DA, counterparts. In general, in the case of all classes, The sensitivity improvements were 5.6% and 2.5% over non-augmented and augmented (with the best DA scheme) counterparts, respectively. Specifically, in the case of melanoma class, the sensitivity improvements were 13.8% and 8.6%. We believe that the proposed approach can be adopted in clinical practice to improve the sensitivity of automated skin lesion detection in dermoscopic images and thus support dermatologists’ efforts to improve melanoma diagnosis. © 2020 Elsevier Ltd","10.1016/j.eswa.2020.113922","Data augmentation; Data imbalance; Deep learning; Generative models; Skin cancer","114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090417710&doi=10.1016%2fj.eswa.2020.113922&partnerID=40&md5=897b80d2bd7438f71195fd366bdb3bbd"
"Performance of a deep neural network in teledermatology: a single-centre prospective diagnostic study","Muñoz-López C.; Ramírez-Cornejo C.; Marchetti M.A.; Han S.S.; Del Barrio-Díaz P.; Jaque A.; Uribe P.; Majerson D.; Curi M.; Del Puerto C.; Reyes-Baraona F.; Meza-Romero R.; Parra-Cares J.; Araneda-Ortega P.; Guzmán M.; Millán-Apablaza R.; Nuñez-Mora M.; Liopyris K.; Vera-Kellet C.; Navarrete-Dechent C.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: The use of artificial intelligence (AI) algorithms for the diagnosis of skin diseases has shown promise in experimental settings but has not been yet tested in real-life conditions. Objective: To assess the diagnostic performance and potential clinical utility of a 174-multiclass AI algorithm in a real-life telemedicine setting. Methods: Prospective, diagnostic accuracy study including consecutive patients who submitted images for teledermatology evaluation. The treating dermatologist chose a single image to upload to a web application during teleconsultation. A follow-up reader study including nine healthcare providers (3 dermatologists, 3 dermatology residents and 3 general practitioners) was performed. Results: A total of 340 cases from 281 patients met study inclusion criteria. The mean (SD) age of patients was 33.7 (17.5) years; 63% (n = 177) were female. Exposure to the AI algorithm results was considered useful in 11.8% of visits (n = 40) and the teledermatologist correctly modified the real-time diagnosis in 0.6% (n = 2) of cases. The overall top-1 accuracy of the algorithm (41.2%) was lower than that of the dermatologists (60.1%), residents (57.8%) and general practitioners (49.3%) (all comparisons P < 0.05, in the reader study). When the analysis was limited to the diagnoses on which the algorithm had been explicitly trained, the balanced top-1 accuracy of the algorithm (47.6%) was comparable to the dermatologists (49.7%) and residents (47.7%) but superior to the general practitioners (39.7%; P = 0.049). Algorithm performance was associated with patient skin type and image quality. Conclusions: A 174-disease class AI algorithm appears to be a promising tool in the triage and evaluation of lesions with patient-taken photographs via telemedicine. © 2020 European Academy of Dermatology and Venereology","10.1111/jdv.16979","","49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096653421&doi=10.1111%2fjdv.16979&partnerID=40&md5=f78267056bb08f511e6fd55f27725c16"
"A new deep learning approach integrated with clinical data for the dermoscopic differentiation of early melanomas from atypical nevi","Tognetti L.; Bonechi S.; Andreini P.; Bianchini M.; Scarselli F.; Cevenini G.; Moscarella E.; Farnetani F.; Longo C.; Lallas A.; Carrera C.; Puig S.; Tiodorovic D.; Perrot J.L.; Pellacani G.; Argenziano G.; Cinotti E.; Cataldo G.; Balistreri A.; Mecocci A.; Gori M.; Rubegni P.; Cartocci A.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Timely recognition of malignant melanoma (MM) is challenging for dermatologists worldwide and represents the main determinant for mortality. Dermoscopic examination is influenced by dermatologists’ experience and fails to achieve adequate accuracy and reproducibility in discriminating atypical nevi (AN) from early melanomas (EM). Objective: We aimed to develop a Deep Convolutional Neural Network (DCNN) model able to support dermatologists in the classification and management of atypical melanocytic skin lesions (aMSL). Methods: A training set (630 images), a validation set (135) and a testing set (214) were derived from the idScore dataset of 979 challenging aMSL cases in which the dermoscopic image is integrated with clinical data (age, sex, body site and diameter) and associated with histological data. A DCNN_aMSL architecture was designed and then trained on both dermoscopic images of aMSL and the clinical/anamnestic data, resulting in the integrated “iDCNN_aMSL” model. Responses of 111 dermatologists with different experience levels on both aMSL classification (intuitive diagnosis) and management decisions (no/long follow-up; short follow-up; excision/preventive excision) were compared with the DCNNs models. Results: In the lesion classification study, the iDCNN_aMSL achieved the best accuracy, reaching an AUC = 90.3 %, SE = 86.5 % and SP = 73.6 %, compared to DCNN_aMSL (SE = 89.2 %, SP = 65.7 %) and intuitive diagnosis of dermatologists (SE = 77.0 %; SP = 61.4 %). Conclusions: The iDCNN_aMSL proved to be the best support tool for management decisions reducing the ratio of inappropriate excision. The proposed iDCNN_aMSL model can represent a valid support for dermatologists in discriminating AN from EM with high accuracy and for medical decision making by reducing their rates of inappropriate excisions. © 2020 Japanese Society for Investigative Dermatology","10.1016/j.jdermsci.2020.11.009","Cutaneous melanoma; Deep convolutional neural network; Deep learning; Dermoscopy; Integrated diagnosis; Non-invasive imaging","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098080028&doi=10.1016%2fj.jdermsci.2020.11.009&partnerID=40&md5=7c2f1b2d2662a357c2fb41b0158e0a01"
"Explainable skin lesion diagnosis using taxonomies","Barata C.; Celebi M.E.; Marques J.S.","2021","1","1","0","0","0","Unique","0","","","","","","","Deep neural networks have rapidly become an indispensable tool in many classification applications. However, the inclusion of deep learning methods in medical diagnostic systems has come at the cost of diminishing their explainability. This significantly reduces the safety of a diagnostic system, since the physician is unable to interpret and validate the output. Therefore, in this work we aim to address this major limitation and improve the explainability of a skin cancer diagnostic system. We propose to leverage two sources of information: (i) medical knowledge, in particular the taxonomic organization of skin lesions, which will be used to develop a hierarchical neural network; and (ii) recent advances in channel and spatial attention modules, which can identify interpretable features and regions in dermoscopy images. We demonstrate that the proposed approach achieves competitive results in two dermoscopy data sets (ISIC 2017 and 2018) and provides insightful information about its decisions, thus increasing the safety of the model. © 2020","10.1016/j.patcog.2020.107413","Channel attention; Explainability; Hierarchical deep learning; Safety-critical CADS; Skin cancer; Spatial attention","105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085040302&doi=10.1016%2fj.patcog.2020.107413&partnerID=40&md5=54f53f3e17323e806531e7da8bdcfd51"
"The Effects of Masking in Melanoma Image Classification with CNNs Towards International Standards for Image Preprocessing","Nunnari F.; Ezema A.; Sonntag D.","2021","0","1","0","0","0","First occurrence","0","","","","","","","The classification of skin lesion images is known to be biased by artifacts of the surrounding skin, but it is still not clear to what extent masking out healthy skin pixels influences classification performances, and why. To better understand this phenomenon, we apply different strategies of image masking (rectangular masks, circular masks, full masking, and image cropping) to three datasets of skin lesion images (ISIC2016, ISIC2018, and MedNode). We train CNN-based classifiers, provide performance metrics through a 10-fold cross-validation, and analyse the behaviour of Grad-CAM saliency maps through an automated visual inspection. Our experiments show that cropping is the best strategy to maintain classification performance and to significantly reduce training times as well. Our analysis through visual inspection shows that CNNs have the tendency to focus on pixels of healthy skin when no malignant features can be identified. This suggests that CNNs have the tendency of “eagerly” looking for pixel areas to justify a classification choice, potentially leading to biased discriminators. To mitigate this effect, and to standardize image preprocessing, we suggest to crop images during dataset construction or before the learning step. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.","10.1007/978-3-030-70569-5_16","AI standardization roadmap; Convolutional neural networks; Masking; Preprocessing; Reducing bias; Skin cancer","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104487178&doi=10.1007%2f978-3-030-70569-5_16&partnerID=40&md5=e5a3a9a3ea5008a51bb168adff04f300"
"Digital Diagnosis of Hand, Foot, and Mouth Disease Using Hybrid Deep Neural Networks","Verma S.; Razzaque M.A.; Sangtongdee U.; Arpnikanondt C.; Tassaneetrithep B.; Hossain A.","2021","0","1","0","0","0","First occurrence","0","","","","","","","Hand, Foot and Mouth Disease (HFMD) is a highly contagious paediatric disease showing up symptoms like fever, diarrhoea, oral ulcers and rashes on the hands and foot, and even in the mouth. This disease has become an epidemic with several outbreaks in many Asian-Pacific countries with the basic reproduction number $R_{0} > 1$. HFMD's diagnosis is very challenging as its lesion pattern may appear quite similar to other skin diseases such as herpangina, aseptic meningitis, and poliomyelitis. Therefore, clinical symptoms are essential besides skin lesion's pattern and position for precise diagnose of this disease. A deep learning-based HFMD detection system can play a significant role in the digital diagnosis of this disease. Various machine learning and deep learning architectures have been proposed for skin disease diagnosis and classification. However, these models are limited to the image classification problem. The diagnosis of similar appearing skin diseases using the image classification approach may result in misclassification or misdiagnosis of the disease. Parallel integration of clinical symptoms and images can improve disease diagnosis and classification performance. However, no deep learning architecture has been developed to diagnose HFMD disease from images and clinical data. This paper has proposed a novel Hybrid Deep Neural Networks integrating Multi-Layer Perceptron (MLP) network and Convolutional Neural Network into a single framework for the diagnosis of HFMD using the integrated features from clinical and image data. The proposed Hybrid Deep Neural Networks is particularly a multi branched model comprising of Multi-Layer Perceptron (MLP) network in the first branch to extract the clinical features and the modified pre-trained CNN architecture: MobileNet or NasNetMobile in the second branch to extract the features from skin disease lesion images. The features learnt from both the branches are merged to form an integrated feature from clinical data and images, which is fed to the subsequent classification network. We conducted several experiments employing image data only, clinical data only and both sources of data. The analyses compared and evaluated the performance of a typical MLP model and CNN model with our proposed Hybrid Deep Neural Networks. The novel approach promotes the existing image classification model and clinical symptoms based disease classification model, particularly the MLP model. From the cross-validated experiments, the results reveal that the proposed Hybrid Deep Neural Networks can diagnose the disease 99%-100% accurately. © 2013 IEEE.","10.1109/ACCESS.2021.3120199","CNN; convolution neural network; hand foot mouth and disease; HFMD; hybrid deep neural network; image classification; mobilenet; NasNetMobile; Thailand","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117840474&doi=10.1109%2fACCESS.2021.3120199&partnerID=40&md5=fff67e694292b13397ccee29ec5e8900"
"Medical and organizational approaches to early diagnosis of skin melanoma","Neretin E.Yu.; Kozlov S.V.; Zolotareva T.G.","2021","1","1","0","0","0","Unique","0","","","","","","","Introduction. The most significant problem is the early diagnosis of skin melanoma (SM). In many countries of the world, there is a constant increase in the incidence rate, and the organization of population screening can help solve this problem. Purpose of the study. Evaluation of the use of multi-agent technology in the diagnosis of SM. Material and methods. Study design: at the 1st stage, primary medical documentation was studied — Charts No. 090/y; 027-2/y, statistical reports of the Samara Regional Clinical Oncological Dispensary — Charts No. 7, No. 35, according to the results revealed at stage 2. There was developed and implemented multiagent technology for SM diagnostics, including various agents of both qualified and specialized levels, these were both individuals and teams of departments who worked in close contact: a public relations agent; artificial intelligence secondary prevention planning agent; agent for training doctors and nurses, patients in the basics of early diagnosis and assessing their level of training; an agent for evaluating performance indicators. Results. After introducing the multi-agent system, the indicator of the share of 1–2 stages of MC in 2010–2019. increased by 48.3% compared to the period 2000–2009 and outpaced the growth in the total number of patients with SM by 6.96%; from 2010 to 2019 the proportion of patients with SM who were actively identified began to increase; one-year mortality rate from 2010 to 2019 decreased in waves (y = 0.0003x5 – 0.0104x4 – 0.2647x3 + 1.4818x2 – 1.8942x + 10.585; R2 = 0.554). Conclusion. The use of multi-agent technology makes it possible to reduce the one-year mortality rate, to achieve a faster growth rate of the newly detected number of patients with an early stage of SM (stage 1–2) compared to the increase in the number of cases, to improve the indicators of early diagnosis, active detection of skin melanoma, which is a positive result. © AUTHORS, 2021.","10.47470/0044-197X-2021-65-6-557-564","Early diagnosis; Expert system; Multi-agent technology for the diagnosis; Skin melanoma; Skin melanoma database","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123791294&doi=10.47470%2f0044-197X-2021-65-6-557-564&partnerID=40&md5=5661890f970c3f47edea8422dd4b5909"
"DEVELOPMENT OF A COMPREHENSIVE METHOD FOR THE DERMATOSCOPIC IMAGES ANALYSIS OF THE FACIAL SKIN WITH ACNE","Selivanova K.G.; Trubitsin A.A.; Avrunin О.G.","2021","1","1","0","0","0","Unique","0","","","","","","","Background: One of the most common inflammatory chronic and recurrent skin diseases is acne (“acne vulgaris”), which appears itself as open or closed comedones and inflammatory skin lesions in the form of papules, pustules, nodes, etc. It has been established that acne is one of the most common dermatoses, since, according to modern data, it affects about 9.4% of the population. During adolescence, up to 90% of people suffer, and in adulthood — about 20% with varying degrees of severity. Currently, there are many approaches to treating this disease through various cosmetic treatments such as phototherapy, ultrasonic skin cleansing, mesotherapy, chemical peels, and medication. Therefore, the development of methods and means of differential diagnosis of acne is one of the urgent tasks in the field of biomedical engineering, dermatology, and clinical medicine, since this allows timely identification of the localization of the disease, its causes, and prescribing appropriate treatment. However, the solution to the problem of monitoring the dynamics of external manifestations of the disease is possible only with the use of combined mathematical methods for image analysis. Objectives: To develop a comprehensive method for analyzing dermatoscopic images for monitoring the external manifestations of acne disease during treatment and isolating the affected areas of the facial skin. Materials and Methods: Dermatological preclinical researches of the skin were conducted in the laboratory of 3D-biomedical technologies of the Department of Biomedical Engineering of the Kharkiv National University of Radio Electronics, using a digital videodermatoscope BIO Bm6+ in daylight and a portable skin analyzer Skin Scope F-102 in the ultraviolet range. Clinical researches were conducted based on the Department of Pediatric Propaedeutics #2 of the Kharkiv National Medical University. The development of a software tool for image analysis was conducted out in Python programming using the libraries OpenCV, Scikit-image, Numpy, PIL, Mathplotlib. Determination of the affected skin areas and calculation of the parameters of inflammation were carried out using multi-Otsu methods and morphological segmentation of digital dermatoscopic images. Results: During the research, automated software was developed that allows to analyze in dynamics the nature of inflammatory processes and the area of facial skin lesions, as well as to carry out a differential diagnosis of acne disease. The proposed method for the analysis of dermatoscopic images makes it possible to perform color segmentation and obtain a map of the gradations of skin inflammations to control the dynamics during the prescribed treatment. Conclusions: The comprehensive method of analysis of dermatoscopic images of the skin of the face makes it possible to effectively control the condition of the skin of the face from acne during treatment, while analyzing the degree of inflammatory processes and the area of lesions, where, using the developed software, in an automated mode, red gradations are calculated to detect the boundaries of inflammation, geometric parameters and percentage of lesions in relation to healthy facial skin. © Selivanova К. G., Trubitsin A. A., Avrunin O. G., 2021.","10.26565/2075-3810-2021-46-03","acne; color image segmentation; dermatology; dermatoscopic imaging; facial skin; multi-Otsu method; post-acne","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134680362&doi=10.26565%2f2075-3810-2021-46-03&partnerID=40&md5=bd8d3aee9c5cb3f0847308bd0b7d6de4"
"Machine learning techniques for mitoses classification","Nofallah S.; Mehta S.; Mercan E.; Knezevich S.; May C.J.; Weaver D.; Witten D.; Elmore J.G.; Shapiro L.","2021","0","1","0","0","0","Unique","0","","","","","","","Background: Pathologists analyze biopsy material at both the cellular and structural level to determine diagnosis and cancer stage. Mitotic figures are surrogate biomarkers of cellular proliferation that can provide prognostic information; thus, their precise detection is an important factor for clinical care. Convolutional Neural Networks (CNNs) have shown remarkable performance on several recognition tasks. Utilizing CNNs for mitosis classification may aid pathologists to improve the detection accuracy. Methods: We studied two state-of-the-art CNN-based models, ESPNet and DenseNet, for mitosis classification on six whole slide images of skin biopsies and compared their quantitative performance in terms of sensitivity, specificity, and F-score. We used raw RGB images of mitosis and non-mitosis samples with their corresponding labels as training input. In order to compare with other work, we studied the performance of these classifiers and two other architectures, ResNet and ShuffleNet, on the publicly available MITOS breast biopsy dataset and compared the performance of all four in terms of precision, recall, and F-score (which are standard for this data set), architecture, training time and inference time. Results: The ESPNet and DenseNet results on our primary melanoma dataset had a sensitivity of 0.976 and 0.968, and a specificity of 0.987 and 0.995, respectively, with F-scores of.968 and.976, respectively. On the MITOS dataset, ESPNet and DenseNet showed a sensitivity of 0.866 and 0.916, and a specificity of 0.973 and 0.980, respectively. The MITOS results using DenseNet had a precision of 0.939, recall of 0.916, and F-score of 0.927. The best published result on MITOS (Saha et al. 2018) reported precision of 0.92, recall of 0.88, and F-score of 0.90. In our architecture comparisons on MITOS, we found that DenseNet beats the others in terms of F-Score (DenseNet 0.927, ESPNet 0.890, ResNet 0.865, ShuffleNet 0.847) and especially Recall (DenseNet 0.916, ESPNet 0.866, ResNet 0.807, ShuffleNet 0.753), while ResNet and ESPNet have much faster inference times (ResNet 6 s, ESPNet 8 s, DenseNet 31 s). ResNet is faster than ESPNet, but ESPNet has a higher F-Score and Recall than ResNet, making it a good compromise solution. Conclusion: We studied several state-of-the-art CNNs for detecting mitotic figures in whole slide biopsy images. We evaluated two CNNs on a melanoma cancer dataset and then compared four CNNs on a public breast cancer data set, using the same methodology on both. Our methodology and architecture for mitosis finding in both melanoma and breast cancer whole slide images has been thoroughly tested and is likely to be useful for finding mitoses in any whole slide biopsy images. © 2020 Elsevier Ltd","10.1016/j.compmedimag.2020.101832","Convolutional neural networks; Machine learning; Melanoma; Mitoses; Pathology","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097453946&doi=10.1016%2fj.compmedimag.2020.101832&partnerID=40&md5=fa677c252bff3be428129299255301fd"
"Detail Study of Different Algorithms for Early Detection of Cancer","Dhar P.; Suganya Devi K.; Satti S.K.; Srinivasan P.","2021","0","1","0","0","0","Unique","0","","","","","","","Cancer is one of the most dangerous disease in human life. Diagnosing the cancer cell in early stages plays an important (valuable) role in saving human life and for successful treatment. At an early stage the spreading of cancer cells of the body can be stopped by removing the benign cells (cancer cells in the early stage). Whereas, malignant tumor (cancer cells in later stages) which are having aggressive spreading capacity, affects different other parts of the body and cannot be controlled. This paper presents a study on five different types of cancer viz., breast, brain, lung, liver and skin; and different published techniques of detecting these cancers, which help the students (researcher) to understand the current ongoing techniques and aids to develop new structure that gives better and accurate result. It also focuses on different segmentation (ACM, PSO, UNet, watershed etc), cancer feature extraction, cancer features reduction (PCA, LDA, SVD). Also, it discusses different cancer classification using machine learning and clustering (SVM, KNN, Bayesian, Neuro fuzzy, k-mean algorithm, GANs etc.), deep learning (CNN, ResNet, VGG etc) technique, also discuss about different evaluating method. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","10.1007/978-981-15-9735-0_12","Cancer classification clustering and deep learning; Cancer segmentation and feature extraction; Early cancer detection","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101073312&doi=10.1007%2f978-981-15-9735-0_12&partnerID=40&md5=ffca7e99750c33dcb9731d54cf043f9c"
"Improved diagnosis by automated macro- and micro-anatomical region mapping of skin photographs","Amruthalingam L.; Gottfrois P.; Gonzalez Jimenez A.; Gökduman B.; Kunz M.; Koller T.; Pouly M.; Navarini A.A.; Maul J.-T.; Maul L.V.; Kostner L.; Jamiolkowski D.; Erni B.; Hsu C.; Meienberger N.; Nicolas Khouri M.; Christiane Palm M.; Damian Wuethrich M.; Anliker M.; Manabu Rohr M.; Horvat M.; Eckert N.; Kei Mathis M.; Salvatore Conticello M.; Baskaralingam S.; Rotondi L.; Pascal Kobel M.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: The exact location of skin lesions is key in clinical dermatology. On one hand, it supports differential diagnosis (DD) since most skin conditions have specific predilection sites. On the other hand, location matters for dermatosurgical interventions. In practice, lesion evaluation is not well standardized and anatomical descriptions vary or lack altogether. Automated determination of anatomical location could benefit both situations. Objective: Establish an automated method to determine anatomical regions in clinical patient pictures and evaluate the gain in DD performance of a deep learning model (DLM) when trained with lesion locations and images. Methods: Retrospective study based on three datasets: macro-anatomy for the main body regions with 6000 patient pictures partially labelled by a student, micro-anatomy for the ear region with 182 pictures labelled by a student and DD with 3347 pictures of 16 diseases determined by dermatologists in clinical settings. For each dataset, a DLM was trained and evaluated on an independent test set. The primary outcome measures were the precision and sensitivity with 95% CI. For DD, we compared the performance of a DLM trained with lesion pictures only with a DLM trained with both pictures and locations. Results: The average precision and sensitivity were 85% (CI 84–86), 84% (CI 83–85) for macro-anatomy, 81% (CI 80–83), 80% (CI 77–83) for micro-anatomy and 82% (CI 78–85), 81% (CI 77–84) for DD. We observed an improvement in DD performance of 6% (McNemar test P-value 0.0009) for both average precision and sensitivity when training with both lesion pictures and locations. Conclusion: Including location can be beneficial for DD DLM performance. The proposed method can generate body region maps from patient pictures and even reach surgery relevant anatomical precision, e.g. the ear region. Our method enables automated search of large clinical databases and make targeted anatomical image retrieval possible. © 2022 The Authors. Journal of the European Academy of Dermatology and Venereology published by John Wiley & Sons Ltd on behalf of European Academy of Dermatology and Venereology.","10.1111/jdv.18476","","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137332058&doi=10.1111%2fjdv.18476&partnerID=40&md5=5fd3242cc88aa448e0cab4295e400131"
"Image segmentation of skin lesions based on dense atrous spatial pyramid pooling and attention mechanism","Yin W.; Zhou D.; Fan T.; Yu Z.; Li Z.","2022","1","1","0","0","0","Unique","0","","","","","","","皮肤是人体最大的器官，很多内脏疾病会直接体现在皮肤上，准确分割皮肤病灶图像具有重要的临床意义。针对皮肤病灶区域颜色复杂、边界模糊、尺度信息参差不齐等特点，本文提出一种基于密集空洞空间金字塔池化（DenseASPP）和注意力机制的皮肤病灶图像分割方法。该方法以U型网络（U-Net）为基础，首先重新设计新的编码器，以大量残差连接代替普通的卷积堆叠，在拓展网络深度后还能有效保留关键特征；其次，将通道注意力与空间注意力融合并加入残差连接，从而使网络自适应地学习图像的通道与空间特征；最后，引入并重新设计的DenseASPP以扩大感受野尺寸并获取多尺度特征信息。本文所提算法在国际皮肤影像协会官方公开数据集（ISIC2016）中得到令人满意的结果，平均交并比（mIOU）、敏感度（SE）、精确率（PC）、准确率（ACC）和戴斯相似性系数（Dice）分别为0.901 8、0.945 9、0.948 7、0.968 1、0.947 3。实验结果证明，本文方法能够提高皮肤病灶图像分割效果，有望能为专业皮肤病医生提供辅助诊断。.; The skin is the largest organ of the human body, and many visceral diseases will be directly reflected on the skin, so it is of great clinical significance to accurately segment the skin lesion images. To address the characteristics of complex color, blurred boundaries, and uneven scale information, a skin lesion image segmentation method based on dense atrous spatial pyramid pooling (DenseASPP) and attention mechanism is proposed. The method is based on the U-shaped network (U-Net). Firstly, a new encoder is redesigned to replace the ordinary convolutional stacking with a large number of residual connections, which can effectively retain key features even after expanding the network depth. Secondly, channel attention is fused with spatial attention, and residual connections are added so that the network can adaptively learn channel and spatial features of images. Finally, the DenseASPP module is introduced and redesigned to expand the perceptual field size and obtain multi-scale feature information. The algorithm proposed in this paper has obtained satisfactory results in the official public dataset of the International Skin Imaging Collaboration (ISIC 2016). The mean Intersection over Union (mIOU), sensitivity (SE), precision (PC), accuracy (ACC), and Dice coefficient (Dice) are 0.901 8, 0.945 9, 0.948 7, 0.968 1, 0.947 3, respectively. The experimental results demonstrate that the method in this paper can improve the segmentation effect of skin lesion images, and is expected to provide an auxiliary diagnosis for professional dermatologists.","10.7507/1001-5515.202208015","Atrous convolution; Attention mechanism; Image segmentation; Skin disease; U-Net","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144760955&doi=10.7507%2f1001-5515.202208015&partnerID=40&md5=cbfca694a58e6448c3e6fadb7c121ab9"
"SIL-Net: A Semi-Isotropic L-shaped network for dermoscopic image segmentation","Zhang Z.; Jiang Y.; Qiao H.; Wang M.; Yan W.; Chen J.","2022","0","1","0","0","0","Unique","0","","","","","","","Background: Dermoscopic image segmentation using deep learning algorithms is a critical technology for skin cancer detection and therapy. Specifically, this technology is a spatially equivariant task and relies heavily on Convolutional Neural Networks (CNNs), which lost more effective features during cascading down-sampling or up-sampling. Recently, vision isotropic architecture has emerged to eliminate cascade procedures in CNNs as well as demonstrates superior performance. Nevertheless, it cannot be used for the segmentation task directly. Based on these discoveries, this research intends to explore an efficient architecture which not only preserves the advantages of the isotropic architecture but is also suitable for clinical dermoscopic diagnosis. Methods: In this work, we introduce a novel Semi-Isotropic L-shaped network (SIL-Net) for dermoscopic image segmentation. First, we propose a Patch Embedding Weak Correlation (PEWC) module to address the issue of no interaction between adjacent patches during the standard Patch Embedding process. Second, a plug-and-play and zero-parameter Residual Spatial Mirror Information (RSMI) path is proposed to supplement effective features during up-sampling and optimize the lesion boundaries. Third, to further reconstruct deep features and get refined lesion regions, a Depth Separable Transpose Convolution (DSTC) based up-sampling module is designed. Results: The proposed architecture obtains state-of-the-art performance on dermoscopy benchmark datasets ISIC-2017, ISIC-2018 and PH2. Respectively, the Dice coefficient (DICE) of above datasets achieves 89.63%, 93.47%, and 95.11%, where the Mean Intersection over Union (MIoU) are 82.02%, 88.21%, and 90.81%. Furthermore, the robustness and generalizability of our method has been demonstrated through additional experiments on standard intestinal polyp datasets (CVC-ClinicDB and Kvasir-SEG). Conclusion: Our findings demonstrate that SIL-Net not only has great potential for precise segmentation of the lesion region but also exhibits stronger generalizability and robustness, indicating that it meets the requirements for clinical diagnosis. Notably, our method shows state-of-the-art performance on all five datasets, which highlights the effectiveness of the semi-isotropic design mechanism. © 2022 Elsevier Ltd","10.1016/j.compbiomed.2022.106146","Convolutional neural networks; Dermoscopic image segmentation; Patch embedding; Residual path; Semi-isotropic architecture","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139595126&doi=10.1016%2fj.compbiomed.2022.106146&partnerID=40&md5=57850133963cee83fc5abb16b98bfbcd"
"Computer Aided Diagnosis of Melanoma Using Deep Neural Networks and Game Theory: Application on Dermoscopic Images of Skin Lesions","Foahom Gouabou A.C.; Collenne J.; Monnier J.; Iguernaissi R.; Damoiseaux J.-L.; Moudafi A.; Merad D.","2022","1","0","0","0","0","Unique","0","","","","","","","Early detection of melanoma remains a daily challenge due to the increasing number of cases and the lack of dermatologists. Thus, AI-assisted diagnosis is considered as a possible solution for this issue. Despite the great advances brought by deep learning and especially convolutional neural networks (CNNs), computer-aided diagnosis (CAD) systems are still not used in clinical practice. This may be explained by the dermatologist’s fear of being misled by a false negative and the assimilation of CNNs to a “black box”, making their decision process difficult to understand by a non-expert. Decision theory, especially game theory, is a potential solution as it focuses on identifying the best decision option that maximizes the decision-maker’s expected utility. This study presents a new framework for automated melanoma diagnosis. Pursuing the goal of improving the performance of existing systems, our approach also attempts to bring more transparency in the decision process. The proposed framework includes a multi-class CNN and six binary CNNs assimilated to players. The players’ strategies is to first cluster the pigmented lesions (melanoma, nevus, and benign keratosis), using the introduced method of evaluating the confidence of the predictions, into confidence level (confident, medium, uncertain). Then, a subset of players has the strategy to refine the diagnosis for difficult lesions with medium and uncertain prediction. We used EfficientNetB5 as the backbone of our networks and evaluated our approach on the public ISIC dataset consisting of 8917 lesions: melanoma (1113), nevi (6705) and benign keratosis (1099). The proposed framework achieved an area under the receiver operating curve (AUROC) of 0.93 for melanoma, 0.96 for nevus and 0.97 for benign keratosis. Furthermore, our approach outperformed existing methods in this task, improving the balanced accuracy (BACC) of the best compared method from 77% to 86%. These results suggest that our framework provides an effective and explainable decision-making strategy. This approach could help dermatologists in their clinical practice for patients with atypical and difficult-to-diagnose pigmented lesions. We also believe that our system could serve as a didactic tool for less experienced dermatologists. © 2022 by the authors.","10.3390/ijms232213838","computer aided-diagnosis; convolutional neural networks; explainability; game theory; hierarchical architecture; melanoma detection; XAI","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142759803&doi=10.3390%2fijms232213838&partnerID=40&md5=a2fa43a873013bbcc5e5853516975a30"
"Anal squamous cell carcinoma: Impact of radiochemotherapy evolution over years and an explorative analysis of MRI prediction of tumor response in a mono-institutional series of 131 patients","Bonù M.L.; La Mattina S.; Singh N.; Toraci C.; Spiazzi L.; Terraneo F.; Barbera F.; Vitali P.; Frassine F.; Guerini A.; Triggiani L.; Tomasini D.; Morelli V.; Imbrescia J.; Andreuccetti J.; Frittoli B.; Pittiani F.; Grazioli L.; Portolani N.; Nicosia L.; Albano D.; Bertagna F.; Magrini S.M.; Buglione M.","2022","0","1","0","0","0","Unique","0","","","","","","","Introduction: Radiochemotherapy (RCHT) for the treatment of anal squamous cell carcinoma (ASCC) has evolved dramatically, also thanks to intensity-modulated RT (IMRT) and 3D image guidance (3D IGRT). Despite most patients presenting fair outcomes, unmet needs still exist. Predictors of poor tumor response are lacking; acute toxicity remains challenging; and local relapse remains the main pattern of failure. Patients and methods: Between 2010 and 2020, ASCC stages I–III treated with 3D conformal radiotherapy or IMRT and CDDP-5FU or Mytomicine-5FU CHT were identified. Image guidance accepted included 2D IGRT or 3D IGRT. The study endpoints included freedom from locoregional recurrence (FFLR), colostomy free survival (CFS), freedom from distant metastasis (FFDM), overall survival (OS), and acute and late toxicity as measured by common terminology criteria for adverse events (CTCAE) version 5.0. An exploratory analysis was performed to identify possible radiomic predictors of tumor response. Feature extraction and data analysis were performed in Python™, while other statistics were performed using SPSS® v.26.0 software (IBM®). Results: A total of 131 patients were identified. After a median FU of 52 months, 83 patients (63.4%) were alive. A total of 35 patients (26.7%) experienced locoregional failure, while 31 patients (23.7%) relapsed with distant metastasis. Five year FFLR, CFS, DMFS and PS resulted 72.3%, 80.1%, 74.5% and 64.6%. In multivariate analysis, 2D IGRT was associated with poorer FFLR, OS, and CFS (HR 4.5, 4.1, and 5.6, respectively); 3DcRT was associated with poorer OS and CFS (HR 3.1 and 6.6, respectively). IMRT reduced severe acute gastro-intestinal (GI) and severe skin acute toxicity in comparison with 3DcRT. In the exploratory analysis, the risk of relapse depended on a combination of three parameters: Total Energy, Gray Level Size Zone Matrix’s Large Area High Gray Level Emphasis (GLSZM’s LAHGLE), and GTV volume. Conclusions: Advances in radiotherapy have independently improved the prognosis of ASCC patients over years while decreasing acute GI and skin toxicity. IMRT and daily 3D image guidance may be considered standard of care in the management of ASCC. A combination of three pre-treatment MRI parameters such as low signal intensity (SI), high GLSZM’s LAHGLE, and GTV volume could be integrated in risk stratification to identify candidates for RT dose-escalation to be enrolled in clinical trials. Copyright © 2022 Bonù, La Mattina, Singh, Toraci, Spiazzi, Terraneo, Barbera, Vitali, Frassine, Guerini, Triggiani, Tomasini, Morelli, Imbrescia, Andreuccetti, Frittoli, Pittiani, Grazioli, Portolani, Nicosia, Albano, Bertagna, Magrini and Buglione.","10.3389/fonc.2022.973223","anal cancer (AC); IMRT (intensity modulated radiation therapy); mri; predictive modeling; radiotherapy–chemotherapy","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141436651&doi=10.3389%2ffonc.2022.973223&partnerID=40&md5=185accce03b9535777f2f9563dab3261"
"Application of a parallel branches network based on Transformer for skin melanoma segmentation","Yi S.; Zhang G.; He J.","2022","1","1","0","0","0","Unique","0","","","","","","","皮肤恶性黑色素瘤是一种常见的恶性肿瘤，针对病灶区域进行准确的分割对于该病的早期诊断非常重要。为了实现对皮肤病灶区域进行更有效、准确的分割，本文提出了一种基于变换器（Transformer）的并联网络结构。该网络由两条并联支路构成：前者为本文新构建的多重残差频域通道注意网络（MFC），后者为视觉变换网络（ViT）。首先，在MFC网络支路中，本文将多重残差模块和频域通道注意力模块（FCA）进行融合，在提高网络鲁棒性的同时加强对图像细节特征的提取；其次，在ViT网络支路中采用Transformer中的多头自注意机制（MSA）使图像的全局特征得以保留；最后，通过并联的方式将两条支路提取的特征信息结合起来，更有效地实现对图像的分割。为了验证本文算法，本文在国际皮肤成像合作组织（ISIC）2018年所公开的皮肤镜图像数据集上进行实验，结果表明本文算法的分割结果中交并比（IoU）和戴斯（Dice）系数分别达到了90.15%和94.82%，相比于最新的皮肤黑色素瘤分割网络均有较好的提升。因此，本文提出的网络能够更好地对病灶区域进行分割，为皮肤科医生提供更准确的病灶数据。.; Cutaneous malignant melanoma is a common malignant tumor. Accurate segmentation of the lesion area is extremely important for early diagnosis of the disease. In order to achieve more effective and accurate segmentation of skin lesions, a parallel network architecture based on Transformer is proposed in this paper. This network is composed of two parallel branches: the former is the newly constructed multiple residual frequency channel attention network (MFC), and the latter is the visual transformer network (ViT). First, in the MFC network branch, the multiple residual module and the frequency channel attention module (FCA) module are fused to improve the robustness of the network and enhance the capability of extracting image detailed features. Second, in the ViT network branch, multiple head self-attention (MSA) in Transformer is used to preserve the global features of the image. Finally, the feature information extracted from the two branches are combined in parallel to realize image segmentation more effectively. To verify the proposed algorithm, we conducted experiments on the dermoscopy image dataset published by the International Skin Imaging Collaboration (ISIC) in 2018. The results show that the intersection-over-union (IoU) and Dice coefficients of the proposed algorithm achieve 90.15% and 94.82%, respectively, which are better than the latest skin melanoma segmentation networks. Therefore, the proposed network can better segment the lesion area and provide dermatologists with more accurate lesion data.","10.7507/1001-5515.202110073","Computer vision; Deep learning; Skin segmentation; Transformer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141003458&doi=10.7507%2f1001-5515.202110073&partnerID=40&md5=bfc1281cfbb517a67b86d7903c9b4854"
"Skin Cancer Detection using Deep Learning","Rajarajeswari S.; Prassanna J.; Abdul Quadir Md.; Christy Jackson J.; Sharma S.; Rajesh B.","2022","1","1","0","0","0","First occurrence","0","","","","","","","Introduction: The identification and monitoring of benign moles and skin cancers leads to a challenging task because of the usual standard significant skin patches. Actually, the skin lesions vary very little in their look and only limited amount of information is available. There are seven fundamental types of skin cancer like Basal Cell Carcinoma (BCC), Melanoma and Squamous Cell Carcinoma (SCC) whereas Melanoma is the highly risky which has low survival rate. Objective: This work classifies skin lesions with the help of Convolution Neural Network and the images are trained end-to-end. A dataset comprised of 10000 clinical images were trained using Convolution Neural Network (CNN). Materials and Methods: The skin cancer identification process is generally separated into two basic components, image pre-processing which includes classification of images and removing the duplicate images and sharpening, which resizes the skin image. This work discusses a methodology to segment the high-level skin lesion and identification of malignancy more accurately with the help of deep learning: 1) Construction of a neural network, which detects the edge of a huge lesion accurately; 2) Designing model that can run on mobile phones. The model designed a transfer learning which is based deep on neural network and the fine turning that supports to attain high prediction accuracy. Results: The dataset comprises of a total of 10,000 images stored in two folders. The information about the data is stored in a data frame. Total 10000 dermoscopic images contains 374 melanoma images, 254 seborrheic keratosis images and 1372 nevus images. Using transfer learning validation loss, Top-2 accuracy and Top-3 accuracy have been calculated. The result has been compared with the different models. Conclusions: The proposed system can categorize healthy skin lesions, eczema, acne, malignant and benign skin lesions. The proposed work investigates the attributes acquired by the deep convolutional neural network. The attributes are extracted and the datasets were divided into seven different categories. Based on that categories the data was trained and validated. Based on the calculation the validation loss, top-2 accuracy, top-3 accuracy was calculated. © RJPT All right reserved.","10.52711/0974-360X.2022.00758","Deep learning; Machine Learning; MobileNet; Neural network; Skin cancer; skin_recnn; skin_segnn)","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146243638&doi=10.52711%2f0974-360X.2022.00758&partnerID=40&md5=d9f013ee6cf1737aa0914118427bb4df"
"Forecasting solar photosynthetic photon flux density under cloud cover effects: novel predictive model using convolutional neural network integrated with long short-term memory network","Deo R.C.; Grant R.H.; Webb A.; Ghimire S.; Igoe D.P.; Downs N.J.; Al-Musaylh M.S.; Parisi A.V.; Soar J.","2022","0","1","0","0","0","First occurrence","0","","","","","","","Forecast models of solar radiation incorporating cloud effects are useful tools to evaluate the impact of stochastic behaviour of cloud movement, real-time integration of photovoltaic energy in power grids, skin cancer and eye disease risk minimisation through solar ultraviolet (UV) index prediction and bio-photosynthetic processes through the modelling of solar photosynthetic photon flux density (PPFD). This research has developed deep learning hybrid model (i.e., CNN-LSTM) to factor in role of cloud effects integrating the merits of convolutional neural networks with long short-term memory networks to forecast near real-time (i.e., 5-min) PPFD in a sub-tropical region Queensland, Australia. The prescribed CLSTM model is trained with real-time sky images that depict stochastic cloud movements captured through a total sky imager (TSI-440) utilising advanced sky image segmentation to reveal cloud chromatic features into their statistical values, and to purposely factor in the cloud variation to optimise the CLSTM model. The model, with its competing algorithms (i.e., CNN, LSTM, deep neural network, extreme learning machine and multivariate adaptive regression spline), are trained with 17 distinct cloud cover inputs considering the chromaticity of red, blue, thin, and opaque cloud statistics, supplemented by solar zenith angle (SZA) to predict short-term PPFD. The models developed with cloud inputs yield accurate results, outperforming the SZA-based models while the best testing performance is recorded by the objective method (i.e., CLSTM) tested over a 7-day measurement period. Specifically, CLSTM yields a testing performance with correlation coefficient r = 0.92, root mean square error RMSE = 210.31 μ mol of photons m−2 s−1, mean absolute error MAE = 150.24 μ mol of photons m−2 s−1, including a relative error of RRMSE = 24.92% MAPE = 38.01%, and Nash Sutcliffe’s coefficient ENS = 0.85, and Legate and McCabe’s Index LM = 0.68 using cloud cover in addition to the SZA as an input. The study shows the importance of cloud inclusion in forecasting solar radiation and evaluating the risk with practical implications in monitoring solar energy, greenhouses and high-value agricultural operations affected by stochastic behaviour of clouds. Additional methodological refinements such as retraining the CLSTM model for hourly and seasonal time scales may aid in the promotion of agricultural crop farming and environmental risk evaluation applications such as predicting the solar UV index and direct normal solar irradiance for renewable energy monitoring systems. © 2022, The Author(s).","10.1007/s00477-022-02188-0","Deep learning; Photosynthetic photon flux density; Photosynthetic radiation; Risk evaluation; Solar radiation modelling; Stochastic cloud effects","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127634525&doi=10.1007%2fs00477-022-02188-0&partnerID=40&md5=adf716974d31caa74775e647b8db94de"
"Issues in Melanoma Detection: Semisupervised Deep Learning Algorithm Development via a Combination of Human and Artificial Intelligence","Zhang X.; Xie Z.; Xiang Y.; Baig I.; Kozman M.; Stender C.; Giancardo L.; Tao C.","2022","0","1","0","0","0","Unique","0","","","","","","","Background: Automatic skin lesion recognition has shown to be effective in increasing access to reliable dermatology evaluation; however, most existing algorithms rely solely on images. Many diagnostic rules, including the 3-point checklist, are not considered by artificial intelligence algorithms, which comprise human knowledge and reflect the diagnosis process of human experts. Objective: In this paper, we aimed to develop a semisupervised model that can not only integrate the dermoscopic features and scoring rule from the 3-point checklist but also automate the feature-annotation process. Methods: We first trained the semisupervised model on a small, annotated data set with disease and dermoscopic feature labels and tried to improve the classification accuracy by integrating the 3-point checklist using ranking loss function. We then used a large, unlabeled data set with only disease label to learn from the trained algorithm to automatically classify skin lesions and features. Results: After adding the 3-point checklist to our model, its performance for melanoma classification improved from a mean of 0.8867 (SD 0.0191) to 0.8943 (SD 0.0115) under 5-fold cross-validation. The trained semisupervised model can automatically detect 3 dermoscopic features from the 3-point checklist, with best performances of 0.80 (area under the curve [AUC] 0.8380), 0.89 (AUC 0.9036), and 0.76 (AUC 0.8444), in some cases outperforming human annotators. Conclusions: Our proposed semisupervised learning framework can help with the automatic diagnosis of skin disease based on its ability to detect dermoscopic features and automate the label-annotation process. The framework can also help combine semantic knowledge with a computer algorithm to arrive at a more accurate and more interpretable diagnostic result, which can be applied to broader use cases. ©Xinyuan Zhang, Ziqian Xie, Yang Xiang, Imran Baig, Mena Kozman, Carly Stender, Luca Giancardo, Cui Tao.","10.2196/39113","3-point checklist; algorithm; automatic diagnosis; deep learning; dermatology; dermoscopic images; melanoma; melanoma classification; semisupervised learning; skin disease; skin lesion","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149833136&doi=10.2196%2f39113&partnerID=40&md5=d21e3aa2177ebcb59188ed8b88db2f39"
"Model soups improve performance of dermoscopic skin cancer classifiers","Maron R.C.; Hekler A.; Haggenmüller S.; von Kalle C.; Utikal J.S.; Müller V.; Gaiser M.; Meier F.; Hobelsberger S.; Gellrich F.F.; Sergon M.; Hauschild A.; French L.E.; Heinzerling L.; Schlager J.G.; Ghoreschi K.; Schlaak M.; Hilke F.J.; Poch G.; Korsing S.; Berking C.; Heppt M.V.; Erdmann M.; Haferkamp S.; Schadendorf D.; Sondermann W.; Goebeler M.; Schilling B.; Kather J.N.; Fröhling S.; Lipka D.B.; Krieghoff-Henning E.; Brinker T.J.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Image-based cancer classifiers suffer from a variety of problems which negatively affect their performance. For example, variation in image brightness or different cameras can already suffice to diminish performance. Ensemble solutions, where multiple model predictions are combined into one, can improve these problems. However, ensembles are computationally intensive and less transparent to practitioners than single model solutions. Constructing model soups, by averaging the weights of multiple models into a single model, could circumvent these limitations while still improving performance. Objective: To investigate the performance of model soups for a dermoscopic melanoma-nevus skin cancer classification task with respect to (1) generalisation to images from other clinics, (2) robustness against small image changes and (3) calibration such that the confidences correspond closely to the actual predictive uncertainties. Methods: We construct model soups by fine-tuning pre-trained models on seven different image resolutions and subsequently averaging their weights. Performance is evaluated on a multi-source dataset including holdout and external components. Results: We find that model soups improve generalisation and calibration on the external component while maintaining performance on the holdout component. For robustness, we observe performance improvements for pertubated test images, while the performance on corrupted test images remains on par. Conclusions: Overall, souping for skin cancer classifiers has a positive effect on generalisation, robustness and calibration. It is easy for practitioners to implement and by combining multiple models into a single model, complexity is reduced. This could be an important factor in achieving clinical applicability, as less complexity generally means more transparency. © 2022 The Authors","10.1016/j.ejca.2022.07.002","Artificial intelligence; Calibration; Deep learning; Dermatology; Ensembles; Generalisation; Melanoma; Model soups; Nevus; Robustness","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135953869&doi=10.1016%2fj.ejca.2022.07.002&partnerID=40&md5=3d1289d0e7f464e9a412ea60237a8be5"
"MNet-10: A robust shallow convolutional neural network model performing ablation study on medical images assessing the effectiveness of applying optimal data augmentation technique","Montaha S.; Azam S.; Rafid A.K.M.R.H.; Hasan M.Z.; Karim A.; Hasib K.M.; Patel S.K.; Jonkman M.; Mannan Z.I.","2022","0","1","0","0","0","Unique","0","","","","","","","Interpretation of medical images with a computer-aided diagnosis (CAD) system is arduous because of the complex structure of cancerous lesions in different imaging modalities, high degree of resemblance between inter-classes, presence of dissimilar characteristics in intra-classes, scarcity of medical data, and presence of artifacts and noises. In this study, these challenges are addressed by developing a shallow convolutional neural network (CNN) model with optimal configuration performing ablation study by altering layer structure and hyper-parameters and utilizing a suitable augmentation technique. Eight medical datasets with different modalities are investigated where the proposed model, named MNet-10, with low computational complexity is able to yield optimal performance across all datasets. The impact of photometric and geometric augmentation techniques on different datasets is also evaluated. We selected the mammogram dataset to proceed with the ablation study for being one of the most challenging imaging modalities. Before generating the model, the dataset is augmented using the two approaches. A base CNN model is constructed first and applied to both the augmented and non-augmented mammogram datasets where the highest accuracy is obtained with the photometric dataset. Therefore, the architecture and hyper-parameters of the model are determined by performing an ablation study on the base model using the mammogram photometric dataset. Afterward, the robustness of the network and the impact of different augmentation techniques are assessed by training the model with the rest of the seven datasets. We obtain a test accuracy of 97.34% on the mammogram, 98.43% on the skin cancer, 99.54% on the brain tumor magnetic resonance imaging (MRI), 97.29% on the COVID chest X-ray, 96.31% on the tympanic membrane, 99.82% on the chest computed tomography (CT) scan, and 98.75% on the breast cancer ultrasound datasets by photometric augmentation and 96.76% on the breast cancer microscopic biopsy dataset by geometric augmentation. Moreover, some elastic deformation augmentation methods are explored with the proposed model using all the datasets to evaluate their effectiveness. Finally, VGG16, InceptionV3, and ResNet50 were trained on the best-performing augmented datasets, and their performance consistency was compared with that of the MNet-10 model. The findings may aid future researchers in medical data analysis involving ablation studies and augmentation techniques. Copyright © 2022 Montaha, Azam, Rafid, Hasan, Karim, Hasib, Patel, Jonkman and Mannan.","10.3389/fmed.2022.924979","ablation study; deep learning models; geometric augmentation; medical image; photometric augmentation; shallow CNN","32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137977355&doi=10.3389%2ffmed.2022.924979&partnerID=40&md5=5104b4827ff779f014e552d10948d57b"
"Using Artificial Intelligence as a Diagnostic Decision Support Tool in Skin Disease: Protocol for an Observational Prospective Cohort Study","Escalé-Besa A.; Fuster-Casanovas A.; Börve A.; Yélamos O.; Fustà-Novell X.; Rafat M.E.; Marin-Gomez F.X.; Vidal-Alaball J.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Dermatological conditions are a relevant health problem. Each person has an average of 1.6 skin diseases per year, and consultations for skin pathology represent 20% of the total annual visits to primary care and around 35% are referred to a dermatology specialist. Machine learning (ML) models can be a good tool to help primary care professionals, as it can analyze and optimize complex sets of data. In addition, ML models are increasingly being applied to dermatology as a diagnostic decision support tool using image analysis, especially for skin cancer detection and classification. Objective: This study aims to perform a prospective validation of an image analysis ML model as a diagnostic decision support tool for the diagnosis of dermatological conditions. Methods: In this prospective study, 100 consecutive patients who visit a participant general practitioner (GP) with a skin problem in central Catalonia were recruited. Data collection was planned to last 7 months. Anonymized pictures of skin diseases were taken and introduced to the ML model interface (capable of screening for 44 different skin diseases), which returned the top 5 diagnoses by probability. The same image was also sent as a teledermatology consultation following the current stablished workflow. The GP, ML model, and dermatologist’s assessments will be compared to calculate the precision, sensitivity, specificity, and accuracy of the ML model. The results will be represented globally and individually for each skin disease class using a confusion matrix and one-versus-all methodology. The time taken to make the diagnosis will also be taken into consideration. Results: Patient recruitment began in June 2021 and lasted for 5 months. Currently, all patients have been recruited and the images have been shown to the GPs and dermatologists. The analysis of the results has already started. Conclusions: This study will provide information about ML models’ effectiveness and limitations. External testing is essential for regulating these diagnostic systems to deploy ML models in a primary care practice setting. © Anna Escalé-Besa, Aïna Fuster-Casanovas, Alexander Börve, Oriol Yélamos, Xavier Fustà-Novell, Mireia Esquius Rafat, Francesc X Marin-Gomez, Josep Vidal-Alaball.","10.2196/37531","artificial intelligence; cohort study; computer-assisted diagnosis; data accuracy; dermatology; machine learning; neural network computer; skin disease; support tool","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138581785&doi=10.2196%2f37531&partnerID=40&md5=87c5b7b3b1c7250e8dd9452853f8e192"
"A Review on Automated Cancer Detection in Medical Images using Machine Learning and Deep Learning based Computational Techniques: Challenges and Opportunities","Manhas J.; Gupta R.K.; Roy P.P.","2022","0","1","0","0","0","First occurrence","0","","","","","","","Cancer is one of the most deadly diseases diagnosed among the population across the globe so far. The number of cases is increasing at a high pace each year that subsequently leads to the advancement in different diagnosis tools and technologies to handle this pandemic. Significant increase in the mortality rate worldwide leads tremendous scope to device and implement latest computer aided diagnostic systems for its early detection. The one among such techniques is machine learning coupled with medical imaging modalities. This combination has proven to be efficient in diagnosing various medical conditions in cancer diagnosis. Current study presents a review of different machine learning techniques applied on imaging modalities for cancer diagnosis from 2008 to 2019. This study focuses on diagnosis of five most prevalent and deadly cancers i.e., cervical cancer, oral cancer, breast cancer, brain cancer and skin cancer. Extensive and exhaustive review was carried out after going through different research papers, research articles and book chapters published by reputed international and national publishers such as Springer Link, Science Direct, IEEE Xplore Digital library and PubMed. A number of conference proceedings have also been included subject to the fulfilling of our quality evaluation criteria. This review article provides a comprehensive overview of machine learning approaches using image modalities for cancer detection and diagnosis with main focus on challenges being faced during their research. Majority of the challenges are identified based on the use of potential machine learning based approaches, image modalities, features and evaluation metrics. This review not only identified challenges but also ear mark and present the new research opportunities for researchers working in this field. It has been widely observed that traditional machine learning algorithms Like SVM, GMM performed excellent in classification whereas the deep learning has dominated the field of medical image analysis to a greater extent. It is evident from the literature survey that the researchers have achieved the accuracies of 100% in classification of cancerous and normal tissue images using different machine learning techniques. This article will provide an insight to the researchers working in this domain to identify which machine learning technique work best on what type of data set, selection of features, various challenges and their proposed solutions in solving this complex problem. Limitations and future research opportunities in the field of implementing different machine learning techniques in cancer diagnosis and classification is also presented at the end of this review article. © 2021, The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE).","10.1007/s11831-021-09676-6","","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119191272&doi=10.1007%2fs11831-021-09676-6&partnerID=40&md5=1e587a5f9a2ee4b9b822890ee2537f80"
"FSPBO-DQN: SeGAN based segmentation and Fractional Student Psychology Optimization enabled Deep Q Network for skin cancer detection in IoT applications","Kumar K.S.; Suganthi N.; Muppidi S.; Kumar B.S.","2022","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is one of the dangerous types of cancer and the rate of death is increasing due to the lack of knowledge in prevention and the symptoms. It is a common cancer type around the world and it occurs when the skin cells are damaged. Hence, the detection of skin cancer near the beginning is important to prevent the spread of cancer and to increase the survival rate. Recently, image processing and machine learning techniques gained more interest in medical applications. However, early analysis of skin cancer images is very challenging due to factors, like variations in the color illumination, light reflections from the skin surface, and different sizes and shapes of lesions. To detect skin cancer at an early stage and to increase the survival rate, an effective skin cancer detection method is introduced in this study using the proposed Fractional Student Psychology Based Optimization-based Deep Q Network (FSPBO-based DQN) in the wireless network scenario. At first, the nodes simulated in the network area are allowed to capture the healthcare information to make the detection strategy using the proposed method. Then, the routing is performed by the proposed Fractional Student Psychology Based Optimization (FSPBO) algorithm by considering the fitness parameters, like distance, energy, trust, and delay. After the images (healthcare information) are reached the Base Station (BS), the pre-processing, segmentation, and cancer detection processes are carried out to detect the skin lesions. Initially, the image is fed to pre-processing phase, where a Type II Fuzzy System and cuckoo search optimization algorithm (T2FCS) filter is employed to remove the noise of images. Then, the pre-processed images are fed to the segmentation phase, where speech enhancement Generative Adversarial Network (SeGAN) is used to generate the segmented results. Afterward, the Deep Q Network (DQN) detects the skin cancer based on the segmented results, and the training of DQN is made using the proposed FSPBO algorithm, which is designed by integrating the Student Psychology Based Optimization (SPBO) and Fractional Calculus (FC). The proposed method is more robust and reduces computation time and complexity. Moreover, the proposed method achieved higher performance by considering the measures, namely accuracy, sensitivity, and specificity with the values of 92.364%, 93.20%, and 92.63%. © 2022 Elsevier B.V.","10.1016/j.artmed.2022.102299","Deep Q Network; Fractional Calculus; Internet of Things; Routing; Skin cancer detection","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129636264&doi=10.1016%2fj.artmed.2022.102299&partnerID=40&md5=35d7f31e349a0f54faf23ccf2d9d69f3"
"Multi-class skin lesion classification using prism- and segmentation-based fractal signatures","Camacho-Gutiérrez J.A.; Solorza-Calderón S.; Álvarez-Borrego J.","2022","1","1","0","0","0","Unique","0","","","","","","","Today, due to technological advances, specialists can count on a wide range of computer-aided diagnostic applications. However, there are still significant challenges to overcome. That is the case when it comes to automating the classification of skin lesions. Amorphous pigmentation, fuzzy edges, unbalanced databases, unrepresentative minority classes, images of different sizes, and resolutions. Reproducibility of the models are examples that we face to develop computer-assisted diagnostic models. In this work, we tackle the problem of amorphous pigmentation lesions and fuzzy edges by proposing two new fractal signatures, which we call a fractal signature based on statistical prisms (SSPF) and a statistical fractal signature (SSTF). The methodologies presented in this work for the multi-class classification of skin lesions were evaluated using the International Skin Imaging Collaboration (ISIC) database from 2019 containing 25,331 images divided into eight highly unbalanced classes. The database was randomly divided into 80%–20% for the training and test datasets images, respectively. In that manner, could be assured the reproducibility of the models. The performance metrics reported for both the training and testing part are accuracy, sensitivity, specificity, and precision. In our methodology, each study case was repeated 30 times to avoid bias, according to the central limit theorem, and the mean ±1 standard deviation was reported. The results showed that it is imperative to have balanced databases and that the marked imbalance affects minority classes. All works must report the training performance and the test datasets to ensure the reproducibility of the models. We obtained that SSTF (the signature that combines the statistical prism-based fractal signature SSPF and SSF) can increase the classifier's performance instead of the use of each fractal signature by itself. The methodology proposed using the SSTF with the LDA classifier is robust and reproducible for the simultaneous classification of 4 classes; we achieved 87% accuracy, 63% sensitivity, 89% specificity, and 65% precision. In the case of the simultaneous 7-class classification, the results were 88% accuracy, 41% sensitivity, 92% specificity, and 46% precision. © 2022 Elsevier Ltd","10.1016/j.eswa.2022.116671","Automation classification; Computer-aided diagnosis; Dermoscopic images; Fractal signatures; Multi-class classification; Skin lesions","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125588501&doi=10.1016%2fj.eswa.2022.116671&partnerID=40&md5=a9aa8d4efc7232b1f9c39bcd57b360f3"
"Artificial intelligence and machine learning algorithms for early detection of skin cancer in community and primary care settings: a systematic review","Jones O.T.; Matin R.N.; van der Schaar M.; Prathivadi Bhayankaram K.; Ranmuthu C.K.I.; Islam M.S.; Behiyat D.; Boscott R.; Calanzani N.; Emery J.; Williams H.C.; Walter F.M.","2022","1","1","0","0","0","Unique","0","","","","","","","Skin cancers occur commonly worldwide. The prognosis and disease burden are highly dependent on the cancer type and disease stage at diagnosis. We systematically reviewed studies on artificial intelligence and machine learning (AI/ML) algorithms that aim to facilitate the early diagnosis of skin cancers, focusing on their application in primary and community care settings. We searched MEDLINE, Embase, Scopus, and Web of Science (from Jan 1, 2000, to Aug 9, 2021) for all studies providing evidence on applying AI/ML algorithms to the early diagnosis of skin cancer, including all study designs and languages. The primary outcome was diagnostic accuracy of the algorithms for skin cancers. The secondary outcomes included an overview of AI/ML methods, evaluation approaches, cost-effectiveness, and acceptability to patients and clinicians. We identified 14 224 studies. Only two studies used data from clinical settings with a low prevalence of skin cancers. We reported data from all 272 studies that could be relevant in primary care. The primary outcomes showed reasonable mean diagnostic accuracy for melanoma (89·5% [range 59·7–100%]), squamous cell carcinoma (85·3% [71·0–97·8%]), and basal cell carcinoma (87·6% [70·0–99·7%]). The secondary outcomes showed a heterogeneity of AI/ML methods and study designs, with high amounts of incomplete reporting (eg, patient demographics and methods of data collection). Few studies used data on populations with a low prevalence of skin cancers to train and test their algorithms; therefore, the widespread adoption into community and primary care practice cannot currently be recommended until efficacy in these populations is shown. We did not identify any health economic, patient, or clinician acceptability data for any of the included studies. We propose a methodological checklist for use in the development of new AI/ML algorithms to detect skin cancer, to facilitate their design, evaluation, and implementation. © 2022 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license","10.1016/S2589-7500(22)00023-1","","175","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131107977&doi=10.1016%2fS2589-7500%2822%2900023-1&partnerID=40&md5=db053c2ff01a5c962160f396dc5464f6"
"Classification of skin cancer using convolutional neural networks analysis of Raman spectra","Bratchenko I.A.; Bratchenko L.A.; Khristoforova Y.A.; Moryatov A.A.; Kozlov S.V.; Zakharov V.P.","2022","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Skin cancer is the most common malignancy in whites accounting for about one third of all cancers diagnosed per year. Portable Raman spectroscopy setups for skin cancer ""optical biopsy"" are utilized to detect tumors based on their spectral features caused by the comparative presence of different chemical components. However, low signal-to-noise ratio in such systems may prevent accurate tumors classification. Thus, there is a challenge to develop methods for efficient skin tumors classification. Methods: We compare the performance of convolutional neural networks and the projection on latent structures with discriminant analysis for discriminating skin cancer using the analysis of Raman spectra with a high autofluorescence background stimulated by a 785 nm laser. We have registered the spectra of 617 cases of skin neoplasms (615 patients, 70 melanomas, 122 basal cell carcinomas, 12 squamous cell carcinomas and 413 benign tumors) in vivo with a portable Raman setup and created classification models both for convolutional neural networks and projection on latent structures approaches. To check the classification models stability, a 10-fold cross-validation was performed for all created models. To avoid models overfitting, the data was divided into a training set (80% of spectral dataset) and a test set (20% of spectral dataset). Results: The results for different classification tasks demonstrate that the convolutional neural networks significantly (p<0.01) outperforms the projection on latent structures. For the convolutional neural networks implementation we obtained ROC AUCs of 0.96 (0.94 – 0.97; 95% CI), 0.90 (0.85–0.94; 95% CI), and 0.92 (0.87 – 0.97; 95% CI) for classifying a) malignant vs benign tumors, b) melanomas vs pigmented tumors and c) melanomas vs seborrheic keratosis respectively. Conclusions: The performance of the convolutional neural networks classification of skin tumors based on Raman spectra analysis is higher or comparable to the accuracy provided by trained dermatologists. The increased accuracy with the convolutional neural networks implementation is due to a more precise accounting of low intensity Raman bands in the intense autofluorescence background. The achieved high performance of skin tumors classifications with convolutional neural networks analysis opens a possibility for wide implementation of Raman setups in clinical setting. © 2022 Elsevier B.V.","10.1016/j.cmpb.2022.106755","Convolutional neural networks; Malignant tumor; Optical biopsy; Projection on latent structures with discriminant analysis; Raman spectroscopy; Skin cancer","63","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127087826&doi=10.1016%2fj.cmpb.2022.106755&partnerID=40&md5=23d79d53bf5498fd24eaeb2ccf9ecc03"
"Explainable artificial intelligence in skin cancer recognition: A systematic review","Hauser K.; Kurz A.; Haggenmüller S.; Maron R.C.; von Kalle C.; Utikal J.S.; Meier F.; Hobelsberger S.; Gellrich F.F.; Sergon M.; Hauschild A.; French L.E.; Heinzerling L.; Schlager J.G.; Ghoreschi K.; Schlaak M.; Hilke F.J.; Poch G.; Kutzner H.; Berking C.; Heppt M.V.; Erdmann M.; Haferkamp S.; Schadendorf D.; Sondermann W.; Goebeler M.; Schilling B.; Kather J.N.; Fröhling S.; Lipka D.B.; Hekler A.; Krieghoff-Henning E.; Brinker T.J.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Due to their ability to solve complex problems, deep neural networks (DNNs) are becoming increasingly popular in medical applications. However, decision-making by such algorithms is essentially a black-box process that renders it difficult for physicians to judge whether the decisions are reliable. The use of explainable artificial intelligence (XAI) is often suggested as a solution to this problem. We investigate how XAI is used for skin cancer detection: how is it used during the development of new DNNs? What kinds of visualisations are commonly used? Are there systematic evaluations of XAI with dermatologists or dermatopathologists? Methods: Google Scholar, PubMed, IEEE Explore, Science Direct and Scopus were searched for peer-reviewed studies published between January 2017 and October 2021 applying XAI to dermatological images: the search terms histopathological image, whole-slide image, clinical image, dermoscopic image, skin, dermatology, explainable, interpretable and XAI were used in various combinations. Only studies concerned with skin cancer were included. Results: 37 publications fulfilled our inclusion criteria. Most studies (19/37) simply applied existing XAI methods to their classifier to interpret its decision-making. Some studies (4/37) proposed new XAI methods or improved upon existing techniques. 14/37 studies addressed specific questions such as bias detection and impact of XAI on man-machine-interactions. However, only three of them evaluated the performance and confidence of humans using CAD systems with XAI. Conclusion: XAI is commonly applied during the development of DNNs for skin cancer detection. However, a systematic and rigorous evaluation of its usefulness in this scenario is lacking. © 2022 The Author(s)","10.1016/j.ejca.2022.02.025","Artificial intelligence; Dermatology; Man-machine systems; Skin neoplasms; Systematic review","105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127607993&doi=10.1016%2fj.ejca.2022.02.025&partnerID=40&md5=b4e65d70580cd7cc1b0dd24fa39b4e0b"
"The Experience of 3D Total-Body Photography to Monitor Nevi: Results from an Australian General Population-Based Cohort Study","Horsham C.; O'Hara M.; Sanjida S.; Ma S.; Jayasinghe D.; Green A.C.; Schaider H.; Aitken J.F.; Sturm R.A.; Prow T.; Soyer H.P.; Janda M.","2022","0","1","0","0","0","Unique","0","","","","","","","Background: Digital 3D total-body photography of the skin surface is an emerging imaging modality that can facilitate the identification of new and changing nevi. Objective: We aimed to describe the experiences of study participants drawn from the general population who were provided 3D total-body photography and dermoscopy for the monitoring of nevi. Methods: A population-based prospective study of adults aged 20-70 years from South East Queensland, Australia was conducted. Participants underwent 3D total-body photography and dermoscopy every 6 months over a 3-year period. Participants were asked to provide closed and open-ended feedback on their 3D total-body photography and dermoscopy experience (eg, comfort, trust, intended future use, and willingness to pay) at the halfway study time point (18 months) and final study time point (36 months). We assessed changes in participants’ reported experience of 3D total-body photography, and patient characteristics associated with patient experience at the end of the study (36 months) were analyzed. Results: A total of 149 participants completed the surveys at both the 18- and 36-month time points (median age 55, range 23-70 years; n=94, 63.1% were male). At the 18-month time point, most participants (n=103, 69.1%) stated they completely trusted 3D total-body imaging for the diagnosis and monitoring of their nevi, and this did not change at the 36-month (n=104, 69.8%) time point. The majority of participants reported that they were very comfortable or comfortable with the technology at both the 18- (n=138, 92.6%) and 36-month (n=140, 94%) time points, respectively; albeit, the number of participants reporting that they were very comfortable reduced significantly between the 18- and 36-month time points, from 71.1% (n=106) to 61.1% (n=91; P=.01). Almost all participants (n=140, 94%) would consider using this technology if it were to become commercially available, and this did not change during the two study time points. Half of the participants (n=74) cited barriers to participating in 3D total-body photography, including trust in the ability of this technology to detect and monitor suspicious lesions, digital privacy, cost, and travel requirements. Conclusions: The majority of participants expressed positive attitudes toward 3D total-body photography for the monitoring of their moles. Half of the participants identified potential barriers to uptake. ©Caitlin Horsham, Montana O'Hara, Saira Sanjida, Samantha Ma, Dilki Jayasinghe, Adele C Green, Helmut Schaider, Joanne F Aitken, Richard A Sturm, Tarl Prow, H Peter Soyer, Monika Janda.","10.2196/37034","3D total-body photography; artificial intelligence; cohort study; early detection; melanocytic nevi; melanoma; moles; skin; skin cancer; skin surface","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133155729&doi=10.2196%2f37034&partnerID=40&md5=59834f402fe731408468b4961e641993"
"Validation of artificial intelligence prediction models for skin cancer diagnosis using dermoscopy images: the 2019 International Skin Imaging Collaboration Grand Challenge","Combalia M.; Codella N.; Rotemberg V.; Carrera C.; Dusza S.; Gutman D.; Helba B.; Kittler H.; Kurtansky N.R.; Liopyris K.; Marchetti M.A.; Podlipnik S.; Puig S.; Rinner C.; Tschandl P.; Weber J.; Halpern A.; Malvehy J.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Previous studies of artificial intelligence (AI) applied to dermatology have shown AI to have higher diagnostic classification accuracy than expert dermatologists; however, these studies did not adequately assess clinically realistic scenarios, such as how AI systems behave when presented with images of disease categories that are not included in the training dataset or images drawn from statistical distributions with significant shifts from training distributions. We aimed to simulate these real-world scenarios and evaluate the effects of image source institution, diagnoses outside of the training set, and other image artifacts on classification accuracy, with the goal of informing clinicians and regulatory agencies about safety and real-world accuracy. Methods: We designed a large dermoscopic image classification challenge to quantify the performance of machine learning algorithms for the task of skin cancer classification from dermoscopic images, and how this performance is affected by shifts in statistical distributions of data, disease categories not represented in training datasets, and imaging or lesion artifacts. Factors that might be beneficial to performance, such as clinical metadata and external training data collected by challenge participants, were also evaluated. 25 331 training images collected from two datasets (in Vienna [HAM10000] and Barcelona [BCN20000]) between Jan 1, 2000, and Dec 31, 2018, across eight skin diseases, were provided to challenge participants to design appropriate algorithms. The trained algorithms were then tested for balanced accuracy against the HAM10000 and BCN20000 test datasets and data from countries not included in the training dataset (Turkey, New Zealand, Sweden, and Argentina). Test datasets contained images of all diagnostic categories available in training plus other diagnoses not included in training data (not trained category). We compared the performance of the algorithms against that of 18 dermatologists in a simulated setting that reflected intended clinical use. Findings: 64 teams submitted 129 state-of-the-art algorithm predictions on a test set of 8238 images. The best performing algorithm achieved 58·8% balanced accuracy on the BCN20000 data, which was designed to better reflect realistic clinical scenarios, compared with 82·0% balanced accuracy on HAM10000, which was used in a previously published benchmark. Shifted statistical distributions and disease categories not included in training data contributed to decreases in accuracy. Image artifacts, including hair, pen markings, ulceration, and imaging source institution, decreased accuracy in a complex manner that varied based on the underlying diagnosis. When comparing algorithms to expert dermatologists (2460 ratings on 1269 images), algorithms performed better than experts in most categories, except for actinic keratoses (similar accuracy on average) and images from categories not included in training data (26% correct for experts vs 6% correct for algorithms, p<0·0001). For the top 25 submitted algorithms, 47·1% of the images from categories not included in training data were misclassified as malignant diagnoses, which would lead to a substantial number of unnecessary biopsies if current state-of-the-art AI technologies were clinically deployed. Interpretation: We have identified specific deficiencies and safety issues in AI diagnostic systems for skin cancer that should be addressed in future diagnostic evaluation protocols to improve safety and reliability in clinical practice. Funding: Melanoma Research Alliance and La Marató de TV3. © 2022 The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY-NC-ND 4.0 license","10.1016/S2589-7500(22)00021-8","","91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128465006&doi=10.1016%2fS2589-7500%2822%2900021-8&partnerID=40&md5=e976a5d3a063b60571135bdbb1e05cd4"
"Reimagining leprosy elimination with AI analysis of a combination of skin lesion images with demographic and clinical data","Barbieri R.R.; Xu Y.; Setian L.; Souza-Santos P.T.; Trivedi A.; Cristofono J.; Bhering R.; White K.; Sales A.M.; Miller G.; Nery J.A.C.; Sharman M.; Bumann R.; Zhang S.; Goldust M.; Sarno E.N.; Mirza F.; Cavaliero A.; Timmer S.; Bonfiglioli E.; Smith C.; Scollard D.; Navarini A.A.; Aerts A.; Ferres J.L.; Moraes M.O.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Leprosy is an infectious disease that mostly affects underserved populations. Although it has been largely eliminated, still about 200’000 new patients are diagnosed annually. In the absence of a diagnostic test, clinical diagnosis is often delayed, potentially leading to irreversible neurological damage and its resulting stigma, as well as continued transmission. Accelerating diagnosis could significantly contribute to advancing global leprosy elimination. Digital and Artificial Intelligence (AI) driven technology has shown potential to augment health workers abilities in making faster and more accurate diagnosis, especially when using images such as in the fields of dermatology or ophthalmology. That made us start the quest for an AI-driven diagnosis assistant for leprosy, based on skin images. Methods: Here we describe the accuracy of an AI-enabled image-based diagnosis assistant for leprosy, called AI4Leprosy, based on a combination of skin images and clinical data, collected following a standardized process. In a Brazilian leprosy national referral center, 222 patients with leprosy or other dermatological conditions were included, and the 1229 collected skin images and 585 sets of metadata are stored in an open-source dataset for other researchers to exploit. Findings: We used this dataset to test whether a CNN-based AI algorithm could contribute to leprosy diagnosis and employed three AI models, testing images and metadata both independently and in combination. AI modeling indicated that the most important clinical signs are thermal sensitivity loss, nodules and papules, feet paresthesia, number of lesions and gender, but also scaling surface and pruritus that were negatively associated with leprosy. Using elastic-net logistic regression provided a high classification accuracy (90%) and an area under curve (AUC) of 96.46% for leprosy diagnosis. Interpretation: Future validation of these models is underway, gathering larger datasets from populations of different skin types and collecting images with smartphone cameras to mimic real world settings. We hope that the results of our research will lead to clinical solutions that help accelerate global leprosy elimination. Funding: This study was partially funded by Novartis Foundation and Microsoft (in-kind contribution). © 2022 The Authors","10.1016/j.lana.2022.100192","AI; AI4leprosy; Artificial intelligence; Dermatology; Image-based diagnosis; Leprosy; Skin lesions","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126925060&doi=10.1016%2fj.lana.2022.100192&partnerID=40&md5=c767072857408866753753ac240599e6"
"To evaluate the use of tandem and cylinder as an intracavitary brachytherapy device for carcinoma of the cervix with regard to local control and toxicities","Gadda I.R.; Khan N.A.; Wani S.Q.; Baba M.H.","2022","0","1","0","0","0","Unique","0","","","","","","","Introduction: Brachytherapy always remains a keystone in the treatment of gynecological carcinoma for both definitive and adjuvant treatments. Due to the rapid fall-off nature of brachytherapy, the target gets a high dose with a low dose to the normal organs nearby and thereby increasing the tumor control probability. Aims and Objectives: This study aims at the evaluation of local control and toxicities in the carcinoma of the cervix using tandem and cylinder as brachytherapy applicator. Materials and Methods: The study was conducted between January 2014 and December 2018 in a tertiary care hospital. Thirty-one patients who fulfilled our set criterion of Clinical stage IB3-IVA, Performance status Eastern Cooperative Oncology Group 0-2 were selected. All patients were treated initially with external beam radiotherapy and later by high dose rate intracavitary brachytherapy after completion of external beam radiation therapy (EBRT). A dose of 18-21 Gy was delivered to the residual disease in three sessions with a 1-week interval between each session. The dose was optimized in such a way that the organs at risk (OAR), namely bladder and rectum received doses within their tolerance levels. The patients were continuously monitored using Common Terminology Criteria for Adverse Events version 5.0 for both acute and late toxicities and by imaging for local control. Statistical analysis using SPSS Version 20.0 (SPSS Inc., Chicago, Illinois, USA) was used to evaluate the results. Continuous variables were expressed as mean ± standard deviation, and categorical variables were summarized as frequencies and percentages. Results: Out of the 31 patients, 5 (16.1%) experienced radiation-induced Grade 1 skin changes which were due to EBRT, 1 (3.2%) had Grade 1 G. I. T toxicity, 1 (3.2%) had Grade 1 radiation-induced vaginal mucositis after brachytherapy. At 6-8-week follow-up, all the patients showed no evidence of disease on radiological imaging. At 3 months of follow-up, 1 (3.2%) patient had radiation-induced proctitis of Grades 2 and 3 (9.7%) had radiation-induced cystitis of Grades 1 and 1 (3.2%) had Grade 2 cystitis. At 6 months of follow-up, 1 (3.2%) had Grade 1, 1 (3.2%) had Grade 2, and 1 (3.2%) had Grade 3 radiation-induced proctitis. At 3 months of follow-up, 29 (93.5%) patients had no evidence of disease, while 2 (6.5%) were having residual disease on imaging. At 6 months of follow-up, all the patients were disease-free. At 12 months of follow-up, 26 (83.9%) patients were disease-free, 1 (3.2%) had local recurrence, 2 (6.5%) had distant metastasis, and 2 (6.5%) had expired. At 24 months of follow-up, 26 patients were disease-free. Acute and late toxicities were similar to those used in the treatment of carcinoma cervix by standard brachytherapy applicators. Local control was achieved in 83.87% of cases. Two-year survival was 93.5%. Conclusion: We observed that the tandem and cylinder applicator is an acceptable applicator to be used for intracavitary brachytherapy. It is safe and simple besides this; the toxicities and local control are similar to the other standard applicators used in brachytherapy in carcinoma cervix. However, the required dose prescription to point A was not possible in all the patients due to limitations of OARs. Furthermore, long-term follow-up is needed to see the patterns of failure, recurrence-free survival, overall survival, and long-term toxicities in the treated patients.  © 2022 Journal of Cancer Research and Therapeutics.","10.4103/jcrt.jcrt_243_21","and cylinder; brachytherapy tandem; Carcinoma; common terminology criteria for adverse events; dose-volume histogram; intracavitary; toxicity","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135202813&doi=10.4103%2fjcrt.jcrt_243_21&partnerID=40&md5=f644a73d728738f3c9c2e4b099c3e3c0"
"Cancer diagnosis using artificial intelligence: a review","Shastry K.A.; Sanjay H.A.","2022","0","1","0","0","0","First occurrence","0","","","","","","","Artificial intelligence (AI) is the usage of scientific techniques to simulate human intellectual skills and to tackle complex medical issues involving complicated genetic defects such as cancer. The rapid expansion of AI in the past era has paved the way to optimum judgment-making by superior intellect, where the human brain is constrained to manage large information in a limited period. Cancer is a complicated ailment along with several genomic variants. AI-centred systems carry enormous potential in detecting these genomic alterations and abnormal protein communications at a very initial phase. The contemporary biomedical study is also dedicated to bringing AI expertise to hospitals securely and ethically. AI-centred support to diagnosticians and doctors can be the big surge ahead for the forecast of illness threat, identification, diagnosis, and therapies. The applications related to AI and Machine Learning (ML) in the identification of cancer and its therapy possess the potential to provide therapeutic support for quicker planning of a novel therapy for each person. Through the utilization of AI- based methods, scientists can work together in real-time and distribute their expertise digitally to possibly cure billions. In this review, the focus was on the study of linking biology with AI and describe how AI-centred support could assist oncologists in accurate therapy. It is essential to identify new biomarkers that inject drug defiance and discover medicinal goals to improve medication methods. The advent of the “next-generation sequencing” (NGS) programs resolves these challenges and has transformed the prospect of “Precision Oncology” (PO). NGS delivers numerous medical functions which are vital for hazard prediction, initial diagnosis of infection, “Sequence” identification and “Medical Imaging” (MI), precise diagnosis, “biomarker” detection, and recognition of medicinal goals for innovation in medicine. NGS creates a huge repository that requires specific “bioinformatics” sources to examine the information that is pertinent and medically important. The malignancy diagnostics and analytical forecast are improved with NGS and MI that provide superior quality images via AI technology. Irrespective of the advancements in technology, AI faces a few problems and constraints, and the clinical application of NGS continues to be authenticated. Through the steady progress of invention and expertise, the prospects of AI and PO look promising. The purpose of this review was to assess, evaluate, classify, and tackle the present developments in cancer diagnosis utilizing AI methods for breast, lung, liver, skin cancer, and leukaemia. The research emphasizes in what way cancer identification, the treatment procedure is aided by utilizing AI with supervised, unsupervised, and deep learning (DL) methods. Numerous AI methods were assessed on benchmark datasets with respect to “accuracy”, “sensitivity”, “specificity”, and “false-positive” (FP) metrics. Lastly, challenges along with future work were discussed. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.","10.1007/s10462-021-10074-4","Applications; Artificial intelligence; Cancer; Machine learning","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115842891&doi=10.1007%2fs10462-021-10074-4&partnerID=40&md5=c2a4cf7479c19fdcc5d8c4c6454d8762"
"Leveraging Artificial Intelligence to Improve the Diversity of Dermatological Skin Color Pathology: Protocol for an Algorithm Development and Validation Study","Rezk E.; Eltorki M.; El-Dakhakhni W.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: The paucity of dark skin images in dermatological textbooks and atlases is a reflection of racial injustice in medicine. The underrepresentation of dark skin images makes diagnosing skin pathology in people of color challenging. For conditions such as skin cancer, in which early diagnosis makes a difference between life and death, people of color have worse prognoses and lower survival rates than people with lighter skin tones as a result of delayed or incorrect diagnoses. Recent advances in artificial intelligence, such as deep learning, offer a potential solution that can be achieved by diversifying the mostly light-skin image repositories through generating images for darker skin tones. Thus, facilitating the development of inclusive cancer early diagnosis systems that are trained and tested on diverse images that truly represent human skin tones. Objective: We aim to develop and evaluate an artificial intelligence-based skin cancer early detection system for all skin tones using clinical images. Methods: This study consists of four phases: (1) Publicly available skin image repositories will be analyzed to quantify the underrepresentation of darker skin tones, (2) Images will be generated for the underrepresented skin tones, (3) Generated images will be extensively evaluated for realism and disease presentation with quantitative image quality assessment as well as qualitative human expert and nonexpert ratings, and (4) The images will be utilized with available light-skin images to develop a robust skin cancer early detection model. Results: This study started in September 2020. The first phase of quantifying the underrepresentation of darker skin tones was completed in March 2021. The second phase of generating the images is in progress and will be completed by March 2022. The third phase is expected to be completed by May 2022, and the final phase is expected to be completed by September 2022. Conclusions: This work is the first step toward expanding skin tone diversity in existing image databases to address the current gap in the underrepresentation of darker skin tones. Once validated, the image bank will be a valuable resource that can potentially be utilized in physician education and in research applications. Furthermore, generated images are expected to improve the generalizability of skin cancer detection. When completed, the model will assist family physicians and general practitioners in evaluating skin lesion severity and in efficient triaging for referral to expert dermatologists. In addition, the model can assist dermatologists in diagnosing skin lesions.  © 2022 Eman Rezk, Mohamed Eltorki, Wael El-Dakhakhni.","10.2196/34896","Artificial intelligence; Classification; Deep learning; Early diagnosis; Image blending; People of color; Skin cancer; Skin tone diversity","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126129097&doi=10.2196%2f34896&partnerID=40&md5=a5a540bf1ff9c871e2822bf5b818e620"
"Exploring convolutional neural networks with transfer learning for diagnosing Lyme disease from skin lesion images","Hossain S.I.; de Goër de Herve J.; Hassan M.S.; Martineau D.; Petrosyan E.; Corbin V.; Beytout J.; Lebert I.; Durand J.; Carravieri I.; Brun-Jacob A.; Frey-Klett P.; Baux E.; Cazorla C.; Eldin C.; Hansmann Y.; Patrat-Delon S.; Prazuck T.; Raffetin A.; Tattevin P.; Vourc'h G.; Lesens O.; Nguifo E.M.","2022","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Lyme disease which is one of the most common infectious vector-borne diseases manifests itself in most cases with erythema migrans (EM) skin lesions. Recent studies show that convolutional neural networks (CNNs) perform well to identify skin lesions from images. Lightweight CNN based pre-scanner applications for resource-constrained mobile devices can help users with early diagnosis of Lyme disease and prevent the transition to a severe late form thanks to appropriate antibiotic therapy. Also, resource-intensive CNN based robust computer applications can assist non-expert practitioners with an accurate diagnosis. The main objective of this study is to extensively analyze the effectiveness of CNNs for diagnosing Lyme disease from images and to find out the best CNN architectures considering resource constraints. Methods: First, we created an EM dataset with the help of expert dermatologists from Clermont-Ferrand University Hospital Center of France. Second, we benchmarked this dataset for twenty-three CNN architectures customized from VGG, ResNet, DenseNet, MobileNet, Xception, NASNet, and EfficientNet architectures in terms of predictive performance, computational complexity, and statistical significance. Third, to improve the performance of the CNNs, we used custom transfer learning from ImageNet pre-trained models as well as pre-trained the CNNs with the skin lesion dataset HAM10000. Fourth, for model explainability, we utilized Gradient-weighted Class Activation Mapping to visualize the regions of input that are significant to the CNNs for making predictions. Fifth, we provided guidelines for model selection based on predictive performance and computational complexity. Results: Customized ResNet50 architecture gave the best classification accuracy of 84.42% ±1.36, AUC of 0.9189±0.0115, precision of 83.1%±2.49, sensitivity of 87.93%±1.47, and specificity of 80.65%±3.59. A lightweight model customized from EfficientNetB0 also performed well with an accuracy of 83.13%±1.2, AUC of 0.9094±0.0129, precision of 82.83%±1.75, sensitivity of 85.21% ±3.91, and specificity of 80.89%±2.95. All the trained models are publicly available at https://dappem.limos.fr/download.html, which can be used by others for transfer learning and building pre-scanners for Lyme disease. Conclusion: Our study confirmed the effectiveness of even some lightweight CNNs for building Lyme disease pre-scanner mobile applications to assist people with an initial self-assessment and referring them to expert dermatologist for further diagnosis. © 2022 Elsevier B.V.","10.1016/j.cmpb.2022.106624","CNN; Erythema migrans; Explainability; Lyme disease; Transfer learning","33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122813912&doi=10.1016%2fj.cmpb.2022.106624&partnerID=40&md5=789f265e2272fd3fbb23be6505eb2114"
"ExAID: A multimodal explanation framework for computer-aided diagnosis of skin lesions","Lucieri A.; Bajwa M.N.; Braun S.A.; Malik M.I.; Dengel A.; Ahmed S.","2022","1","1","0","0","0","Unique","0","","","","","","","Background and objectives: One principal impediment in the successful deployment of Artificial Intelligence (AI) based Computer-Aided Diagnosis (CAD) systems in everyday clinical workflows is their lack of transparent decision-making. Although commonly used eXplainable AI (XAI) methods provide insights into these largely opaque algorithms, such explanations are usually convoluted and not readily comprehensible. The explanation of decisions regarding the malignancy of skin lesions from dermoscopic images demands particular clarity, as the underlying medical problem definition is ambiguous in itself. This work presents ExAID (Explainable AI for Dermatology), a novel XAI framework for biomedical image analysis that provides multi-modal concept-based explanations, consisting of easy-to-understand textual explanations and visual maps, to justify the predictions. Methods: Our framework relies on Concept Activation Vectors to map human-understandable concepts to those learned by an arbitrary Deep Learning (DL) based algorithm, and Concept Localisation Maps to highlight those concepts in the input space. This identification of relevant concepts is then used to construct fine-grained textual explanations supplemented by concept-wise location information to provide comprehensive and coherent multi-modal explanations. All decision-related information is presented in a diagnostic interface for use in clinical routines. Moreover, the framework includes an educational mode providing dataset-level explanation statistics as well as tools for data and model exploration to aid medical research and education processes. Results: Through rigorous quantitative and qualitative evaluation of our framework on a range of publicly available dermoscopic image datasets, we show the utility of multi-modal explanations for CAD-assisted scenarios even in case of wrong disease predictions. We demonstrate that concept detectors for the explanation of pre-trained networks reach accuracies of up to 81.46%, which is comparable to supervised networks trained end-to-end. Conclusions: We present a new end-to-end framework for the multi-modal explanation of DL-based biomedical image analysis in Melanoma classification and evaluate its utility on an array of datasets. Since perspicuous explanation is one of the cornerstones of any CAD system, we believe that ExAID will accelerate the transition from AI research to practice by providing dermatologists and researchers with an effective tool that they can both understand and trust. ExAID can also serve as the basis for similar applications in other biomedical fields. © 2022","10.1016/j.cmpb.2022.106620","Artificial intelligence in dermatology; Computer-aided diagnosis; Explainable artificial intelligence; Interpretability; Medical image processing; Textual explanations","63","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122635590&doi=10.1016%2fj.cmpb.2022.106620&partnerID=40&md5=d4c94eac4b86f6422e44de08689c37a0"
"Does sex matter? Analysis of sex-related differences in the diagnostic performance of a market-approved convolutional neural network for skin cancer detection","Sies K.; Winkler J.K.; Fink C.; Bardehle F.; Toberer F.; Buhl T.; Enk A.; Blum A.; Stolz W.; Rosenberger A.; Haenssle H.A.","2022","1","1","0","0","0","Unique","0","","","","","","","Background: Advances in biomedical artificial intelligence may introduce or perpetuate sex and gender discriminations. Convolutional neural networks (CNN) have proven a dermatologist-level performance in image classification tasks but have not been assessed for sex and gender biases that may affect training data and diagnostic performance. In this study, we investigated sex-related imbalances in training data and diagnostic performance of a market-approved CNN for skin cancer classification (Moleanalyzer Pro®, Fotofinder Systems GmbH, Bad Birnbach, Germany). Methods: We screened open-access dermoscopic image repositories widely used for CNN training for distribution of sex. Moreover, the sex-related diagnostic performance of the market-approved CNN was tested in 1549 dermoscopic images stratified by sex (female n = 773; male n = 776). Results: Most open-access repositories showed a marked under-representation of images originating from female (40%) versus male (60%) patients. Despite these imbalances and well-known sex-related differences in skin anatomy or skin-directed behaviour, the tested CNN achieved a comparable sensitivity of 87.0% [80.9%–91.3%] versus 87.1% [81.1%–91.4%], specificity of 98.7% [97.4%–99.3%] versus 96.9% [95.2%–98.0%] and ROC-AUC of 0.984 [0.975–0.993] versus 0.979 [0.969–0.988] in dermoscopic images of female versus male origin, respectively. In the sample at hand, sex-related differences in ROC-AUCs were not statistically significant in the per-image analysis nor in an additional per-individual analysis (p ≥ 0.59). Conclusion: Design and training of artificial intelligence algorithms for medical applications should generally acknowledge sex and gender dimensions. Despite sex-related imbalances in open-access training data, the diagnostic performance of the tested CNN showed no sex-related bias in the classification of skin lesions. © 2022 Elsevier Ltd","10.1016/j.ejca.2021.12.034","Automated melanoma detection; Bias; Computer-assisted diagnostics; Convolutional neural network; Deep learning; Dermoscopy; Melanoma; Nevi; Pigmented skin lesions; Sex","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124599912&doi=10.1016%2fj.ejca.2021.12.034&partnerID=40&md5=27c3dae4ed043ffbb1666f0e9ccc3902"
"Cost-effectiveness of Artificial Intelligence as a Decision-Support System Applied to the Detection and Grading of Melanoma, Dental Caries, and Diabetic Retinopathy","Gomez Rossi J.; Rojas-Perilla N.; Krois J.; Schwendicke F.","2022","0","1","0","0","0","Unique","0","","","","","","","Objective: To assess the cost-effectiveness of artificial intelligence (AI) for supporting clinicians in detecting and grading diseases in dermatology, dentistry, and ophthalmology. Importance: AI has been referred to as a facilitator for more precise, personalized, and safer health care, and AI algorithms have been reported to have diagnostic accuracies at or above the average physician in dermatology, dentistry, and ophthalmology. Design, Setting, and Participants: This economic evaluation analyzed data from 3 Markov models used in previous cost-effectiveness studies that were adapted to compare AI vs standard of care to detect melanoma on skin photographs, dental caries on radiographs, and diabetic retinopathy on retina fundus imaging. The general US and German population aged 50 and 12 years, respectively, as well as individuals with diabetes in Brazil aged 40 years were modeled over their lifetime. Monte Carlo microsimulations and sensitivity analyses were used to capture lifetime efficacy and costs. An annual cycle length was chosen. Data were analyzed between February 2021 and August 2021. Exposure: AI vs standard of care. Main Outcomes and Measures: Association of AI with tooth retention-years for dentistry and quality-adjusted life-years (QALYs) for individuals in dermatology and ophthalmology; diagnostic costs. Results: In 1000 microsimulations with 1000 random samples, AI as a diagnostic-support system showed limited cost-savings and gains in tooth retention-years and QALYs. In dermatology, AI showed mean costs of $750 (95% CI, $608-$970) and was associated with 86.5 QALYs (95% CI, 84.9-87.9 QALYs), while the control showed higher costs $759 (95% CI, $618-$970) with similar QALY outcome. In dentistry, AI accumulated costs of 320 (95% CI, 299-341) (purchasing power parity [PPP] conversion, $429 [95% CI, $400-$458]) with 62.4 years per tooth retention (95% CI, 60.7-65.1 years). The control was associated with higher cost, 342 (95% CI, 318-368) (PPP, $458; 95% CI, $426-$493) and fewer tooth retention-years (60.9 years; 95% CI, 60.5-63.1 years). In ophthalmology, AI accrued costs of R $1321 (95% CI, R $1283-R $1364) (PPP, $559; 95% CI, $543-$577) at 8.4 QALYs (95% CI, 8.0-8.7 QALYs), while the control was less expensive (R $1260; 95% CI, R $1222-R $1303) (PPP, $533; 95% CI, $517-$551) and associated with similar QALYs. Dominance in favor of AI was dependent on small differences in the fee paid for the service and the treatment assumed after diagnosis. The fee paid for AI was a factor in patient preferences in cost-effectiveness between strategies. Conclusions and Relevance: The findings of this study suggest that marginal improvements in diagnostic accuracy when using AI may translate into a marginal improvement in outcomes. The current evidence supporting AI as decision support from a cost-effectiveness perspective is limited; AI should be evaluated on a case-specific basis to capture not only differences in costs and payment mechanisms but also treatment after diagnosis. © 2022 Rossi JG et al. JAMA Network Open.","10.1001/jamanetworkopen.2022.0269","","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126653415&doi=10.1001%2fjamanetworkopen.2022.0269&partnerID=40&md5=df3b474fc63df85aa764022127b26b13"
"The Rise of Artificial Intelligence-High Prediction Accuracy in Early Detection of Pigmented Melanoma; [Künstliche Intelligenz auf dem Vormarsch-Hohe Vorhersage-Genauigkeit bei der Früherkennung pigmentierter Melanome]","Jutzi T.B.; Krieghoff-Henning E.I.; Brinker T.J.","2022","0","1","0","0","0","Unique","0","","","","","","","The incidence of malignant melanoma is increasing worldwide. If detected early, melanoma is highly treatable, so early detection is vital. Skin cancer early detection has improved significantly in recent decades, for example by the introduction of screening in 2008 and dermoscopy. Nevertheless, in particular visual detection of early melanomas remains challenging because they show many morphological overlaps with nevi. Hence, there continues to be a high medical need to further develop methods for early skin cancer detection in order to be able to reliably diagnose melanomas at a very early stage. Routine diagnostics for melanoma detection include visual whole body inspection, often supplemented by dermoscopy, which can significantly increase the diagnostic accuracy of experienced dermatologists. A procedure that is additionally offered in some practices and clinics is whole-body photography combined with digital dermoscopy for the early detection of malignant melanoma, especially for monitoring high-risk patients. In recent decades, numerous noninvasive adjunctive diagnostic techniques were developed for the examination of suspicious pigmented moles, that may have the potential to allow improved and, in some cases, automated evaluation of these lesions. First, confocal laser microscopy should be mentioned here, as well as electrical impedance spectroscopy, multiphoton laser tomography, multispectral analysis, Raman spectroscopy or optical coherence tomography. These diagnostic techniques usually focus on high sensitivity to avoid malignant melanoma being overlooked. However, this usually implies lower specificity, which may lead to unnecessary excision of benign lesions in screening. Also, some of the procedures are time-consuming and costly, which also limits their applicability in skin cancer screening. In the near future, the use of artificial intelligence might change skin cancer diagnostics in many ways. The most promising approach may be the analysis of routine macroscopic and dermoscopic images by artificial intelligence. For the classification of pigmented skin lesions based on macroscopic and dermoscopic images, artificial intelligence, especially in form of neural networks, has achieved comparable diagnostic accuracies to dermatologists under experimental conditions in numerous studies. In particular, it achieved high accuracies in the binary melanoma/nevus classification task, but it also performed comparably well to dermatologists in multiclass differentiation of various skin diseases. However, proof of the basic applicability and utility of such systems in clinical practice is still pending. Prerequisites that remain to be established to enable translation of such diagnostic systems into dermatological routine are means that allow users to comprehend the system's decisions as well as a uniformly high performance of the algorithms on image data from other hospitals and practices. At present, hints are accumulating that computer-aided diagnosis systems could provide their greatest benefit as assistance systems, since studies indicate that a combination of human and machine achieves the best results. Diagnostic systems based on artificial intelligence are capable of detecting morphological characteristics quickly, quantitatively, objectively and reproducibly, and could thus provide a more objective analytical basis-in addition to medical experience.  © 2022. Thieme. All rights reserved.","10.1055/a-1514-2013","","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127365602&doi=10.1055%2fa-1514-2013&partnerID=40&md5=55fbf1d4614f39e5b60e491d3d03e519"
"Evaluation of a 3.8-µm laser-induced skin injury and their repair with in vivo OCT imaging and noninvasive monitoring","Fan Y.; Ma Q.; Wang J.; Wang W.; Kang H.","2022","1","1","0","0","0","First occurrence","0","","","","","","","To explore a 3.8-µm laser-induced damage and wound healing effect, we propose using optical coherence tomography (OCT) and a noninvasive monitoring-based in vivo evaluation method to quantitatively and qualitatively analyze the time-dependent biological effect of a 3.8-µm laser. The optical attenuation coefficient (OAC) is computed using a Fourier-domain algorithm. Three-dimensional (3-D) visualization of OCT images has been implemented to visualize the burnt spots. Furthermore, the burnt spots from the 3-D volumetric data was segmented and visualized, and the quantitative parameters of the burnt spots, such as the mean OACs, areas, and volumes, were computed. Then, OCT images and histological sections were analyzed to compare the structural changes. Within a certain radiation range, there is a linear relationship between radiation dose and temperature. Dermoscopic images, OCT images, and histological sections showed that, within a certain dose range, as the radiation doses increased, the cutaneous damage became more serious. One hour after laser radiation, the mean OACs increased and then decreased; the areas of burnt spots always increased and were 0.95 ± 0.07, 1.01 ± 0.06, 1.025 ± 0.07, 0.99 ± 0.07, 0.98 ± 0.07, 1.00 ± 0.07, 0.96 ± 0.05, and 0.98 ± 0.06 mm−1, respectively; the areas were 2.10 ± 0.63, 3.75 ± 1.85, 5.95 ± 1.62, 8.35 ± 0.88, 9.44 ± 1.28, 10.29 ± 0.49, 12.27 ± 0.96, and 13.127 ± 1.90 mm2; and the volumes were 1.54 ± 0.41, 2.86 ± 0.09, 3.73 ± 0.49, 4.14 ± 0.80, 7.21 ± 0.52, 6.77 ± 0.45, 8.36 ± 0.25, and 10.65 ± 0.51 mm3; and 21 days after laser radiation, the volumes were 0.67 ± 0.18, 1.64 ± 0.08, 1.87 ± 0.12, 2.57 ± 0.34, 3.43 ± 0.26, 3.64 ± 0.04, 3.84 ± 0.15, and 4.16 ± 0.53 mm3, respectively. We investigated the time-dependent biological effect of 3.8-µm laser-induced cutaneous damage and wound healing using the quantitative parameters of OCT imaging and noninvasive monitoring. The real-time temperature reflects the photothermal effect during laser radiation of mouse skin. OCT images of burnt spots were segmented to compute the mean OACs, burnt area, and quantitative volumes. This study has the potential for in vivo noninvasive and quantitative clinical evaluation in the future. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","10.1007/s10103-021-03388-w","3.8-µm laser; Laser-induced skin damage; Laser-tissue interaction; Noninvasive quantitative evaluation; Optical coherence tomography","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112063925&doi=10.1007%2fs10103-021-03388-w&partnerID=40&md5=91a002bbf00f857ab69d247665afb1ab"
"Improving Skin cancer Management with ARTificial Intelligence (SMARTI): Protocol for a preintervention/postintervention trial of an artificial intelligence system used as a diagnostic aid for skin cancer management in a specialist dermatology setting","Felmingham C.; MacNamara S.; Cranwell W.; Williams N.; Wada M.; Adler N.R.; Ge Z.; Sharfe A.; Bowling A.; Haskett M.; Wolfe R.; Mar V.","2022","1","1","0","0","0","Unique","0","","","","","","","Introduction Convolutional neural networks (CNNs) can diagnose skin cancers with impressive accuracy in experimental settings, however, their performance in the real-world clinical setting, including comparison to teledermatology services, has not been validated in prospective clinical studies. Methods and analysis Participants will be recruited from dermatology clinics at the Alfred Hospital and Skin Health Institute, Melbourne. Skin lesions will be imaged using a proprietary dermoscopic camera. The artificial intelligence (AI) algorithm, a CNN developed by MoleMap Ltd and Monash eResearch, classifies lesions as benign, malignant or uncertain. This is a preintervention/postintervention study. In the preintervention period, treating doctors are blinded to AI lesion assessment. In the postintervention period, treating doctors review the AI lesion assessment in real time, and have the opportunity to then change their diagnosis and management. Any skin lesions of concern and at least two benign lesions will be selected for imaging. Each participant's lesions will be examined by a registrar, the treating consultant dermatologist and later by a teledermatologist. At the conclusion of the preintervention period, the safety of the AI algorithm will be evaluated in a primary analysis by measuring its sensitivity, specificity and agreement with histopathology where available, or the treating consultant dermatologists' classification. At trial completion, AI classifications will be compared with those of the teledermatologist, registrar, treating dermatologist and histopathology. The impact of the AI algorithm on diagnostic and management decisions will be evaluated by: (1) comparing the initial management decision of the registrar with their AI-assisted decision and (2) comparing the benign to malignant ratio (for lesions biopsied) between the preintervention and postintervention periods. Ethics and dissemination Human Research Ethics Committee (HREC) approval received from the Alfred Hospital Ethics Committee on 14 February 2019 (HREC/48865/Alfred-2018). Findings from this study will be disseminated through peer-reviewed publications, non-peer reviewed media and conferences. Trial registration number NCT04040114. © 2022 BMJ Publishing Group. All rights reserved.","10.1136/bmjopen-2021-050203","adult dermatology; dermatological tumours; dermatology","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122750261&doi=10.1136%2fbmjopen-2021-050203&partnerID=40&md5=2fb3b4870aac4d19444f2dd0ecc12034"
"Implementing of Transfer Learning Method in the Diagnosis of Skin Diseases with Convolutional Neural Networks; [Evrisimsel Sinir Aglan ile Cilt Hastaliklarmm Tanismda Transfer Ogrenme Yonteminin Uygulanmasi]","Sari A.; Nizam A.; Aydin M.","2022","1","1","0","0","0","Unique","0","","","","","","","Millions of people are diagnosed with skin cancer every year around the world, and many people die from this disease. Early diagnosis is important in skin diseases. For this reason, studies on identifying skin diseases with high accuracy using computer-assisted machine learning-based algorithms have gained importance. Convolutional neural networks are frequently used to detect skin diseases quickly and with high accuracy using medical images. In this study, a method using transfer learning is proposed to classify the HAM10000 dataset with high accuracy. Pre-trained models with the ImageNet dataset were transferred and used for classification of the HAM10000 dataset. To demonstrate the effectiveness of the proposed method, Xception and DenseNet201 convolutional neural network models are used separately. In experimental studies, the number of images in the dataset was increased by real-time data augmentation method. In the study, better classification results were obtained in the Xcepiton model compared to the DenseNet201 model, according to the test accuracy, precision, sensitivity and fl-score criteria. It has been observed that higher performances are obtained when the results in this study are compared with similar studies in the literature.  © 2022 IEEE.","10.1109/UBMK55850.2022.9919472","Classification Transfer; Convolutional Neural Networks; HAM10000; Learning","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141865904&doi=10.1109%2fUBMK55850.2022.9919472&partnerID=40&md5=f205fd38594d2cd922191dbe01853bc1"
"Synergy-Net: Artificial Intelligence at the Service of Oncological Prevention","Bollino R.; Bovenzi G.; Cipolletta F.; Docimo L.; Gravina M.; Marrone S.; Parmeggiani D.; Sansone C.","2022","0","1","0","0","0","First occurrence","0","","","","","","","In recent years the constant development of diagnostic techniques has contributed to improving the prognosis of many diseases. Among all, oncological diseases remain those in which a correct and early diagnosis can not only significantly improve the patient’s quality of life but also impact the effectiveness of the therapy itself. Artificial Intelligence can provide valuable aid to this need through the development of predictive models to support the physicians in the diagnosis of the disease. The project “Synergy-Net: Research and Digital Solutions in the Fight Against Oncological Diseases”, born from the collaboration between the Department of Medical and Advanced Surgical Sciences of the University of Campania “L. Vanvitelli”, the National Informatics Inter-University Consortium (National Informatics Inter-University Consortium), Lab ITEM “C. Savy” and Bollino IT S.p.A., aims at the realisation of a technological platform to support the early oncological diagnosis based on the integration of an interoperable communication and clinical data management system leveraging AI. The project has a deeply interdisciplinary nature (lung cancer, breast cancer, colorectal cancer, gastrointestinal carcinomas, prostate cancer, thyroid cancer and malignant skin tumours), which requires the collaboration of very different professionals, including general practitioners, specialist doctors, radiologists, surgeons, pathologists, molecular biologists and oncologists, as well as the support of a team of researchers for aspects related to machine learning and expert system development in health care. The core of the project consists in the creation of a Computer-Aided Detection/Diagnosis (Computer-Aided Detection/Diagnosis) system that, based on Machine Learning and Deep Learning techniques, assists the operator in the analysis of screening data such as anamnestic information, blood tests, instrumental and diagnostic images. The assistance to the operator is achieved by suggesting the portions of information (e.g. regions in an X-ray image) on which to focus more attention. The use of the system will help the physician in the development of increasingly personalised diagnostic and therapeutic strategies, meeting the criteria of tailored therapy/surgery, a desirable objective of any cancer prevention program. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","10.1007/978-3-030-79161-2_16","Artificial intelligence; CAD system; Deep learning; Early diagnosis; Machine learning; Oncological diseases","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115612372&doi=10.1007%2f978-3-030-79161-2_16&partnerID=40&md5=011755cea7f67c6af8077050b42eeb66"
"Does your dermatology classifier know what it doesn't know? Detecting the long-tail of unseen conditions","Guha Roy A.; Ren J.; Azizi S.; Loh A.; Natarajan V.; Mustafa B.; Pawlowski N.; Freyberg J.; Liu Y.; Beaver Z.; Vo N.; Bui P.; Winter S.; MacWilliams P.; Corrado G.S.; Telang U.; Liu Y.; Cemgil T.; Karthikesalingam A.; Lakshminarayanan B.; Winkens J.","2022","0","1","0","0","0","Unique","0","","","","","","","Supervised deep learning models have proven to be highly effective in classification of dermatological conditions. These models rely on the availability of abundant labeled training examples. However, in the real-world, many dermatological conditions are individually too infrequent for per-condition classification with supervised learning. Although individually infrequent, these conditions may collectively be common and therefore are clinically significant in aggregate. To prevent models from generating erroneous outputs on such examples, there remains a considerable unmet need for deep learning systems that can better detect such infrequent conditions. These infrequent ‘outlier’ conditions are seen very rarely (or not at all) during training. In this paper, we frame this task as an out-of-distribution (OOD) detection problem. We set up a benchmark ensuring that outlier conditions are disjoint between the model training, validation, and test sets. Unlike traditional OOD detection benchmarks where the task is to detect dataset distribution shift, we aim at the more challenging task of detecting subtle differences resulting from a different pathology or condition. We propose a novel hierarchical outlier detection (HOD) loss, which assigns multiple abstention classes corresponding to each training outlier class and jointly performs a coarse classification of inliers vs. outliers, along with fine-grained classification of the individual classes. We demonstrate that the proposed HOD loss based approach outperforms leading methods that leverage outlier data during training. Further, performance is significantly boosted by using recent representation learning methods (BiT, SimCLR, MICLe). Further, we explore ensembling strategies for OOD detection and propose a diverse ensemble selection process for the best result. We also perform a subgroup analysis over conditions of varying risk levels and different skin types to investigate how OOD performance changes over each subgroup and demonstrate the gains of our framework in comparison to baseline. Furthermore, we go beyond traditional performance metrics and introduce a cost matrix for model trust analysis to approximate downstream clinical impact. We use this cost matrix to compare the proposed method against the baseline, thereby making a stronger case for its effectiveness in real-world scenarios. © 2021 Elsevier B.V.","10.1016/j.media.2021.102274","Deep learning; Dermatology; Ensembles; Long-tailed recognition; Out-of-distribution detection; Outlier exposure; Representation learning","71","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118505482&doi=10.1016%2fj.media.2021.102274&partnerID=40&md5=1a9379e85e1905e84bcc6b0acbef3e8b"
"Pact-Net: Parallel CNNs and Transformers for medical image segmentation","Chen W.; Zhang R.; Zhang Y.; Bao F.; Lv H.; Li L.; Zhang C.","2023","0","1","0","0","0","Unique","0","","","","","","","Background and Objective: The image segmentation of diseases can help clinical diagnosis and treatment in medical image analysis. Because medical images usually have low contrast and large changes in the size and shape of some structures, this will lead to over-segmentation and under-segmentation. These problems are particularly evident in the segmentation of skin damage. The blurring of the boundary in skin images and the specificity of patients will further increase the difficulty of skin lesion segmentation. Currently, most researchers use deep learning networks to solve these skin segmentation problems. However, traditional convolution methods often fail to obtain satisfactory segmentation performance due to their shortcomings in obtaining global features. Recently, Transformers with good global information extraction ability has achieved satisfactory results in computer vision, which brings new solutions to optimize the model of medical image segmentation further. Methods: To extract more features related to medical image segmentation and effectively use features to further optimize the skin image segmentation model, we designed a network that combines CNNs and Transformers to improve local and global features, called Parallel CNNs and Transformers for Medical Image Segmentation (Pact-Net). Specifically, due to the advantages of Transformers in extracting global information, we create a novel fusion module CSMF, which uses channel and spatial attention mechanism and multi-scale mechanism to effectively fuse the global information extracted by Transformers into the local features extracted by CNNs. Therefore, our Pact-Net dual-branch runs in parallel to effectively capture global and local information. Results: Our Pact-Net exceeds the models submitted on the three datasets ISIC 2016, ISIC 2017 and ISIC 2018, and the indicators required for the datasets reach 86.95%, 79.31% and 84.14%, respectively. We also conducted medical image segmentation experiments on cell and polyp datasets to evaluate the robustness, learning and generalization ability of the network. The ablation study of each part of Pact-Net proves the validity of each component, and the comparison with state-of-the-art methods on different indicators proves the predominance of the network. Conclusions: This paper uses the advantages of CNNs and Transformers in extracting local and global features, and further integrates features for skin lesion segmentation. Compared with the state-of-the-art methods, Pact-Net can achieve the most advanced segmentation ability on the skin lesion segmentation dataset, which can help doctors diagnose and treat diseases. © 2023 Elsevier B.V.","10.1016/j.cmpb.2023.107782","Convolutional neural networks; Fusion; Medical image segmentation; Transformers","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170437305&doi=10.1016%2fj.cmpb.2023.107782&partnerID=40&md5=1a1438f9b7c3219f88090efab94660a0"
"TGDAUNet: Transformer and GCNN based dual-branch attention UNet for medical image segmentation","Song P.; Li J.; Fan H.; Fan L.","2023","0","1","0","0","0","Unique","0","","","","","","","Accurate and automatic segmentation of medical images is a key step in clinical diagnosis and analysis. Currently, the successful application of Transformers’ model in the field of computer vision, researchers have begun to gradually explore the application of Transformers in medical segmentation of images, especially in combination with convolutional neural networks with coding–decoding structure, which have achieved remarkable results in the field of medical segmentation. However, most studies have combined Transformers with CNNs at a single scale or processed only the highest-level semantic feature information, ignoring the rich location information in the lower-level semantic feature information. At the same time, for problems such as blurred structural boundaries and heterogeneous textures in images, most existing methods usually simply connect contour information to capture the boundaries of the target. However, these methods cannot capture the precise outline of the target and ignore the potential relationship between the boundary and the region. In this paper, we propose the TGDAUNet, which consists of a dual-branch backbone network of CNNs and Transformers and a parallel attention mechanism, to achieve accurate segmentation of lesions in medical images. Firstly, high-level semantic feature information of the CNN backbone branches is fused at multiple scales, and the high-level and low-level feature information complement each other's location and spatial information. We further use the polarised self-attentive (PSA) module to reduce the impact of redundant information caused by multiple scales, to better couple with the feature information extracted from the Transformers backbone branch, and to establish global contextual long-range dependencies at multiple scales. In addition, we have designed the Reverse Graph-reasoned Fusion (RGF) module and the Feature Aggregation (FA) module to jointly guide the global context. The FA module aggregates high-level semantic feature information to generate an original global predictive segmentation map. The RGF module captures non-significant features of the boundaries in the original or secondary global prediction segmentation graph through a reverse attention mechanism, establishing a graph reasoning module to explore the potential semantic relationships between boundaries and regions, further refining the target boundaries. Finally, to validate the effectiveness of our proposed method, we compare our proposed method with the current popular methods in the CVC-ClinicDB, Kvasir-SEG, ETIS, CVC-ColonDB, CVC-300,datasets as well as the skin cancer segmentation datasets ISIC-2016 and ISIC-2017. The large number of experimental results show that our method outperforms the currently popular methods. Source code is released at https://github.com/sd-spf/TGDAUNet. © 2023 Elsevier Ltd","10.1016/j.compbiomed.2023.107583","Convolutional neural networks; Graph convolution network; Medical image segmentation; Multi-scale fusion; Transformers","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174910559&doi=10.1016%2fj.compbiomed.2023.107583&partnerID=40&md5=c4f6e420749f993c0ab213f60857e880"
"Comparative study and analysis on skin cancer detection using machine learning and deep learning algorithms","Nancy V.A.O.; Prabhavathy P.; Arya M.S.; Ahamed B.S.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Exposure to UV rays due to global warming can lead to sunburn and skin damage, ultimately resulting in skin cancer. Early prediction of this type of cancer is crucial. A detailed review in this paper explores various algorithms, including machine learning (ML) techniques as well as deep learning (DL) techniques. While deep learning strategies, particularly CNNs, are commonly employed for skin cancer identification and classification, there is also some usage of machine learning and hybrid approaches. These techniques have proven to be effective classifiers of skin lesions, offering promising results for early detection. The paper analyzes various researchers’ reviews on skin cancer diagnosis to identify a suitable methodology for improving diagnostic accuracy. A publicly available dataset of dermoscopic images retrieved from the ISIC archive has been trained and evaluated. Performance analysis is done, considering metrics such as test and validation accuracy. The results indicate that the RF(random forest) algorithm outperforms other machine learning algorithms in both scenarios, with accuracies of 58.57% without augmentation and 87.32% with augmentation. MobileNetv2, ensemble of Dense Net and Inceptionv3 exhibit superior performance. During training without augmentation, MobileNetv2 achieves an accuracy of 88.81%, while the ensemble model achieves an accuracy of 88.80%. With augmentation techniques applied, the accuracies improved to 97.58% and 97.50%, respectively. Furthermore, experiment with a customized convolutional neural network (CNN) model was also conducted, varying the number of layers and applying various hyperparameter tuning methodologies. Suitable architectures, including a CNN with 7 layers and batch normalization, a CNN with 5 layers, and a CNN with 3 layers were identified. These models achieved accuracies of 77.92%, 97.72%, and 98.02% on the raw data and augmentation datasets, respectively. The experimental results suggest that these techniques hold promise for integration into clinical settings, and further research and validation are necessary. The results highlight the effectiveness of transfer learning models, in achieving high accuracy rates. The findings support the future adoption of these techniques in clinical practice, pending further research and validation. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","10.1007/s11042-023-16422-6","Benign; CNN; Deep learning; KNN; Machine learning; Melanoma; Random forest; SVM; VGG16","23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168085065&doi=10.1007%2fs11042-023-16422-6&partnerID=40&md5=79e9576bfa7d7d5233f0f3c0499fc103"
"A comprehensive review of extreme learning machine on medical imaging","Huérfano-Maldonado Y.; Mora M.; Vilches K.; Hernández-García R.; Gutiérrez R.; Vera M.","2023","0","1","0","0","0","Unique","0","","","","","","","The feedforward neural network based on randomization has been of great interest in the scientific community, particularly extreme learning machines, due to its simplicity, training speed, and levels of accuracy comparable to traditional learning algorithms. Extreme learning machines (ELMs) are a type of artificial neural network (ANN) with one or more hidden layers that are trained under supervised, unsupervised, or semi-supervised learning approaches. These networks are widely used in various research areas, such as medical image processing (MI). This research work presents an exhaustive review of extreme learning machines (ELM) and medical image processing (MI), due to the high impact that these networks have had on the scientific community and the importance of MI for physicians who use them to diagnose different injuries and diseases. First, the theoretical construct of ELMs is developed based on the types of supervised, unsupervised, and semi-supervised learning. Then, the importance of MI for the diagnosis of a disease or classification of the most commonly used imaging modalities is analyzed for articles concerning radiography, computed tomography (CT), magnetic resonance (MR), ultrasound (US), and mammography (MG). Next, the reference data sets linked to various human body organs, such as the brain, lungs, skin, eyes, breasts, and cervix are described. Then, a review, analysis, and classification of the development of the last 6 years (2017–2022) of ELMs, based on learning types and MI, is performed. With the information obtained above, a construction of summary tables of the articles, classified according to the type of learning, is performed, highlighting the organ, reference, year, methodology, database, modality, and results. Finally, the discussion, conclusions and challenges related to this topic are presented. The findings indicate that the review articles reported in the literature have not addressed the relationship between ELMs and medical imaging in depth and have excluded key aspects, which are developed in this article. These aspects include a comprehensive analysis of the most popular imaging modalities, a detailed description of both the most popular databases and the most relevant databases for the machine learning community and, finally, the incorporation of schemes that explain the fundamentals of the main learnings considered when generating ELM-based trained smart models, which can be useful for medical image processing. © 2023 Elsevier B.V.","10.1016/j.neucom.2023.126618","Extreme learning machine; Medical imaging; Semi-supervised training; Supervised training; Unsupervised training","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169602025&doi=10.1016%2fj.neucom.2023.126618&partnerID=40&md5=dafd6465c133bedb2118efe198a4024f"
"Enhanced U-Net segmentation with ensemble convolutional neural network for automated skin disease classification","Reddy D.A.; Roy S.; Kumar S.; Tripathi R.","2023","1","1","0","0","0","First occurrence","0","","","","","","","In recent years, skin-related problems induce psychological problems and also injure physical health, particularly if the patient’s face was disfigured or damaged. Smart devices are used for gathering medical images for knowing their skin condition. Skin disease diagnosis is a complex task, which can be solved by adopting different lesion detection and classification approaches. However, the existing challenges cannot be solved by mixing the disease samples from diverse data sources while using simple data fusion approaches. The traditional deep learning-based computer-aided diagnosis approaches suffer from poor extraction of skin lesions due to complex features like limited training datasets, low contrast with the background, presence of artifacts, and fuzzy boundaries. It also includes problems like complex computation, poor generalization, and over-fitting while using the appropriate tuning of large-scale parameters. This paper intends to propose a new framework by using skin lesions classification and segmentation procedures for the automated diagnosis of various skin diseases. The significant stages of the given offered method are pre-processing lesion segmentation and classification. In the beginning, grey-level conversion, hair removal, and contrast enhancement are performed to make the image fit for effective classification. Once image pre-processing is over, the segmentation of skin lesions is done by the enhanced U-Net segmentation, in which the improvement is attained by proposing a hybrid optimization algorithm. Moreover, the offered hybridized optimization algorithm solves the local optimum issues, and also it has the ability for resolving a finite set of problems. Merging the optimization algorithms can balance the exploration and exploitation capability owing to its ability of convergence speed, searching global optimum, and simplicity. The classification is further performed by the optimized ensemble-convolutional neural network (E-CNN). Instead of the fully connected layer in CNN, five different expert systems like random forest, artificial neural network, support vector machine, Adaboost, and Extreme Gradient Boosting (XGBoost) are used for classifying the skin disease by CNN. The system also employs optimization of different parameters in the classification stage to improve computing efficiency and reduce network complexity. The hybrid meta-heuristic termed whale-electric fish optimization (W-EFO) based on EFO and whale optimization algorithm is used for improvising the segmentation and classification task. The comparative analysis over conventional models proves that the developed model encourages effective performance when analyzing diverse measures. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","10.1007/s10115-023-01865-y","Adaboost; Artificial neural network; Enhanced U-Net; Extreme gradient boosting; Optimized ensemble convolutional neural network; Random forest; Skin disease classification; Support vector machine; Whale-electric fish optimization","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160836587&doi=10.1007%2fs10115-023-01865-y&partnerID=40&md5=0b89034bdf1484fa522ac9a4990bd5a6"
"Deep ensemble learning using a demographic machine learning risk stratifier for binary classification of skin lesions using dermatoscopic images","Roge A.; Ting P.; Chern A.; Ting W.","2023","1","1","0","0","0","Unique","0","","","","","","","Background: Skin lesion classification through dermatoscopic images is the most common method for non-invasive diagnostics of dermatologic conditions. Feature extraction through deep learning (DL) based convolutional neural networks (CNNs) provides insight into differential attributes of skin lesions that may pertain to its malignancy. In this study, we sought to improve the performance of standard CNN architectures in skin lesion classification by providing a machine learning (ML)-derived risk score from patient demographic data. Methods: We isolated 1,340 patients (n=2,200) from the HAM10000 dataset with ground-truth diagnoses of either melanoma or benign keratosis-like lesions. Images were split into train, validation, and test, with equal representation of each class in each phase. Baseline CNN performance was established by training 5 DL network architectures (Ni) with 3-fold cross-validation (CV); each of which employed leave-one-out CV and an early stopping criterion. Learning rate (LR) and weight decay (WD) were optimized to yield networks with the highest area under the receiver operating characteristic curve (AUC). For ML training, one-hot encoding was applied to demographic variables (age, sex, localization of lesion). This risk score was added as an additional feature in the final convolutional layers while training CNNs, yielding deep ensemble networks (Ei); all optimized parameters were the same as Ni. Results: Amongst 7 ML classifiers, the random forest algorithm (MRF) yielded the highest test AUC of 0.710. No significant difference was observed in test AUCs across DL networks (Ni =0.81±0.04) and ensemble networks (Ei =0.88±0.03), demonstrating network architecture did not significantly influence performance. A statistically significant increase in AUCs was observed in Ei compared to Ni (P=4.23E−3), indicating a significant contribution with the inclusion of a demographic risk score. Furthermore, activation maps generated for network visualization of test set images show higher specificity of differential features to inform network prediction in Ei. Average predictions on Dholdout are significantly closer to true values in Ei compared to Ni. Conclusions: The ensemble inclusion of a ML risk stratifier from demographic data may improve DL binary classification of dermatoscopic lesions. © Journal of Medical Artificial Intelligence.","10.21037/jmai-23-38","computer vision; Deep ensemble learning; machine learning (ML); melanoma; skin lesion classification","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178933041&doi=10.21037%2fjmai-23-38&partnerID=40&md5=b69a698019c3a8d617bc0d344d938b21"
"Dynamically aggregating MLPs and CNNs for skin lesion segmentation with geometry regularization","Qin C.; Zheng B.; Zeng J.; Chen Z.; Zhai Y.; Genovese A.; Piuri V.; Scotti F.","2023","1","1","0","0","0","Unique","0","","","","","","","Background and objective: Melanoma is a highly malignant skin tumor. Accurate segmentation of skin lesions from dermoscopy images is pivotal for computer-aided diagnosis of melanoma. However, blurred lesion boundaries, variable lesion shapes, and other interference factors pose a challenge in this regard. Methods: This work proposes a novel framework called CFF-Net (Cross Feature Fusion Network) for supervised skin lesion segmentation. The encoder of the network includes dual branches, where the CNNs branch aims to extract rich local features while MLPs branch is used to establish both the global-spatial-dependencies and global-channel-dependencies for precise delineation of skin lesions. Besides, a feature-interaction module between two branches is designed for strengthening the feature representation by allowing dynamic exchange of spatial and channel information, so as to retain more spatial details and inhibit irrelevant noise. Moreover, an auxiliary prediction task is introduced to learn the global geometric information, highlighting the boundary of the skin lesion. Results: Comprehensive experiments using four publicly available skin lesion datasets (i.e., ISIC 2018, ISIC 2017, ISIC 2016, and PH2) indicated that CFF-Net outperformed the state-of-the-art models. In particular, CFF-Net greatly increased the average Jaccard Index score from 79.71% to 81.86% in ISIC 2018, from 78.03% to 80.21% in ISIC 2017, from 82.58% to 85.38% in ISIC 2016, and from 84.18% to 89.71% in PH2 compared with U-Net. Ablation studies demonstrated the effectiveness of each proposed component. Cross-validation experiments in ISIC 2018 and PH2 datasets verified the generalizability of CFF-Net under different skin lesion data distributions. Finally, comparison experiments using three public datasets demonstrated the superior performance of our model. Conclusion: The proposed CFF-Net performed well in four public skin lesion datasets, especially for challenging cases with blurred edges of skin lesions and low contrast between skin lesions and background. CFF-Net can be employed for other segmentation tasks with better prediction and more accurate delineation of boundaries. © 2023","10.1016/j.cmpb.2023.107601","Feature interaction; Geometric information; Multi-layer perceptions; Skin lesion segmentation","13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159630773&doi=10.1016%2fj.cmpb.2023.107601&partnerID=40&md5=1c629cf0429ab05d8f0c02650f8e0155"
"Using feature maps to unpack the CNN ‘Black box’ theory with two medical datasets of different modality","Azam S.; Montaha S.; Fahim K.U.; Rafid A.K.M.R.H.; Mukta M.S.H.; Jonkman M.","2023","0","1","0","0","0","Unique","0","","","","","","","Convolutional neural networks (CNNs) have been established for a comprehensive range of computer vision problems across several benchmarks. Visualization and analysis of feature maps generated by convolutional layers can be an effective approach to explore the hidden and complex characteristic of a CNN model. Convolutional layers provide diverse feature maps however, the extent of this diversity needs to be explored. This research attempts to provide five insights of the ‘Black box’ mechanism of CNNs, using skin cancer dermoscopy and lung scan computed tomography (CT) Scan datasets by statistically analyzing layer by layer (three convolutional layers) feature maps using 17 geometrical and 6 intensity-based features to determine the characteristics and level of diversity. Significance and difference of the feature maps layer by layer, black feature maps analysis, difference of the feature maps to each other and to the original image, variations among the feature maps when running the model multiple times and inter-class variation among the feature maps for different iteration are explored. Various statistical methods including T-test, analysis of variance (ANOVA), mean, median, mean squared error (MSE), peak signal to noise ratio (PSNR), structural similarity index (SSIM), root mean squared error (RMSE), dice similarity score (DSC), universal image quality index (UQI) and Spectral angle mapper (SAM) are employed. Experimental results show that for the skin cancer dermoscopy dataset, a large number of black feature maps are produced (20–60%) while the proportion of black feature maps for the CT Scan dataset is comparatively low (2–20%). This demonstrates that for different datasets, feature maps with diverse characteristics can be produced. The layer by layer differences between the feature maps is evaluated using T-tests and ANOVA for seventeen geometrical features and six intensity-based features. For both datasets across most of the geometrical features and across most of the intensity-based features a significant diversity can be observed. The difference of the feature maps to each other and to the original image is quite high, with MSE values for the dermoscopy and CT Scan datasets in the range of 1860–31,399 and 171–6089, respectively, PSNR 3–15 and 10–25, SSIM values of 0.01–0.84 and 0.3–0.81, RMSE values of 0.81–1 and 0.21–1, DSC values of 0.37–0.53 and 0.47–0.75, UQI values of 0.02–0.86 and 0.01–0.88 and SAM values of 0.12–1.53 and 0.19–1.55 for the dermoscopy and CT Scan datasets respectively. When running the model multiple times (three iterations), a notable iteration by iteration diversity is found in terms of mean, median, maximum and minimum values for most of the geometrical features. The inter-class variation among the feature maps for different iterations and layers are evaluated based on the F-value of the ANOVA test. For the dermoscopy dataset, the highest mean F-value is found for layer 1 and iteration 3 while for the CT scan dataset the highest mean F-value is found for layer 3 and iteration 3 indicating that for these feature maps the highest inter-class dissimilarity is generated. The findings of this study may aid in exploring the complex mechanism of convolutional layers, kernels and feature maps. © 2023","10.1016/j.iswa.2023.200233","ANOVA test; Black box; Convolutional neural network; Feature map analysis; Geometric feature; T-test","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159372463&doi=10.1016%2fj.iswa.2023.200233&partnerID=40&md5=0a35beaca70aecf58a9a29121d71bf13"
"Design and development of deep learning model for predicting skin cancer and deployed using a mobile app","Madiwal S.M.; Sudhakar M.; Subramanian M.; Venkata Srinivasulu B.; Nagaprasad S.; Khurana M.","2023","1","1","0","0","0","Unique","0","","","","","","","Melanoma, the deadliest form of skin cancer, is becoming more common every year, according to the American Cancer Society. As a result of the artifacts, low contrast, and similarity to other lesions, such as moles and scars on the skin, diagnosing skin cancer from the lesions might be difficult. Skin cancer can be diagnosed using a variety of techniques, including dermatology, dermoscopic examination, biopsy and histological testing. Even though the vast majority of skin cancers are non-cancerous and do not constitute a threat to survival, certain more malignant tumors can be fatal if not detected and treated on time. In reality, it is not feasible for every patient to have a dermatologist do a complete examination of his or her skin at every visit to the doctor's office or clinic. To solve this challenge, numerous investigations are being conducted to provide computer-aided diagnoses. In this work, skin cancer can be predicted from an image of the skin using deep learning techniques such as convolutional neural networks. The accuracy and loss functions of the model are used to evaluate its overall performance. The mobile app is created to detect skin cancer using the developed model. As soon as the images have been submitted, the app can communicate with the user about their progress. © 2023, Bentham Books imprint. All rights reserved.","10.2174/9789815136531123010012","Convolutional Neural Network; Deep Learning Algorithm; Melanoma; Web Development","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85205276844&doi=10.2174%2f9789815136531123010012&partnerID=40&md5=d1333972340ebc0b10d5969d551c591c"
"Multiclass skin lesion localization and classification using deep learning based features fusion and selection framework for smart healthcare","Maqsood S.; Damaševičius R.","2023","1","1","0","0","0","Unique","0","","","","","","","Background: The idea of smart healthcare has gradually gained attention as a result of the information technology industry's rapid development. Smart healthcare uses next-generation technologies i.e., artificial intelligence (AI) and Internet of Things (IoT), to intelligently transform current medical methods to make them more efficient, dependable and individualized. One of the most prominent uses of telemedicine and e-health in medical image analysis is teledermatology. Telecommunications technologies are used in this industry to send medical information to professionals. Teledermatology is a useful method for the identification of skin lesions, particularly in rural locations, because the skin is visually perceptible. One of the most recent tools for diagnosing skin cancer is dermoscopy. To classify skin malignancies, numerous computational approaches have been proposed in the literature. However, difficulties still exist i.e., lesions with low contrast, imbalanced datasets, high level of memory complexity, and the extraction of redundant features. Methods: In this work, a unified CAD model is proposed based on a deep learning framework for skin lesion segmentation and classification. In the proposed approach, the source dermoscopic images are initially pre-processed using a contrast enhancement based modified bio-inspired multiple exposure fusion approach. In the second stage, a custom 26-layered convolutional neural network (CNN) architecture is designed to segment the skin lesion regions. In the third stage, four pre-trained CNN models (Xception, ResNet-50, ResNet-101 and VGG16) are modified and trained using transfer learning on the segmented lesion images. In the fourth stage, the deep features vectors are extracted from all the CNN models and fused using the convolutional sparse image decomposition fusion approach. In the fifth stage, the univariate measurement and Poisson distribution feature selection approach is used for the best features selection for classification. Finally, the selected features are fed to the multi-class support vector machine (MC-SVM) for the final classification. Results: The proposed approach employed to the HAM10000, ISIC2018, ISIC2019, and PH2 datasets and achieved an accuracy of 98.57%, 98.62%, 93.47%, and 98.98% respectively which are better than previous works. Conclusion: When compared to renowned state-of-the-art methods, experimental results show that the proposed skin lesion detection and classification approach achieved higher performance in terms of both visually and enhanced quantitative evaluation with enhanced accuracy. © 2023 Elsevier Ltd","10.1016/j.neunet.2023.01.022","Classification; Deep features; Deep learning; Dermoscopy imaging; Skin cancer; Skin lesion analysis","107","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146720766&doi=10.1016%2fj.neunet.2023.01.022&partnerID=40&md5=9f5ab9b84f343892720a37b6fbb6c325"
"Multi-site cross-organ calibrated deep learning (MuSClD): Automated diagnosis of non-melanoma skin cancer","Zhou Y.; Koyuncu C.; Lu C.; Grobholz R.; Katz I.; Madabhushi A.; Janowczyk A.","2023","1","1","0","0","0","Unique","0","","","","","","","Although deep learning (DL) has demonstrated impressive diagnostic performance for a variety of computational pathology tasks, this performance often markedly deteriorates on whole slide images (WSI) generated at external test sites. This phenomenon is due in part to domain shift, wherein differences in test-site pre-analytical variables (e.g., slide scanner, staining procedure) result in WSI with notably different visual presentations compared to training data. To ameliorate pre-analytic variances, approaches such as CycleGAN can be used to calibrate visual properties of images between sites, with the intent of improving DL classifier generalizability. In this work, we present a new approach termed Multi-Site Cross-Organ Calibration based Deep Learning (MuSClD) that employs WSIs of an off-target organ for calibration created at the same site as the on-target organ, based off the assumption that cross-organ slides are subjected to a common set of pre-analytical sources of variance. We demonstrate that by using an off-target organ from the test site to calibrate training data, the domain shift between training and testing data can be mitigated. Importantly, this strategy uniquely guards against potential data leakage introduced during calibration, wherein information only available in the testing data is imparted on the training data. We evaluate MuSClD in the context of the automated diagnosis of non-melanoma skin cancer (NMSC). Specifically, we evaluated MuSClD for identifying and distinguishing (a) basal cell carcinoma (BCC), (b) in-situ squamous cell carcinomas (SCC-In Situ), and (c) invasive squamous cell carcinomas (SCC-Invasive), using an Australian (training, n = 85) and a Swiss (held-out testing, n = 352) cohort. Our experiments reveal that MuSCID reduces the Wasserstein distances between sites in terms of color, contrast, and brightness metrics, without imparting noticeable artifacts to training data. The NMSC-subtyping performance is statistically improved as a result of MuSCID in terms of one-vs. rest AUC: BCC (0.92 vs 0.87, p = 0.01), SCC-In Situ (0.87 vs 0.73, p = 0.15) and SCC-Invasive (0.92 vs 0.82, p = 1e-5). Compared to baseline NMSC-subtyping with no calibration, the internal validation results of MuSClD (BCC (0.98), SCC-In Situ (0.92), and SCC-Invasive (0.97)) suggest that while domain shift indeed degrades classification performance, our on-target calibration using off-target tissue can safely compensate for pre-analytical variabilities, while improving the robustness of the model. © 2022","10.1016/j.media.2022.102702","Deep learning; Digital pathology; Domain adaptation; Domain shift; Non-melanoma skin cancer; Subtyping","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143881051&doi=10.1016%2fj.media.2022.102702&partnerID=40&md5=c6c66d2426a5630b20d8a2eb6b1d5e5b"
"Skin Cancer Classification Utilizing a Hybrid Model of Machine Learning Models Trained on Dermoscopic Images","Ahmed S.G.; Zeng F.; Alrifaey M.; Ahmadipour M.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is the most prevalent kind of cancer, and early recognition is critical for successful treatment. Computer-aided skin cancer detection systems have been developed to use image analysis techniques to examine skin lesions and detect suspicious ones. Artificial intelligence techniques, especially machine learning, have been widely applied for skin cancer detection. Using machine learning algorithms, it is possible to scrutinize vast databases of skin lesion images, recognize patterns, and classify lesions as either benign or malignant. Similarly, machine learning algorithms can learn to identify key features from skin lesion images and classify them with remarkable precision. Technology integration has transformed the area of skin cancer detection, and computer-assisted methods have the potential to increase the speed and accuracy of skin cancer diagnosis, resulting in better patient outcomes. Our model uses several machine learning algorithms proposed to classify skin cancer, such as Convolutional Neural Networks (CNNs), Support Vector Machine (SVM), Naïve Bayes, and K-Nearest Neighbor (K-NN) classifiers. The research problem revolves around developing algorithms and techniques to analyze skin lesion images and accurately categorize them as either benign or malignant. The primary aim is to develop an accurate, robust, and automated system that can assist clinicians in the early-stage diagnosis of skin cancer, thereby enhancing patient outcomes and reducing healthcare costs. This paper evaluates various machine learning algorithms and selects a model based on its optimal performance in predicting and classifying skin cancer. The dataset utilized in this paper for training and testing the model comprises 1439 images of benign skin lesions and 1196 images of melanoma lesions, gathered from the International Skin Image Collaboration (ISIC). The proposed model is composed of five steps: image acquisition, preprocessing, segmentation, feature extraction, and classification. In addition to using machine learning algorithms for evaluating the model, such as CNNS, SVM, Naïve Bayes, and (K-NN) classifiers, we achieved 99.5%, 95.6%, 86.1% and 77.1%, respectively. Furthermore, our results demonstrate that Convolutional Neural Networks (CNNs) performed most accurately in visual image tasks, as shown in our proposed work.  © 2023 IEEE.","10.1109/eSmarTA59349.2023.10293619","Artificial intelligence; Benign; Machine Learning; Melanoma; SIC; Skin Cancer; Skin Diagnosis","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178099428&doi=10.1109%2feSmarTA59349.2023.10293619&partnerID=40&md5=5ff41b1a2a2721f62b66d5abd11e3448"
"26th International Conference on Medical Image Computing and Computer-Assisted Intervention , MICCAI 2023","","2023","0","1","0","0","0","First occurrence","0","","","","","","","The proceedings contain 35 papers. The special focus in this conference is on Medical Image Computing and Computer-Assisted Intervention. The topics include: Communication-Efficient Federated Skin Lesion Classification with Generalizable Dataset Distillation; GPC: Generative and General Pathology Image Classifier; towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation; concept Bottleneck with Visual Concept Filtering for Explainable Medical Image Classification; SAM Meets Robotic Surgery: An Empirical Study on Generalization, Robustness and Adaptation; evaluation and Improvement of Segment Anything Model for Interactive Histopathology Image Segmentation; task-Driven Prompt Evolution for Foundation Models; generalizing Across Domains in Diabetic Retinopathy via Variational Autoencoders; CEmb-SAM: Segment Anything Model with Condition Embedding for Joint Learning from Heterogeneous Datasets; histopathological Image Analysis with Style-Augmented Feature Domain Mixing for Improved Generalization; DISBELIEVE: Distance Between Client Models Is Very Essential for Effective Local Model Poisoning Attacks; AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets; ConDistFL: Conditional Distillation for Federated Learning from Partially Annotated Data; validation of Federated Unlearning on Collaborative Prostate Segmentation; federated Model Aggregation via Self-supervised Priors for Highly Imbalanced Medical Image Classification; FedAutoMRI: Federated Neural Architecture Search for MR Image Reconstruction; Fed-CoT: Co-teachers for Federated Semi-supervised MS Lesion Segmentation; splitFed Resilience to Packet Loss: Where to Split, that is the Question; test-Time Selection for Robust Skin Lesion Analysis; global and Local Explanations for Skin Cancer Diagnosis Using Prototypes; evidence-Driven Differential Diagnosis of Malignant Melanoma; an Interpretable Machine Learning Model with Deep Learning-Based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease; generating Chinese Radiology Reports from X-Ray Images: A Public Dataset and an X-ray-to-Reports Generation Method; gradient Self-alignment in Private Deep Learning; ISIC Preface; Continual-GEN: Continual Group Ensembling for Domain-agnostic Skin Lesion Classification; multimodal Learning for Improving Performance and Explainability of Chest X-Ray Classification; cross-Task Attention Network: Improving Multi-task Learning for Medical Imaging Applications.","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180634676&partnerID=40&md5=e27b1ed6bef8d2d27f9a49d50c407c0f"
"Skin cancer diagnosis based on deep transfer learning and sparrow search algorithm","Balaha H.M.; Hassan A.E.-S.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer affects the lives of millions of people every year, as it is considered the most popular form of cancer. In the USA alone, approximately three and a half million people are diagnosed with skin cancer annually. The survival rate diminishes steeply as the skin cancer progresses. Despite this, it is an expensive and difficult procedure to discover this cancer type in the early stages. In this study, a threshold-based automatic approach for skin cancer detection, classification, and segmentation utilizing a meta-heuristic optimizer named sparrow search algorithm (SpaSA) is proposed. Five U-Net models (i.e., U-Net, U-Net++, Attention U-Net, V-net, and Swin U-Net) with different configurations are utilized to perform the segmentation process. Besides this, the meta-heuristic SpaSA optimizer is used to perform the optimization of the hyperparameters using eight pre-trained CNN models (i.e., VGG16, VGG19, MobileNet, MobileNetV2, MobileNetV3Large, MobileNetV3Small, NASNetMobile, and NASNetLarge). The dataset is gathered from five public sources in which two types of datasets are generated (i.e., 2-classes and 10-classes). For the segmentation, concerning the “skin cancer segmentation and classification” dataset, the best reported scores by U-Net++ with DenseNet201 as a backbone architecture are 0.104, 94.16 % , 91.39 % , 99.03 % , 96.08 % , 96.41 % , 77.19 % , 75.47 % in terms of loss, accuracy, F1-score, AUC, IoU, dice, hinge, and squared hinge, respectively, while for the “PH2” dataset, the best reported scores by the Attention U-Net with DenseNet201 as backbone architecture are 0.137, 94.75 % , 92.65 % , 92.56 % , 92.74 % , 96.20 % , 86.30 % , 92.65 % , 69.28 % , and 68.04 % in terms of loss, accuracy, F1-score, precision, sensitivity, specificity, IoU, dice, hinge, and squared hinge, respectively. For the “ISIC 2019 and 2020 Melanoma” dataset, the best reported overall accuracy from the applied CNN experiments is 98.27 % by the MobileNet pre-trained model. Similarly, for the “Melanoma Classification (HAM10K)” dataset, the best reported overall accuracy from the applied CNN experiments is 98.83 % by the MobileNet pre-trained model. For the “skin diseases image” dataset, the best reported overall accuracy from the applied CNN experiments is 85.87 % by the MobileNetV2 pre-trained model. After computing the results, the suggested approach is compared with 13 related studies. © 2022, The Author(s).","10.1007/s00521-022-07762-9","Convolution neural network (CNN); Deep learning (DL); Melanoma cancer; Meta-heuristic optimization; Non-melanoma cancer; Segmentation; Skin cancer; Sparrow search algorithm (SpaSA)","103","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138714690&doi=10.1007%2fs00521-022-07762-9&partnerID=40&md5=1b9c56d9a1979261bd6a0a526d8c9fbb"
"Activation Functions for Analysis of Skin Lesion and Melanoma Cancer Detection","Anupama D.; Sumathi D.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is the most common occurred cancer on the body which is time consuming and tedious. Detection of this cancer at early stage has a high effect on the disease treatment. Skin cancers are defined as either malignant or benign cells. With the advancements in deep learning algorithms in image processing and computer vision, there was a great evolution in medical image analysis especially for the classification of melanoma from skin cancer diagnosis. A convolution layer with activation function will move to next layer. The activation function plays a prominent role in the deep neural networks. On the proposed transfer learning method, various activation functions are implemented to check the model accuracy and to classify the melanoma. For experimental purpose, we use the datasets collected from various patients toward melanoma detection. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","10.1007/978-981-19-4052-1_40","Activation functions; Classification; Deep learning; Melanoma; Skin cancer; Skin lesion images","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138803233&doi=10.1007%2f978-981-19-4052-1_40&partnerID=40&md5=c6971fe5b1cfa6d3b643f6f7a150d85c"
"Transfer Learning for Automated Melanoma Classification System: Data Augmentation","Yousra D.; Abdelhakim A.B.; Mohamed B.A.","2023","0","1","0","0","0","First occurrence","0","","","","","","","Skin cancer, and especially melanoma, is the most dangerous diseases today because it is one of the fastest and active growing cancers worldwide [1]. It can be diagnosed and analyzed by specialist dermatologists, using Dermoscopic imaging techniques. Early detection is an important factor in reducing death rates related with this type of skin cancer. This is the cause why many researchers in this domain wanted to obtain accurate computer-aided diagnosis systems based with Artificial Intelligence (AI) algorithms to simplify the early detection of such diseases. Thanks to wide applications of Artificial Intelligence and especially Deep Learning in healthcare, including Dermatology. In this paper, we concentrate on the problem of skin lesion classification particularly melanoma, where we proposed an effective, accurate and fast model to solve the problem of Dermoscopic image classification containing a cutaneous melanoma as malignant or benign. This model based on different pre-trained deep learning algorithms with fine-tuned to maximizing accuracy. The proposed model includes a preprocessing step for data organization, normalization and data augmentation; after all, we employed transfer learning algorithms with fine-tuning, where we first change the last layer to match the classes in our dataset, then, used the different strategies of freezing layer’s techniques of fine-tuning, add other layers and finally retrain the layers of the network. We used six algorithms for state-of-the-art models previously developed for the ImageNet Large Scale Visual Recognition Challenge (ILSVRC): (EfficientNetB0) / (Mobile-Net) / (InceptionV3) / (XceptionV3) / (VGG19) / (ResNet152V2), Finally, we have evaluated our performances models using the following three metrics: Accuracy, Precision, F1 Score. Our experimental study and the results prove that transfer learning is efficient in skin melanoma cancer classification. Mobile-Net, XceptionV3, Inceptionv3 and VGG19 obtained the highest accuracy of 87%, 80%, 80%, 82%, respectively. Resnet152v2 obtained the lowest accuracy of 70%. However, Efficient-Net outperformed all other models in melanoma detection with an accuracy 92%. The outstanding performance of these pre-trained model can significantly improve the speed, efficacy and accuracy of skin cancer diagnosis. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.","10.1007/978-3-031-26852-6_30","Data augmentation; Deep learning; Dermoscopy; Melanoma; Skin cancer; Transfer learning","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151165956&doi=10.1007%2f978-3-031-26852-6_30&partnerID=40&md5=e20153efdf9c1af653a726038c743ac0"
"High-fidelity detection, subtyping, and localization of five skin neoplasms using supervised and semi-supervised learning","Requa J.; Godard T.; Mandal R.; Balzer B.; Whittemore D.; George E.; Barcelona F.; Lambert C.; Lee J.; Lambert A.; Larson A.; Osmond G.","2023","1","1","0","0","0","Unique","0","","","","","","","Background: Skin cancers are the most common malignancies diagnosed worldwide. While the early detection and treatment of pre-cancerous and cancerous skin lesions can dramatically improve outcomes, factors such as a global shortage of pathologists, increased workloads, and high rates of diagnostic discordance underscore the need for techniques that improve pathology workflows. Although AI models are now being used to classify lesions from whole slide images (WSIs), diagnostic performance rarely surpasses that of expert pathologists. Objectives: The objective of the present study was to create an AI model to detect and classify skin lesions with a higher degree of sensitivity than previously demonstrated, with potential to match and eventually surpass expert pathologists to improve clinical workflows. Methods: We combined supervised learning (SL) with semi-supervised learning (SSL) to produce an end-to-end multi-level skin detection system that not only detects 5 main types of skin lesions with high sensitivity and specificity, but also subtypes, localizes, and provides margin status to evaluate the proximity of the lesion to non-epidermal margins. The Supervised Training Subset consisted of 2188 random WSIs collected by the PathologyWatch (PW) laboratory between 2013 and 2018, while the Weakly Supervised Subset consisted of 5161 WSIs from daily case specimens. The Validation Set consisted of 250 curated daily case WSIs obtained from the PW tissue archives and included 50 “mimickers”. The Testing Set (3821 WSIs) was composed of non-curated daily case specimens collected from July 20, 2021 to August 20, 2021 from PW laboratories. Results: The performance characteristics of our AI model (i.e., Mihm) were assessed retrospectively by running the Testing Set through the Mihm Evaluation Pipeline. Our results show that the sensitivity of Mihm in classifying melanocytic lesions, basal cell carcinoma, and atypical squamous lesions, verruca vulgaris, and seborrheic keratosis was 98.91% (95% CI: 98.27%, 99.55%), 97.24% (95% CI: 96.15%, 98.33%), 95.26% (95% CI: 93.79%, 96.73%), 93.50% (95% CI: 89.14%, 97.86%), and 86.91% (95% CI: 82.13%, 91.69%), respectively. Additionally, our multi-level (i.e., patch-level, ROI-level, and WSI-level) detection algorithm includes a qualitative feature that subtypes lesions, an AI overlay in the front-end digital display that localizes diagnostic ROIs, and reports on margin status by detecting overlap between lesions and non-epidermal tissue margins. Conclusions: Our AI model, developed in collaboration with dermatopathologists, detects 5 skin lesion types with higher sensitivity than previously published AI models, and provides end users with information such as subtyping, localization, and margin status in a front-end digital display. Our end-to-end system has the potential to improve pathology workflows by increasing diagnostic accuracy, expediting the course of patient care, and ultimately improving patient outcomes. © 2022 The Authors","10.1016/j.jpi.2022.100159","Artificial intelligence; Dermatopathology; Semi-supervised learning; Skin cancer detection; Supervised learning; Whole-slide imaging","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143838503&doi=10.1016%2fj.jpi.2022.100159&partnerID=40&md5=71f04b1410851a74ff5a5b3339206fac"
"Learning from dermoscopic images in association with clinical metadata for skin lesion segmentation and classification","Dong C.; Dai D.; Zhang Y.; Zhang C.; Li Z.; Xu S.","2023","1","1","0","0","0","Unique","0","","","","","","","Automatic segmentation and classification of lesions are two clinically significant tasks in the computer-aided diagnosis of skin diseases. Both tasks are challenging due to the nonnegligible lesion differences in dermoscopic images from different patients. In this paper, we propose a novel pipeline to efficiently implement skin lesions’ segmentation and classification tasks, which consists of a segmentation network and a classification network. To improve the performance of the segmentation network, we propose a novel module of Multi-Scale Holistic Feature Exploration (MSH) to thoroughly exploit perceptual clues latent among multi-scale feature maps as synthesized by the decoder. The MSH module enables holistic exploration of features across multiple scales to more effectively support downstream image analysis tasks. To boost the performance of the classification network, we propose a novel module of Cross-Modality Collaborative Feature Exploration (CMC) to discover latent discriminative features by collaboratively exploiting potential relationships between cross-modal features of dermoscopic images and clinical metadata. The CMC module enables dynamically capturing versatile interaction effects among cross-modal features during the model's representation learning procedure by discriminatively and adaptively learning the interaction weight associated with each crossmodality feature pair. In addition, to effectively reduce background noise and boost the lesion discrimination ability of the classification network, we crop the images based on lesion masks generated by the best segmentation model. We evaluate the proposed pipeline on the four public skin lesion datasets, where the ISIC 2018 and PH2 are for segmentation, and the ISIC 2019 and ISIC 2020 are combined into a new dataset, ISIC 2019&2020, for classification. It achieves a Jaccard index of 83.31% and 90.14% in skin lesion segmentation, an AUC of 97.98% and an Accuracy of 92.63% in skin lesion classification, which is superior to the performance of representative state-of-the-art skin lesion segmentation and classification methods. Last but not least, the new model for segmentation utilizes much fewer model parameters (3.3 M) than its peer approaches, leading to a greatly reduced number of labeled samples required for model training, which obtains substantially stronger robustness than its peers. © 2022 Elsevier Ltd","10.1016/j.compbiomed.2022.106321","Cross-modality collaborative feature exploration; Multi-scale holistic feature exploration; Skin lesion classification; Skin lesion segmentation","34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143536851&doi=10.1016%2fj.compbiomed.2022.106321&partnerID=40&md5=90a2a629449fc1bc62bc728bdd2f881a"
"Skin cancer detection using image features and ensemble learning","Behera N.P.; Pratap Singh A.; Rout J.K.; Kumar Balabantaray B.; Swetapadma A.","2023","1","1","0","0","0","Unique","0","","","","","","","This work suggests a technique for skin cancer detection using CT image features and boosting ensemble learning method. First, the skin cancer images are collected. Then the images are processed, and the features extraction using a convolutional neural network (CNN). The extracted features are then used as input for the ensemble classifier to detect skin cancer. The ensemble classifiers used are bagging, AdaBoost, gentleboost, logitboost, and XGBoost. The results of the XGBoost method are better than all other ensemble-learning methods used. The performance measures such as sensitivity, specificity, and accuracy of the method in detecting skin cancer are 97.95%, 97.00% and 97.45% respectively. The proposed technique can be used to give second opinions to doctors. © 2023 IEEE.","10.1109/OCIT59427.2023.10431321","Boosting and bagging; ensemble learning; image features; Skin cancer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186671787&doi=10.1109%2fOCIT59427.2023.10431321&partnerID=40&md5=c4c3db0548ec27409064be39be9043f1"
"Design and Implementation of Convolutional Neural Network Model for Melanoma Classification","Hamsalekha R.; Satheesha T.Y.","2023","0","1","0","0","0","First occurrence","0","","","","","","","The automated detection of melanomas out of dermo-scopic skins sample remains a particularly issue of subject. But, these problems can be remedied by deploying deep learning technique like a tools of mechanic visions. This would allow for more accurate image analysis. This research proposes the use of an automatic mechanism of melanoma grouper that depends on a (DCNN) deep convolutional neural network. The goal of this classifier is to accurately differentiate between benign and malignant forms of melanoma. The structure of the DCNN was deliberately built by arranging a number of substrates which were accountable so as to collect lower-level to higher-level information from skin photos form of specific manner. This arrangement was done in order to produce the structure. Choosing a large number of filters and the sizes of those filters, using deep learning layers that are suitable, determining the network's depth, and tweaking hyperparameters are other key variables in the creation of a DCNN. In order to efficiently diagnose melanoma skin cancer, the primary objective is to propose a DCNN that is both lightweight and less difficult in comparison to other cutting-edge methods. For the purpose of this inquiry, which involved analyzing numerous cancer samples, dermoscopic images were obtained via the utilization of the datastores made available by the International Skin Imaging Collaboration (ISIC 2020). In order to evaluate the model, we looked at its accuracy, precision, recall, and specificity, as well as its F1 score. On the ISIC datasets from 2020, suggested DCNN grouper attained precision of about 90.42%, 81.41% and 88.23% respectively. This demonstrates high performance in contrast to the performance of other cutting-edge networks. As a result, the proposed technique can provide a less difficult and more cutting-edge infrastructure pertaining to automatic identification of melanoma, speeding procedures in diagnosing it, hence increasing the likelihood of successfully saving a life. © 2023 IEEE.","10.1109/GCAT59970.2023.10353265","Classification; ISIC Dataset, CNN; Melanoma; Skin cancer","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182566165&doi=10.1109%2fGCAT59970.2023.10353265&partnerID=40&md5=3c447850ad2aa3c645c99cae8f9c31e5"
"Categorical Weighting Domination for Imbalanced Classification with Skin Cancer in Intelligent Healthcare Systems","Hoang V.-D.; Vo X.-T.; Jo K.-H.","2023","1","1","0","0","0","First occurrence","0","","","","","","","In the field of dermatological diseases, especially for skin cancer, machine learning (ML) methods are used to classify melanoma and nevus using skin images. ML techniques result in high accuracy of diagnostic tasks since they are trained on balanced datasets. However, MLs working with imbalanced datasets produce erroneous results on precision, sensitivity, and specificity measured criteria. To deal with this problem, an augmentation approach combined with a category seesaw is used for the compensation factor. It increases the penalty for misclassified instances, thereby reducing the occurrence of false positives within the less common categories. This paper presents an approach to improve the efficiency of DCNN for classifying multi-class medical images on imbalanced datasets. The solution consists of three major contributions: (1) feature extraction based on some backbone models with customizing fully connected layers for classifier layers, (2) optimizing loss function (LF) and training parameters, (3) solving the problem of imbalanced samples using optimizing domination of weights between asymmetric classes with majority and minority categories. The method was evaluated and analyzed using the ISIC2018 benchmark and Chest X-ray dataset. Some well-known backbones were used for this study, e.g., EfficientNets, MobileNets, and DenseNets. The use of these backbones is to demonstrate that our methods are more efficient and stable in both light and heavy DCNN architectures. We also provide comparisons with existing methods that deal with the imbalance problem, e.g., data augmentation (AU), downsamples, customizing LF, and focal loss method (FL) for focusing on hard samples. Experimental results showed that these methods achieve good performance. However, there are several problems caused by generating new samples, and weighting samples, such as data overloading to train classifier models, a corrupt problem when applied to imbalanced data. Moreover, the FL method produced insufficient results on various DCNN backbones. Differently, our approach solves the imbalanced dataset based on boosting the sample weights of the minority and reducing the impact ratio of samples in majority categories. This strategy results in high precision and stable performance with various DCNN models without augmenting the dataset. Experiment results on ISIC2018 dataset demonstrated that our approach achieves more efficiency than other methods in some specific evaluation criteria as follows: higher than the FL method with 2.73% recall, 2.63% precision, 2.81% specificity, and 3.09% F1 using EfficientNet backbones; higher than AU method with 5.16% recall, 5.97% precision, 8.93% specificity, 6.16% F1 using DenseNet backbones.  © 2013 IEEE.","10.1109/ACCESS.2023.3319087","DCNN; Deep learning; disease diagnosis; feature extraction; imbalance data; machine learning","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172984523&doi=10.1109%2fACCESS.2023.3319087&partnerID=40&md5=708f4332ba7e44b70e4d938a47038b20"
"Incorporating a Novel Dual Transfer Learning Approach for Medical Images","Mukhlif A.A.; Al-Khateeb B.; Mohammed M.A.","2023","0","1","0","0","0","First occurrence","0","","","","","","","Recently, transfer learning approaches appeared to reduce the need for many classified medical images. However, these approaches still contain some limitations due to the mismatch of the domain between the source domain and the target domain. Therefore, this study aims to propose a novel approach, called Dual Transfer Learning (DTL), based on the convergence of patterns between the source and target domains. The proposed approach is applied to four pre-trained models (VGG16, Xception, ResNet50, MobileNetV2) using two datasets: ISIC2020 skin cancer images and ICIAR2018 breast cancer images, by fine-tuning the last layers on a sufficient number of unclassified images of the same disease and on a small number of classified images of the target task, in addition to using data augmentation techniques to balance classes and to increase the number of samples. According to the obtained results, it has been experimentally proven that the proposed approach has improved the performance of all models, where without data augmentation, the performance of the VGG16 model, Xception model, ResNet50 model, and MobileNetV2 model are improved by 0.28%, 10.96%, 15.73%, and 10.4%, respectively, while, with data augmentation, the VGG16 model, Xception model, ResNet50 model, and MobileNetV2 model are improved by 19.66%, 34.76%, 31.76%, and 33.03%, respectively. The Xception model obtained the highest performance compared to the rest of the models when classifying skin cancer images in the ISIC2020 dataset, as it obtained 96.83%, 96.919%, 96.826%, 96.825%, 99.07%, and 94.58% for accuracy, precision, recall, F1-score, sensitivity, and specificity respectively. To classify the images of the ICIAR 2018 dataset for breast cancer, the Xception model obtained 99%, 99.003%, 98.995%, 99%, 98.55%, and 99.14% for accuracy, precision, recall, F1-score, sensitivity, and specificity, respectively. Through these results, the proposed approach improved the models’ performance when fine-tuning was performed on unclassified images of the same disease. © 2023 by the authors.","10.3390/s23020570","breast cancer; data augmentation; fine-tuning; imbalanced datasets; medical images; skin cancer; transfer learning","44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146529166&doi=10.3390%2fs23020570&partnerID=40&md5=7f6ea0a895c85867b3e717037e71eaaa"
"HI-MViT: A lightweight model for explainable skin disease classification based on modified MobileViT","Ding Y.; Yi Z.; Li M.; long J.; Lei S.; Guo Y.; Fan P.; Zuo C.; Wang Y.","2023","1","1","0","0","0","Unique","0","","","","","","","Objective: To develop an explainable lightweight skin disease high-precision classification model that can be deployed to the mobile terminal. Methods: In this study, we present HI-MViT, a lightweight network for explainable skin disease classification based on Modified MobileViT. HI-MViT is mainly composed of ordinary convolution, Improved-MV2, MobileViT block, global pooling, and fully connected layers. Improved-MV2 uses the combination of shortcut and depth classifiable convolution to substantially decrease the amount of computation while ensuring the efficient implementation of information interaction and memory. The MobileViT block can efficiently encode local and global information. In addition, semantic feature dimensionality reduction visualization and class activation mapping visualization methods are used for HI-MViT to further understand the attention area of the model when learning skin lesion images. Results: The International Skin Imaging Collaboration has assembled and made available the ISIC series dataset. Experiments using the HI-MViT model on the ISIC-2018 dataset achieved scores of 0.931, 0.932, 0.961, and 0.977 on F1-Score, Accuracy, Average Precision (AP), and area under the curve (AUC). Compared with the top five algorithms of ISIC-2018 Task 3, Marco's average F1-Score, AP, and AUC have increased by 6.9%, 6.8%, and 0.8% compared with the suboptimal performance model. Compared with ConvNeXt, the most competitive convolutional neural network architecture, our model is 5.0%, 3.4%, 2.3%, and 2.2% higher in F1-Score, Accuracy, AP, and AUC, respectively. The experiments on the ISIC-2017 dataset also achieved excellent results, and all indicators were better than the top five algorithms of ISIC-2017 Task 3. Using the trained model to test on the PH2 dataset, an excellent performance score is obtained, which shows that it has good generalization performance. Conclusions: The skin disease classification model HI-MViT proposed in this article shows excellent classification performance and generalization performance in experiments. It demonstrates how the classification outcomes can be applied to dermatologists’ computer-assisted diagnostics, enabling medical professionals to classify various dermoscopic images more rapidly and reliably. © The Author(s) 2023.","10.1177/20552076231207197","deep learning; Dermatology classification; explainable artificial intelligence; HI-MViT; lightweight model","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175049078&doi=10.1177%2f20552076231207197&partnerID=40&md5=863f465930f78af0e4a8237cbcf5ba85"
"Deep Learning based Model for Detection of Vitiligo Skin Disease using Pretrained Inception V3","Sharma S.; Guleria K.; Kumar S.; Tiwari S.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Skin diseases are commonly identified problems all over the world. There are various kinds of skin diseases, such as skin cancer, vulgaris, ichthyosis, and eczema. Vitiligo is one of the skin diseases that can occur in any area of the body, including the inner part of the mouth. This type of skin can have immense negative impacts on the human body, involving memory issues, hypertension, and mental health problems. Conventionally, dermatologists use biopsy, blood tests, and patch testing to identify the presence of skin diseases and provide medications to patients. However, these treatments don't always provide results due to the transformation of a macule into a patch. Various machine learning (ML) and deep learning (DL) models have been developed for the early identification of macules to avoid delays in treatments. This work has implemented a DL-based model for predicting and classifying vitiligo skin disease in healthy skin. The features from the images have been extracted using a pre-trained Inception V3 model and substituted for each classifier, namely, naive Bayes, convolutional neural network (CNN), random forest, and decision tree. The results have been determined as accuracy, recall, precision, area under the curve (AUC), and F1-score for Inception V3 with naive Bayes as 99.5%, 0.995, 0.995, 0.997, and 0.995, respectively. The Inception V3 with CNN has achieved 99.8% accuracy, 0.998 recall, 0.998 precision, 1.00 AUC, and 0.998 F1-score. Further, Inception V3 with random forest shows 99.9% accuracy, 0.999 recall, 0.999 precision, 1.00 AUC, and 0.999 F1-score values whereas, Inception V3 with decision tree classifier shows an accuracy value of 97.8%, 0.978 recall, 0.977 precision, 0.969 AUC, and 0.977 F1-score. Results exhibit that Inception V3 with a random forest classifier outperforms in terms of accuracy, recall, precision, and F1-score, whereas for the AUC metric, Inception V3 with a random forest and Inception V3 with CNN have shown the same outcomes of 1.00. © 2023 International Journal of Mathematical, Engineering and Management Sciences. All rights reserved.","10.33889/IJMEMS.2023.8.5.059","Convolutional neural networks; Decision tree; InceptionV3; Naive Bayes; Random forest; Vitiligo","62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171449904&doi=10.33889%2fIJMEMS.2023.8.5.059&partnerID=40&md5=691133bcdbee5391496d4bfe73799ad6"
"Explainable Artificial Intelligence and Deep Transfer Learning for Skin Disease Diagnosis","Mayanja J.; Asanda E.H.; Mwesigwa J.; Tumwebaze P.; Marvin G.","2023","1","1","0","0","0","First occurrence","0","","","","","","","Accurate diagnosis of skin diseases, including skin cancers, is challenging and often leads to misdiagnosis and incorrect treatments. Whereas misdiagnoses have far-reaching consequences for both the patients and the healthcare system, resulting in delayed interventions, disease progression, and compromised outcomes, diagnostic precision is extremely crucial for improved prognosis and disease management. Unfortunately, the complexity of skin diseases with their diverse symptoms and subjective interpretation adds to the diagnostic challenges faced by dermatologists. It complicates accurate identification of essential features to prevent unnecessary interventions and ensure appropriate treatment, saving healthcare resources. To address misdiagnosis, we apply intelligent approaches to dermatology particularly integration of artificial intelligence (AI) and machine learning (ML) using comprehensive datasets to enhance diagnostic accuracy. The AI models are trained on diverse skin disease data to provide reliable diagnostic support using explainable AI (XAI) techniques tailored for skin disease diagnosis. We provide transparency and interpretability to clinicians to enable them understand AI-driven diagnostics, promote trust and collaboration with AI diagnostic tools. This facilitates informed decision-making and improves the overall diagnostic process where accurate diagnosis is crucial given the prevalence of skin diseases. Therefore, this work presents advancements in ML and XAI to mitigate errors and improve dermatology diagnostic outcomes. This collaboration between dermatologists and technology experts demonstrates the need and potential to enhance accuracy, efficiency, and effectiveness in skin disease diagnosis, leading to better healthcare delivery and patient well-being. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","10.1007/978-981-99-7093-3_47","Artificial intelligence (AI); Convolutional neural networks (CNN); Deep transfer learning (DTL); Explainable artificial intelligence (XAI); Skin disease diagnosis","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177856340&doi=10.1007%2f978-981-99-7093-3_47&partnerID=40&md5=bd1927211536b06b8ffb4e2daf560d26"
"Improved Face Morphing Attack Detection Method Using PCA and Convolutional Neural Network","Razaq I.S.; Shukur B.K.","2023","0","1","0","0","0","Unique","0","","","","","","","Face recognition is the most extensively utilized security and public safety verification method. In many nations, the Automatic Border Control system uses face recognition to confirm the identification of travelers The ABC system is vulnerable to face morphing attacks; the face recognition systems give acceptance for the traveller, even though the passport photo does not represent the actual image of the person but is a result of the merger of two images. Therefore, it is vital to determine whether the passport image is altering (morph) or actual. This research proposes an improved method to extract features from facial images. The proposed method consists of four phases: In the first stage, morph images were generated using a set of databases of images of real people, used every two images that were similar in general shape or landmarks in producing the morphed image using three types of techniques used in this field (Automatic selection landmark, StyleGAN, and Manual selection landmark). StyleGAN has been relied upon to achieve the best results in producing artefact-free images. In the second phase, a Faster Region Convolution neural network is utilizing for determining and cutting important landmarks area (eyes, nose, mouth, and skin) in the face, where we leave the hair, ears, and image background for every image in the database. In the third phase, the features are extracted using three techniques Principal component analysis, eigenvalue, and eigenvector; a matrix of two-dimensional features is generated with one layer for each technique. Then merge the extracted features (with out s) from each image into one image with three layers. The first layer represents the principal component analysis features, the second the eigenvalue features, and the third the eigenvector features. Finally, the features are introduced into the convolutional neural networks to obtain optimal features. The fourth phase represents the classification process using the Deep Neural Network (DNN) classifier and Support Vector Machine (SVM) second classifier. The DNN classifier achieved an average accuracy of 99.02% compared with SVM, with an accuracy of 98.64%. The power of the proposed work is evident through the FRA and RFF evaluation. Which achieved values as low as possible for DNN FAR 0.018, indicating the error rate in calculating morphed images is actual, and FRR 0.003, meaning the error rate in calculating the actual images is morphed, FAR 0.023, FRR 0.06 for SVM whenever these ratios are less than one, the higher system's accuracy in detection. The AMSL dataset (Accuracy 95.8%, FAR 0.039, FRR 0%) (Accuracy 95.2%, FAR 0.047, FRR 0.98) for DNN and SVM, respectively. It turned out that the training of the proposed network optimized for the features extracted for the landmarks area significantly affects finding the difference and discovering the modified images, even in the case of minor modifications as in the AMSL dataset. © 2023 University of Kerbala.","10.33640/2405-609X.3298","Convolution neural network; Deep learning; Face morphing attacks; Faster region convolution neural network; Principle component analysis; Support vector machine","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165091337&doi=10.33640%2f2405-609X.3298&partnerID=40&md5=de67494db056364cc3c4754738619e91"
"Ensemble feature selection and tabular data augmentation with generative adversarial networks to enhance cutaneous melanoma identification and interpretability","Gómez-Martínez V.; Chushig-Muzo D.; Veierød M.B.; Granja C.; Soguero-Ruiz C.","2024","0","1","0","0","0","First occurrence","0","","","","","","","Background: Cutaneous melanoma is the most aggressive form of skin cancer, responsible for most skin cancer-related deaths. Recent advances in artificial intelligence, jointly with the availability of public dermoscopy image datasets, have allowed to assist dermatologists in melanoma identification. While image feature extraction holds potential for melanoma detection, it often leads to high-dimensional data. Furthermore, most image datasets present the class imbalance problem, where a few classes have numerous samples, whereas others are under-represented. Methods: In this paper, we propose to combine ensemble feature selection (FS) methods and data augmentation with the conditional tabular generative adversarial networks (CTGAN) to enhance melanoma identification in imbalanced datasets. We employed dermoscopy images from two public datasets, PH2 and Derm7pt, which contain melanoma and not-melanoma lesions. To capture intrinsic information from skin lesions, we conduct two feature extraction (FE) approaches, including handcrafted and embedding features. For the former, color, geometric and first-, second-, and higher-order texture features were extracted, whereas for the latter, embeddings were obtained using ResNet-based models. To alleviate the high-dimensionality in the FE, ensemble FS with filter methods were used and evaluated. For data augmentation, we conducted a progressive analysis of the imbalance ratio (IR), related to the amount of synthetic samples created, and evaluated the impact on the predictive results. To gain interpretability on predictive models, we used SHAP, bootstrap resampling statistical tests and UMAP visualizations. Results: The combination of ensemble FS, CTGAN, and linear models achieved the best predictive results, achieving AUCROC values of 87% (with support vector machine and IR=0.9) and 76% (with LASSO and IR=1.0) for the PH2 and Derm7pt, respectively. We also identified that melanoma lesions were mainly characterized by features related to color, while not-melanoma lesions were characterized by texture features. Conclusions: Our results demonstrate the effectiveness of ensemble FS and synthetic data in the development of models that accurately identify melanoma. This research advances skin lesion analysis, contributing to both melanoma detection and the interpretation of main features for its identification. © The Author(s) 2024.","10.1186/s13040-024-00397-7","Class imbalance; Ensemble feature selection; Interpretability methods; Melanoma classification; Skin lesion classification; Tabular generative adversarial networks","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85208471833&doi=10.1186%2fs13040-024-00397-7&partnerID=40&md5=41a96c650c85c42b48eecb9ea8a240b8"
"Explainable AI-Based Skin Cancer Detection Using CNN, Particle Swarm Optimization and Machine Learning","Shah S.A.H.; Shah S.T.H.; Khaled R.; Buccoliero A.; Shah S.B.H.; Di Terlizzi A.; Di Benedetto G.; Deriu M.A.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is among the most prevalent cancers globally, emphasizing the need for early detection and accurate diagnosis to improve outcomes. Traditional diagnostic methods, based on visual examination, are subjective, time-intensive, and require specialized expertise. Current artificial intelligence (AI) approaches for skin cancer detection face challenges such as computational inefficiency, lack of interpretability, and reliance on standalone CNN architectures. To address these limitations, this study proposes a comprehensive pipeline combining transfer learning, feature selection, and machine-learning algorithms to improve detection accuracy. Multiple pretrained CNN models were evaluated, with Xception emerging as the optimal choice for its balance of computational efficiency and performance. An ablation study further validated the effectiveness of freezing task-specific layers within the Xception architecture. Feature dimensionality was optimized using Particle Swarm Optimization, reducing dimensions from 1024 to 508, significantly enhancing computational efficiency. Machine-learning classifiers, including Subspace KNN and Medium Gaussian SVM, further improved classification accuracy. Evaluated on the ISIC 2018 and HAM10000 datasets, the proposed pipeline achieved impressive accuracies of 98.5% and 86.1%, respectively. Moreover, Explainable-AI (XAI) techniques, such as Grad-CAM, LIME, and Occlusion Sensitivity, enhanced interpretability. This approach provides a robust, efficient, and interpretable solution for automated skin cancer diagnosis in clinical applications. © 2024 by the authors.","10.3390/jimaging10120332","ablation; explainable-AI; feature extraction; feature selection; medium Gaussian SVM; particle swarm optimization; subspace KNN; transfer learning","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213436359&doi=10.3390%2fjimaging10120332&partnerID=40&md5=d11c67f785beb221449ed0c71f9b659f"
"DermAI: An Innovative AI-Driven Chatbot for Enhanced Dermatological Diagnosis and Patient Interaction","Rajeshkumar P.; Kharche S.P.; Poojari P.; Utekar S.; Saini S.; Bdwai S.","2024","0","1","0","0","0","First occurrence","0","","","","","","","Skin disorders constitute a noteworthy public health concern globally, with earnest impacts on both physical and mental well-being. However, effective dermatological care faces challenges in resource-limited regions due to poor in-frastructure, limited access to medical facilities & expertise, and inadequate advanced diagnostic tools. The existing research work majorly focuses on cancer and uncommon skin diseases with models trying to achieve a higher training accuracy with no regards to misclassification rate. The products currently available in the market provide a limited initial diagnosis and suggest consulting a doctor to get an accurate diagnosis or offer a list of other possible skin disorders. To address these challenges, we propose DermAI, an innovative AI-based Chat-bot made entirely of open-source technologies, which integrates the ResNet-50 model and LLM via Chainlit, with Retrieval Augmented Generation(RAG), utilising AstraDB vector database and OpenAI embedding model for person-alised responses. enabling accurate classification of common skin diseases. The proposed DermAI ensures minimal misclassification and comprehensive cover-age of diseases, leveraging Retrieval-Augmented Generation and comparative model analysis. The metrics indicate that the model has a high true positive rate, with a misclassification rate of 2.17%, mean sensitivity, specificity & AUC of 92.6%, 99.8% & 99.9% respectively. This is demonstrated in the situations of melanoma, chickenpox, shingles, impetigo, and nail fungus, where it obtained 100% validation accuracy, a feat not attained by previous studies. Additionally, the model is highly capable of correctly identifying negative cases. The hallucination metric suggests the model may have a minimal tendency to hallucinate as the average hallucination score of 7% which falls far within the manually set threshold value of 50%. By setting the threshold value to 50%, the model gener-ates grounded answers that are pertaining to the knowledge base and also allows it to be flexible with its responses. Overall, DermAI outperforms all solutions proposed in research literature. © 2024 Institute of Advanced Engineering and Science. All rights reserved.","10.52549/ijeei.v12i4.5806","Chatbot; Deep Learning (DL); Dermatology; Large Language Model (LLM); Natural Language Processing (NLP); Retrieval Augmented Generation (RAG); Telemedicine platform; Transfer Learning (TL)","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215605796&doi=10.52549%2fijeei.v12i4.5806&partnerID=40&md5=9cf66ec6cf9f2f5284c491f26ed71d8a"
"Skin Cancer Diagnosis With Multi-Level Classification","El-Badawy R.; Tawfik B.S.; Zeidan M.A.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin cancer arises from the uncontrolled proliferation of abnormal skin cells, primarily triggered by exposure to the harmful ultraviolet (UV) rays of the sun and the utilization of UV tanning beds. This condition poses a heightened risk due to its potential to progress into blood cancer and lead to rapid fatality. Extensive research efforts have been dedicated to advance the treatment of this perilous disease. This paper presents a system designed for the examination and diagnosis of pigmented skin lesions and melanoma. The system incorporates a supervised classification algorithm that combines Convolutional Neural Network (CNN) and Deep Neural Network (DNN) architectures with feature extraction techniques. It operates in two distinct stages: the initial stage classifies images into two categories, namely benign or malignant, while the subsequent stage further categorizes the images into one of three classes: basal cell carcinomas, squamous cell carcinomas, or melanoma. Consequently, the comprehensive system addresses four classes, namely benign, basal cell carcinomas, squamous cell carcinomas, and melanoma. This work contributes to the system’s design in three significant ways. Firstly, it implements multiple iterations to select the most optimal images, resulting in the highest classification accuracy. Secondly, it employs various statistical methods to identify the most pertinent features, thereby enhancing the classifier’s accuracy by focusing on the most informative features for the classification task. Lastly, a two-stage classification approach is implemented, employing two distinct classifiers at different levels within the overall system. Despite the inherent complexity of the real-world problem, the overall system attains a commendable level of classification accuracy. Following rigorous experimentation, the study identifies the top three models. Each approach culminates in a classifier for each stage. The first approach, utilizing a deep learning classifier, achieves an accuracy of 86.08% in the initial cancer discrimination stage and 79.12% in the subsequent stage. The second approach, employing a machine learning classifier, attains an accuracy of 74.63% in the first stage and 64.41% in the second stage. The third approach, utilizing a linear regression classifier, achieves an accuracy of 98% in the first stage and 90% in the second stage. These results underscore the significance of feature selection in influencing model accuracy and suggest the potential for further optimization. Copyright © 2024 International Academic Press","10.19139/soic-2310-5070-2090","deep learning; dermoscopy images; feature extraction; melanoma; neural network; skin cancer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207256248&doi=10.19139%2fsoic-2310-5070-2090&partnerID=40&md5=6670c5936d9157eeadf92dc270fa2c25"
"Conditional adversarial segmentation and deep learning approach for skin lesion sub-typing from dermoscopic images","Mirunalini P.; Desingu K.; Aswatha S.; Deepika R.; Deepika V.; Jaisakthi S.M.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Automatic skin lesion subtyping is a crucial step for diagnosing and treating skin cancer and acts as a first level diagnostic aid for medical experts. Although, in general, deep learning is very effective in image processing tasks, there are notable areas of the processing pipeline in the dermoscopic image regime that can benefit from refinement. Our work identifies two such areas for improvement. First, most benchmark dermoscopic datasets for skin cancers and lesions are highly imbalanced due to the relative rarity and commonality in the occurrence of specific lesion types. Deep learning methods tend to exhibit biased performance in favor of the majority classes with such datasets, leading to poor generalization. Second, dermoscopic images can be associated with irrelevant information in the form of skin color, hair, veins, etc.; hence, limiting the information available to a neural network by retaining only relevant portions of an input image has been successful in prompting the network towards learning task-relevant features and thereby improving its performance. Hence, this research work augments the skin lesion characterization pipeline in the following ways. First, it balances the dataset to overcome sample size biases. Two balancing methods, synthetic minority oversampling TEchnique (SMOTE) and Reweighting, are applied, compared, and analyzed. Second, a lesion segmentation stage is introduced before classification, in addition to a preprocessing stage, to retain only the region of interest. A baseline segmentation approach based on Bi-Directional ConvLSTM U-Net is improved using conditional adversarial training for enhanced segmentation performance. Finally, the classification stage is implemented using EfficientNets, where the B2 variant is used to benchmark and choose between the balancing and segmentation techniques, and the architecture is then scaled through to B7 to analyze the performance boost in lesion classification. From these experiments, we find that the pipeline that balances using SMOTE and uses the adversarially trained segmentation network achieves the best baseline performance of 91% classification accuracy with EfficientNet B2. Based on the scaling experiments, we find that optimal performance is reached with the B6 architecture that classifies with a 97% accuracy. Furthermore, the proposed pipeline for lesion characterization outperforms the state of the art performance on the ISIC dataset. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.","10.1007/s00521-024-09964-9","Adversarial learning; Adversarial segmentation; BCDU-Net; Deep learning; Dermoscopic images; EfficientNet; Reweighting; Skin lesion; SMOTE","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194534145&doi=10.1007%2fs00521-024-09964-9&partnerID=40&md5=1c9396e010e7c786aa0689f08ac8bc71"
"Skin-CAD: Explainable deep learning classification of skin cancer from dermoscopic images by feature selection of dual high-level CNNs features and transfer learning","Attallah O.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin cancer (SC) significantly impacts many individuals' health all over the globe. Hence, it is imperative to promptly identify and diagnose such conditions at their earliest stages using dermoscopic imaging. Computer-aided diagnosis (CAD) methods relying on deep learning techniques especially convolutional neural networks (CNN) can effectively address this issue with outstanding outcomes. Nevertheless, such black box methodologies lead to a deficiency in confidence as dermatologists are incapable of comprehending and verifying the predictions that were made by these models. This article presents an advanced an explainable artificial intelligence (XAI) based CAD system named “Skin-CAD” which is utilized for the classification of dermoscopic photographs of SC. The system accurately categorises the photographs into two categories: benign or malignant, and further classifies them into seven subclasses of SC. Skin-CAD employs four CNNs of different topologies and deep layers. It gathers features out of a pair of deep layers of every CNN, particularly the final pooling and fully connected layers, rather than merely depending on attributes from a single deep layer. Skin-CAD applies the principal component analysis (PCA) dimensionality reduction approach to minimise the dimensions of pooling layer features. This also reduces the complexity of the training procedure compared to using deep features from a CNN that has a substantial size. Furthermore, it combines the reduced pooling features with the fully connected features of each CNN. Additionally, Skin-CAD integrates the dual-layer features of the four CNNs instead of entirely depending on the features of a single CNN architecture. In the end, it utilizes a feature selection step to determine the most important deep attributes. This helps to decrease the general size of the feature set and streamline the classification process. Predictions are analysed in more depth using the local interpretable model-agnostic explanations (LIME) approach. This method is used to create visual interpretations that align with an already existing viewpoint and adhere to recommended standards for general clarifications. Two benchmark datasets are employed to validate the efficiency of Skin-CAD which are the Skin Cancer: Malignant vs. Benign and HAM10000 datasets. The maximum accuracy achieved using Skin-CAD is 97.2 % and 96.5 % for the Skin Cancer: Malignant vs. Benign and HAM10000 datasets respectively. The findings of Skin-CAD demonstrate its potential to assist professional dermatologists in detecting and classifying SC precisely and quickly. © 2024 Elsevier Ltd","10.1016/j.compbiomed.2024.108798","Convolutional neural networks; Deep learning; Dermoscopic imaging; Feature fusion; Feature selection; Principal component analysis; Skin cancer diagnosis","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196761918&doi=10.1016%2fj.compbiomed.2024.108798&partnerID=40&md5=8f49214070a7f7f30634b5f209a7893e"
"Heuristic Segmentation Assisted Deep-Spatial Feature Learning Model for Leprosy Detection","Jitendra R.; Simha J.B.; Abhi S.; Chadha V.K.","2024","0","1","0","0","0","First occurrence","0","","","","","","","The high pace of rising leprosy cases and resulting socio-stigma has alarmed academia-industries to achieve non-invasive, touchless, and vision-based computer aided diagnosis (CAD) solution for leprosy detection and classification. Unlike classical CAD systems, leprosy detection and classification has remained a least explored research area. Invasive and chemical assay-based leprosy detection approaches are often found time-consuming, complex, and limited to cope-up with real-world’s demand. Though, in the last few years every effort has been made towards vision-based leprosy detection and classification; however, limited data, inferior feature, high-annotation demands, and importantly low accuracy limit their suitability. Considering these facts as motivation, in this paper a highly robust Heuristic Driven Segmentation assisted hybrid deep-spatio-textural feature learning for leprosy detection and classification is developed. It emphasizes on both region of interest segmentation and its optimization, as well as feature improvement to achieve higher accuracy and reliability. Here, firefly heuristic driven Fuzzy C-Means clustering (FFCM) was developed to perform ROI specific segmentation, which was followed by a hybrid deep spatio-textural feature extraction process. Noticeably, FFCM at one hand enabled automatic and accurate ROI-segmentation. On the contrary, the use of hybrid features including descriptive spatio-textural textural statistics (i.e., Gray-level co-occurrence metrics) and 4096-dimensional AlexNet features provided an intrinsic feature rich vector to perform accurate leprosy classification. The ROI-specific hybrid features (i.e., GLCM and AlexNet features) were processed for two-class classification using ensemble random forest classifier that labelled each input skin lesion as the normal image or the leprosy detected. The proposed leprosy detection method exhibited superior performance (accuracy = 96.6%, precision = 99.7%, recall = 95.8%, F-score = 0.9771) over other state-of-art methods like GLCM (accuracy = 88.51%) and Convolutional Neural Network based methods (91%). It affirms its suitability of the proposed CAD model towards automated and touchless real-time leprosy diagnosis. © The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd. 2024.","10.1007/s42979-024-03053-3","AlexNet; Ensemble classification; GLCM; Heuristic-based lesion segmentation; Hybrid spatio-textural deep features; Leprosy detection and classification","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85200024676&doi=10.1007%2fs42979-024-03053-3&partnerID=40&md5=decc6273c2294a8ea23afc4fcb3899c3"
"LAMA: Lesion-Aware Mixup Augmentation for Skin Lesion Segmentation","Lama N.; Stanley R.J.; Lama B.; Maurya A.; Nambisan A.; Hagerty J.; Phan T.; Van Stoecker W.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Deep learning can exceed dermatologists’ diagnostic accuracy in experimental image environments. However, inaccurate segmentation of images with multiple skin lesions can be seen with current methods. Thus, information present in multiple-lesion images, available to specialists, is not retrievable by machine learning. While skin lesion images generally capture a single lesion, there may be cases in which a patient’s skin variation may be identified as skin lesions, leading to multiple false positive segmentations in a single image. Conversely, image segmentation methods may find only one region and may not capture multiple lesions in an image. To remedy these problems, we propose a novel and effective data augmentation technique for skin lesion segmentation in dermoscopic images with multiple lesions. The lesion-aware mixup augmentation (LAMA) method generates a synthetic multi-lesion image by mixing two or more lesion images from the training set. We used the publicly available International Skin Imaging Collaboration (ISIC) 2017 Challenge skin lesion segmentation dataset to train the deep neural network with the proposed LAMA method. As none of the previous skin lesion datasets (including ISIC 2017) has considered multiple lesions per image, we created a new multi-lesion (MuLe) segmentation dataset utilizing publicly available ISIC 2020 skin lesion images with multiple lesions per image. MuLe was used as a test set to evaluate the effectiveness of the proposed method. Our test results show that the proposed method improved the Jaccard score 8.3% from 0.687 to 0.744 and the Dice score 5% from 0.7923 to 0.8321 over a baseline model on MuLe test images. On the single-lesion ISIC 2017 test images, LAMA improved the baseline model’s segmentation performance by 0.08%, raising the Jaccard score from 0.7947 to 0.8013 and the Dice score 0.6% from 0.8714 to 0.8766. The experimental results showed that LAMA improved the segmentation accuracy on both single-lesion and multi-lesion dermoscopic images. The proposed LAMA technique warrants further study. © The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine 2024.","10.1007/s10278-024-01000-5","Data augmentation; Deep learning; Dermoscopy; Image segmentation; Melanoma","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007819268&doi=10.1007%2fs10278-024-01000-5&partnerID=40&md5=a4bd29d5a81751368cdb032a1e017caa"
"LitefusionNet: Boosting the performance for medical image classification with an intelligent and lightweight feature fusion network","Asif S.; Ain Q.-U.; Al-Sabri R.; Abdullah M.","2024","0","1","0","0","0","Unique","0","","","","","","","Medical image analysis plays a crucial role in modern healthcare for accurate diagnosis and treatment. However, the inherent challenges and limitations posed by the complexity and variability of medical images, coupled with the shortcomings of existing methods, necessitate the development of novel approaches. In this study, we propose LiteFusionNet (Lightweight Fusion Network), a lightweight model that effectively addresses these challenges, offering the advantage of accurate and efficient medical image classification while mitigating the computational demands. The LitefusionNet leverages the power of deep convolutional neural networks (DCNNs) and feature fusion techniques to achieve improved performance in medical image classification. LitefusionNet combines the strengths of MobileNet and MobileNetV2 architectures to extract robust features from medical images. These features capture discriminative information from different levels of abstraction, enhancing the model's ability to capture fine-grained patterns. The fusion process employs a concatenation method to combine the extracted features, resulting in a more comprehensive representation that improves the model's classification accuracy. To evaluate the effectiveness of LitefusionNet, extensive experiments are conducted on a diverse set of publicly available medical image datasets, including brain MRI, skin, CT, X-ray, and histology. The results demonstrate that LitefusionNet outperforms several existing models in terms of classification accuracy, showcasing its efficacy in different medical imaging modalities. Furthermore, we provide interpretability to the model's predictions by performing Grad-CAM analysis, enabling insights into the important regions in the medical images that contribute to the classification decision. In addition, we compare LitefusionNet with five pre-trained models. LiteFusionNet excels in medical image classification, boasting impressive accuracies across diverse datasets: 97.33% for brain MRI, 91.11% for skin, 99.00% for CT, 98.15% for X-ray, and 92.11% for histology. These results underscore LiteFusionNet's robust and versatile performance, making it a compelling solution for accurate and efficient medical image analysis. Overall, LitefusionNet strikes a balance between accuracy, efficiency, and real-time performance. Our findings demonstrate its potential as a promising solution for accurate and efficient medical image analysis, with applications in diagnostic support systems and clinical decision-making. © 2024 Elsevier B.V.","10.1016/j.jocs.2024.102324","Convolutional neural network; Feature fusion; Medical image classification; Medical imaging; Transfer learning","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194330985&doi=10.1016%2fj.jocs.2024.102324&partnerID=40&md5=3db707980fe9f60608fd2d43144857d2"
"Residual cosine similar attention and bidirectional convolution in dual-branch network for skin lesion image classification","Li A.; Zhang D.; Yu L.; Kang X.; Tian S.; Wu W.; You H.; Huo X.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is one of the most serious threats to human health among skin lesions. Computer-aided diagnosis methods can assist patients in identifying and detecting skin lesion types early, thereby enabling corresponding treatments. In this paper, we propose a dual-branch neural network model Conformer with Residual Cosine Similarity Attention and Bidirectional Convolutional fusion strategy, named RCSABC-Conformer. The core of this network structure comprises three parts: a Convolutional Neural Network (CNN) branch with Residual Cosine Similarity Attention (RCSA), a Transformer branch, and a Feature Couple Unit with Bidirectional Convolutional strategy (BC-FCU). The RCSA module calculates the cosine similarity value between the feature map generated by the convolutional operation and the feature map of the residual edge to assess whether their semantic information is similar. The semantic information of similar parts is weighted by exponential normalization to enhance the network's memory of similar features of the same type of skin lesion. The BC-FCU module interactively fuses local features and global representations of skin lesion images with different resolutions in the two branches. Specifically, when the global representations is integrated into local features, we introduce a new bidirectional convolution strategy to extract the feature map from both forward and backward directions, and then select the element with the smaller feature value from the two directions to fuse into local features. In this way, we can minimize the interference of the artifact features extracted by the Transformer branch on the CNN branch. In addition, taking advantage of the Transformer branch's capacity to construct global representations, our model can learn contextual semantic information of normal skin and lesion areas to enhance model robustness. We conducted experiments on three datasets, consisting of clinical and dermoscopic skin lesion images, as well as a hybrid of both. The experimental results show that RCSABC-Conformer outperforms both advanced and classical classification methods in terms of classification accuracy across all three datasets, without requiring an increase in the number of parameters and computational complexity. Compared with the baseline model, the classification accuracy of our proposed method improves by 2.40%, 5.39%, and 4.44% on the three datasets, respectively. To the best of our knowledge, this is the first study to apply an interactive fusion dual-branch network for multi-disease classification on different modalities of skin lesion databases. Code will be available at https://github.com/AlenLi817/RCSABC-Conformer. © 2024 Elsevier Ltd","10.1016/j.engappai.2024.108386","Attention mechanism; Cosine similarity; Multi-label classification; Skin lesion recognition","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189460073&doi=10.1016%2fj.engappai.2024.108386&partnerID=40&md5=5f079e76a54955916edde58c3a0fca80"
"ESDMR-Net: A lightweight network with expand-squeeze and dual multiscale residual connections for medical image segmentation","Khan T.M.; Naqvi S.S.; Meijering E.","2024","0","1","0","0","0","First occurrence","0","","","","","","","Segmentation is an important task in a wide range of computer vision applications, including medical image analysis. Recent years have seen an increase in the complexity of medical image segmentation approaches based on sophisticated convolutional neural network architectures. This progress has led to incremental enhancements in performance on widely recognised benchmark datasets. However, most of the existing approaches are computationally demanding, which limits their practical applicability. This paper presents an expand-squeeze dual multiscale residual network (ESDMR-Net), which is a full y convolutional network that is particularly well-suited for resource-constrained computing hardware such as mobile devices. ESDMR-Net focusses on extracting multiscale features, enabling the learning of contextual dependencies among semantically distinct features. The ESDMR-Net architecture allows dual-stream information flow within encoder–decoder pairs. The expansion operation (depthwise separable convolution) makes all of the rich features with multiscale information available to the squeeze operation (bottleneck layer), which then extracts the necessary information for the segmentation task. The Expand-Squeeze (ES) block helps the network pay more attention to under-represented classes, which contributes to improved segmentation accuracy. To enhance the flow of information across multiple resolutions or scales, we integrated dual multiscale residual (DMR) blocks into the skip connection. This integration enables the decoder to access features from various levels of abstraction, ultimately resulting in more comprehensive feature representations. We present experiments on seven datasets from five distinct examples of applications: segmentation of retinal vessels (2×), skin lesions (2×), digestive tract polyps, lung regions, and cells. Our model demonstrates strong performance, with an F1 score of 0.8287%, 0.8211%, 0.9034%, 0.9451%, 0.9543%, 0.9840%, and 0.8424% on the DRIVE, CHASE, ISIC2017, ISIC2016, CVC-ClinicDB, MC and MoNuSeg datasets, respectively. Remarkably, our model achieves these results despite having significantly fewer trainable parameters, with a reduction of two or even three orders of magnitude. © 2024 The Author(s)","10.1016/j.engappai.2024.107995","Deep neural networks; Lightweight networks; Medical image segmentation; Resource-constrained networks","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183940456&doi=10.1016%2fj.engappai.2024.107995&partnerID=40&md5=5e12d2622f0b463e4b1b8c34d7e79d01"
"Enhancing Skin Disease Diagnosis Through Deep Learning: A Comprehensive Study on Dermoscopic Image Preprocessing and Classification","Haner Kırğıl E.; Erdaş Ç.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer occurs when abnormal cells in the top layer of the skin, known as the epidermis, undergo uncontrolled growth due to unrepaired DNA damage, leading to the development of mutations. These mutations lead to rapid cell growth and development of cancerous tumors. The type of cancerous tumor depends on the cells of origin. Overexposure to ultraviolet rays from the sun, tanning beds, or sunlamps is a primary factor in the occurrence of skin cancer. Since skin cancer is one of the most common types of cancer and has a high mortality, early diagnosis is extremely important. The dermatology literature has many studies of computer-aided diagnosis for early and highly accurate skin cancer detection. In this study, the classification of skin cancer was provided by Regnet x006, EfficientNetv2 B0, and InceptionResnetv2 deep learning methods. To increase the classification performance, hairs and black pixels in the corners due to the nature of dermoscopic images, which could create noise for deep learning, were eliminated in the preprocessing step. Preprocessing was done by hair removal, cropping, segmentation, and applying a median filter to dermoscopic images. To measure the performance of the proposed preprocessing technique, the results were obtained with both raw images and preprocessed images. The model developed to provide a solution to the classification problem is based on deep learning architectures. In the four experiments carried out within the scope of the study, classification was made for the eight classes in the dataset, squamous cell carcinoma and basal cell carcinoma classification, benign keratosis and actinic keratosis classification, and finally benign and malignant disease classification. According to the results obtained, the best accuracy values of the experiments were obtained as 0.858, 0.929, 0.917, and 0.906, respectively. The study underscores the significance of early and accurate diagnosis in addressing skin cancer, a prevalent and potentially fatal condition. The primary aim of the preprocessing procedures was to attain enhanced performance results by concentrating solely on the area spanning the lesion instead of analyzing the complete image. Combining the suggested preprocessing strategy with deep learning techniques shows potential for enhancing skin cancer diagnosis, particularly in terms of sensitivity and specificity. © 2024 The Author(s). International Journal of Imaging Systems and Technology published by Wiley Periodicals LLC.","10.1002/ima.23148","classification; deep learning; ISIC 2019; melanoma; skin cancer","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85198710048&doi=10.1002%2fima.23148&partnerID=40&md5=d6db431d568d52323483eee9165ce93f"
"A novel end-to-end deep convolutional neural network based skin lesion classification framework","A R.S.; Chamola V.; Hussain Z.; Albalwy F.; Hussain A.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Background: Skin diseases are reported to contribute 1.79% of the global burden of disease. The accurate diagnosis of specific skin diseases is known to be a challenging task due, in part, to variations in skin tone, texture, body hair, etc. Classification of skin lesions using machine learning is a demanding task, due to the varying shapes, sizes, colors, and vague boundaries of some lesions. The use of deep learning for the classification of skin lesion images has been shown to help diagnose the disease at its early stages. Recent studies have demonstrated that these models perform well in skin detection tasks, with high accuracy and efficiency. Objective: Our paper proposes an end-to-end framework for skin lesion classification, and our contributions are two-fold. Firstly, two fundamentally different algorithms are proposed for segmenting and extracting features from images during image preprocessing. Secondly, we present a deep convolutional neural network model, S-MobileNet that aims to classify 7 different types of skin lesions. Methods: We used the HAM10000 dataset, which consists of 10000 dermatoscopic images from different populations and is publicly available through the International Skin Imaging Collaboration (ISIC) Archive. The image data was preprocessed to make it suitable for modeling. Exploratory data analysis (EDA) was performed to understand various attributes and their relationships within the dataset. A modified version of a Gaussian filtering algorithm and SFTA was applied for image segmentation and feature extraction. The processed dataset was then fed into the S-MobileNet model. This model was designed to be lightweight and was analyzed in three dimensions: using the Relu Activation function, the Mish activation function, and applying compression at intermediary layers. In addition, an alternative approach for compressing layers in the S-MobileNet architecture was applied to ensure a lightweight model that does not compromise on performance. Results: The model was trained using several experiments and assessed using various performance measures, including, loss, accuracy, precision, and the F1-score. Our results demonstrate an improvement in model performance when applying a preprocessing technique. The Mish activation function was shown to outperform Relu. Further, the classification accuracy of the compressed S-MobileNet was shown to outperform S-MobileNet. Conclusions: To conclude, our findings have shown that our proposed deep learning-based S-MobileNet model is the optimal approach for classifying skin lesion images in the HAM10000 dataset. In the future, our approach could be adapted and applied to other datasets, and validated to develop a skin lesion framework that can be utilized in real-time. © 2023 The Author(s)","10.1016/j.eswa.2023.123056","Classification; Convolution neural network; Deep learning; Image segmentation; MobileNet; Skin lesion","44","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182269929&doi=10.1016%2fj.eswa.2023.123056&partnerID=40&md5=ee1d2ffc8bf930f27eca51dfe5355efd"
"Explainable deep inherent learning for multi-classes skin lesion classification","Hosny K.M.; Said W.; Elmezain M.; Kassem M.A.","2024","1","1","0","0","0","Unique","0","","","","","","","There is often a lack of explanation when artificial intelligence (AI) is used to diagnose skin lesions, which makes the physician unable to interpret and validate the output; thus, diagnostic systems become significantly less safe. In this paper, we proposed a deep inherent learning method to classify seven types of skin lesions. The proposed deep inherent learning was validated using different explanation techniques. Explainable AI (X-AI) was used to explain decision-making processes at the local and global levels. In addition, we provide visual information to help physicians trust the proposed method. The challenging dataset, HAM10000, was used to evaluate the proposed method. Medical practitioners can better understand the mechanisms of black-box AI models using our simple, stage-based X-AI framework. They can trust the proposed method because the rationale for its decisions is explained. © 2024 Elsevier B.V.","10.1016/j.asoc.2024.111624","Explainable AI; Image classification; Inherent deep learning; Occlusion sensitivity; Skin lesions","60","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190987808&doi=10.1016%2fj.asoc.2024.111624&partnerID=40&md5=4964ae7ed019795b0942a5e7f1424ddd"
"A deep learning outline aimed at prompt skin cancer detection utilizing gated recurrent unit networks and improved orca predation algorithm","Zhang L.; Zhang J.; Gao W.; Bai F.; Li N.; Ghadimi N.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is a risky ailment that can be effectively managed if detected promptly. However, timely recognition of skin cancer remains a challenge. In this study, a robotic computer-assisted tactic is proposed for the early detection of skin cancer. The motivation behind this research is to improve the accuracy and efficiency of skin cancer recognition. The proposed methodology involves a series of steps. Initially, the input images undergo preprocessing to enhance their quality and extract relevant features. These preprocessed images are then fed into a Gated Recurrent Unit (GRU) Network, a type of deep learning model known for its ability to capture sequential information. To optimize the performance of the GRU Network, we employ an enhanced variant of the Orca Predation Algorithm (OPA). This algorithm helps fine-tune the network parameters, improving its diagnostic capabilities. To validate and evaluate the effectiveness of our skin cancer diagnosis algorithm, we conducted experiments using the HAM10000 dataset, which contains a large collection of skin lesion images. We compared the results obtained from our proposed method, named GRU/IOPA, with eight existing techniques commonly used for skin cancer diagnosis. Five execution indices, namely sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), and accuracy, were used to assess the performance of these methods. Our findings demonstrate that the GRU/IOPA system outperforms other existing methods in terms of sensitivity (0.95), specificity (0.97), PPV (0.95), NPV (0.96), and accuracy. These results indicate the effectiveness of our proposed method in diagnosing skin cancer compared to traditional approaches. The superior performance of GRU/IOPA highlights its potential impact on improving skin cancer diagnosis and reinforces its promise as an advanced tool in the field. In conclusion, our study presents a novel approach for timely skin cancer recognition through a robotic computer-assisted tactic. By utilizing the GRU/IOPA system, we achieve superior accuracy and efficiency in diagnosing skin cancer compared to existing techniques. This research offers significant contributions to the field of skin cancer diagnosis and opens up new avenues for future advancements in this area. © 2023 Elsevier Ltd","10.1016/j.bspc.2023.105858","Automatic diagnosis; Deep learning; Gated recurrent unit network; Improved orca predation algorithm; Medical image analysis; Skin cancer","115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180532581&doi=10.1016%2fj.bspc.2023.105858&partnerID=40&md5=ce0a7945c6745f275e321f55b44d34c4"
"SNC_Net: Skin Cancer Detection by Integrating Handcrafted and Deep Learning-Based Features Using Dermoscopy Images","Naeem A.; Anees T.; Khalil M.; Zahra K.; Naqvi R.A.; Lee S.-W.","2024","1","1","0","0","0","First occurrence","0","","","","","","","The medical sciences are facing a major problem with the auto-detection of disease due to the fast growth in population density. Intelligent systems assist medical professionals in early disease detection and also help to provide consistent treatment that reduces the mortality rate. Skin cancer is considered to be the deadliest and most severe kind of cancer. Medical professionals utilize dermoscopy images to make a manual diagnosis of skin cancer. This method is labor-intensive and time-consuming and demands a considerable level of expertise. Automated detection methods are necessary for the early detection of skin cancer. The occurrence of hair and air bubbles in dermoscopic images affects the diagnosis of skin cancer. This research aims to classify eight different types of skin cancer, namely actinic keratosis (AKs), dermatofibroma (DFa), melanoma (MELa), basal cell carcinoma (BCCa), squamous cell carcinoma (SCCa), melanocytic nevus (MNi), vascular lesion (VASn), and benign keratosis (BKs). In this study, we propose SNC_Net, which integrates features derived from dermoscopic images through deep learning (DL) models and handcrafted (HC) feature extraction methods with the aim of improving the performance of the classifier. A convolutional neural network (CNN) is employed for classification. Dermoscopy images from the publicly accessible ISIC 2019 dataset for skin cancer detection is utilized to train and validate the model. The performance of the proposed model is compared with four baseline models, namely EfficientNetB0 (B1), MobileNetV2 (B2), DenseNet-121 (B3), and ResNet-101 (B4), and six state-of-the-art (SOTA) classifiers. With an accuracy of 97.81%, a precision of 98.31%, a recall of 97.89%, and an F1 score of 98.10%, the proposed model outperformed the SOTA classifiers as well as the four baseline models. Moreover, an Ablation study is also performed on the proposed method to validate its performance. The proposed method therefore assists dermatologists and other medical professionals in early skin cancer detection. © 2024 by the authors.","10.3390/math12071030","computer-aided diagnosis (CAD); convolutional neural networks (CNNs); deep learning; diagnostic imaging; machine learning; medical image processing; skin cancer","42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190266900&doi=10.3390%2fmath12071030&partnerID=40&md5=9f8305d3e54de94b3f4f24d047ddef50"
"Naturalize Revolution: Unprecedented AI-Driven Precision in Skin Cancer Classification Using Deep Learning","Abou Ali M.; Dornaika F.; Arganda-Carreras I.; Ali H.; Karaouni M.","2024","1","1","0","0","0","Unique","0","","","","","","","Background: In response to the escalating global concerns surrounding skin cancer, this study aims to address the imperative for precise and efficient diagnostic methodologies. Focusing on the intricate task of eight-class skin cancer classification, the research delves into the limitations of conventional diagnostic approaches, often hindered by subjectivity and resource constraints. The transformative potential of Artificial Intelligence (AI) in revolutionizing diagnostic paradigms is underscored, emphasizing significant improvements in accuracy and accessibility. Methods: Utilizing cutting-edge deep learning models on the ISIC2019 dataset, a comprehensive analysis is conducted, employing a diverse array of pre-trained ImageNet architectures and Vision Transformer models. To counteract the inherent class imbalance in skin cancer datasets, a pioneering “Naturalize” augmentation technique is introduced. This technique leads to the creation of two indispensable datasets—the Naturalized 2.4K ISIC2019 and groundbreaking Naturalized 7.2K ISIC2019 datasets—catalyzing advancements in classification accuracy. The “Naturalize” augmentation technique involves the segmentation of skin cancer images using the Segment Anything Model (SAM) and the systematic addition of segmented cancer images to a background image to generate new composite images. Results: The research showcases the pivotal role of AI in mitigating the risks of misdiagnosis and under-diagnosis in skin cancer. The proficiency of AI in analyzing vast datasets and discerning subtle patterns significantly augments the diagnostic prowess of dermatologists. Quantitative measures such as confusion matrices, classification reports, and visual analyses using Score-CAM across diverse dataset variations are meticulously evaluated. The culmination of these endeavors resulted in an unprecedented achievement—100% average accuracy, precision, recall, and F1-score—within the groundbreaking Naturalized 7.2K ISIC2019 dataset. Conclusion: This groundbreaking exploration highlights the transformative capabilities of AI-driven methodologies in reshaping the landscape of skin cancer diagnosis and patient care. The research represents a pivotal stride towards redefining dermatological diagnosis, showcasing the remarkable impact of AI-powered solutions in surmounting the challenges inherent in skin cancer diagnosis. The attainment of 100% across crucial metrics within the Naturalized 7.2K ISIC2019 dataset serves as a testament to the transformative capabilities of AI-driven approaches in reshaping the trajectory of skin cancer diagnosis and patient care. This pioneering work paves the way for a new era in dermatological diagnostics, heralding the dawn of unprecedented precision and efficacy in the identification and classification of skin cancers. © 2024 by the authors.","10.3390/biomedinformatics4010035","convolutional neural net (CNN); deep learning (DP); ImageNet models; machine learning (ML); naturalize; Segment Anything Model (SAM); skin cancer; transfer learning (TL); vision transformer (ViT)","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188795063&doi=10.3390%2fbiomedinformatics4010035&partnerID=40&md5=45582a34535ad115f994df3aa16e3d9e"
"A user-friendly AI-based clinical decision support system for rapid detection of pandemic diseases: Covid-19 and Monkeypox","Adar T.; Delice E.K.; Delice O.","2024","0","1","0","0","0","First occurrence","0","","","","","","","Accurate and rapid diagnosis is a significant factor in reducing incidence rate; especially when the number of people inflicted with a disease is considerably high. In the healthcare sector, the decision-making process might be a complex and error-prone one due to excessive workload, negligence, time restrictions, incorrect or incomplete evaluation of medical reports and analyses, and lack of experience as well as insufficient knowledge and skills. Clinical decision support systems (CDSSs) are those developed to improve effectiveness of decisions by supporting physicians’ decision-making process regarding their patients. In this study, a new artificial intelligence-based CDSS and a user-friendly interface for this system were developed to ensure rapid and accurate detection of pandemic diseases. The proposed CDSS, which is called panCdss, uses hybrid models consisting of the Convolutional Neural Network (CNN) model and Machine Learning (ML) methods in order to detect covid-19 from lung computed tomography (CT) images. Transfer Learning (TL) models were used to detect monkeypox from skin lesion images and covid-19 from chest X-Ray images. The results obtained from these models were evaluated according to accuracy, precision, recall and F1-score performance metrics. Of these models, the ones with the highest classification performance were used in the panCdss. The highest classification values obtained for each dataset were as follows: %91.71 accuracy, %92.07 precision, %90.29 recall and %91.71 F1-score for covid-19 CT dataset by using CNN+RF hybrid model; %99.56 accuracy, %100 precision, %99.12 recall and %99.55 F1-score for covid-19 X-ray dataset by using VGG16 model; and %90.38 accuracy, %93.32 precision, %88.11 recall and %90.64 F1-score for monkeypox dataset by using MobileNetV2. It is believed that panCdss can be successfully employed for rapid and accurate classification of pandemic diseases and can help reduce physicians’ workload. Furthermore, the study showed that the proposed CDSS is an adaptable, flexible and dynamic system that can be practiced not only for the detection of pandemic diseases but also for other diseases. To the authors’ knowledge, this proposed CDSS is the first CDSS developed for pandemic disease detection. © 2024 – IOS Press. All rights reserved.","10.3233/JIFS-232477","artificial intelligence; Clinical decision support system; deep learning; pandemic diseases; user interface","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185540147&doi=10.3233%2fJIFS-232477&partnerID=40&md5=86b7f249fe2f44f4a47199ee44e6fdb8"
"Grid-Based Structural and Dimensional Skin Cancer Classification with Self-Featured Optimized Explainable Deep Convolutional Neural Networks","Behara K.; Bhero E.; Agee J.T.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is a severe and potentially lethal disease, and early detection is critical for successful treatment. Traditional procedures for diagnosing skin cancer are expensive, time-intensive, and necessitate the expertise of a medical practitioner. In recent years, many researchers have developed artificial intelligence (AI) tools, including shallow and deep machine learning-based approaches, to diagnose skin cancer. However, AI-based skin cancer diagnosis faces challenges in complexity, low reproducibility, and explainability. To address these problems, we propose a novel Grid-Based Structural and Dimensional Explainable Deep Convolutional Neural Network for accurate and interpretable skin cancer classification. This model employs adaptive thresholding for extracting the region of interest (ROI), using its dynamic capabilities to enhance the accuracy of identifying cancerous regions. The VGG-16 architecture extracts the hierarchical characteristics of skin lesion images, leveraging its recognized capabilities for deep feature extraction. Our proposed model leverages a grid structure to capture spatial relationships within lesions, while the dimensional features extract relevant information from various image channels. An Adaptive Intelligent Coney Optimization (AICO) algorithm is employed for self-feature selected optimization and fine-tuning the hyperparameters, which dynamically adapts the model architecture to optimize feature extraction and classification. The model was trained and tested using the ISIC dataset of 10,015 dermascope images and the MNIST dataset of 2357 images of malignant and benign oncological diseases. The experimental results demonstrated that the model achieved accuracy and CSI values of 0.96 and 0.97 for TP 80 using the ISIC dataset, which is 17.70% and 16.49% more than lightweight CNN, 20.83% and 19.59% more than DenseNet, 18.75% and 17.53% more than CNN, 6.25% and 6.18% more than Efficient Net-B0, 5.21% and 5.15% over ECNN, 2.08% and 2.06% over COA-CAN, and 5.21% and 5.15% more than ARO-ECNN. Additionally, the AICO self-feature selected ECNN model exhibited minimal FPR and FNR of 0.03 and 0.02, respectively. The model attained a loss of 0.09 for ISIC and 0.18 for the MNIST dataset, indicating that the model proposed in this research outperforms existing techniques. The proposed model improves accuracy, interpretability, and robustness for skin cancer classification, ultimately aiding clinicians in early diagnosis and treatment. © 2024 by the authors.","10.3390/ijms25031546","adaptive intelligent coney optimization algorithm; explainable convolutional neural network; grid-based structural pattern; skin cancer; VGG-16","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184682031&doi=10.3390%2fijms25031546&partnerID=40&md5=03e8c5916a30222d69373f9abb9f1cfd"
"Transformer guided self-adaptive network for multi-scale skin lesion image segmentation","Xin C.; Liu Z.; Ma Y.; Wang D.; Zhang J.; Li L.; Zhou Q.; Xu S.; Zhang Y.","2024","1","1","0","0","0","Unique","0","","","","","","","Background: In recent years, skin lesion has become a major public health concern, and the diagnosis and management of skin lesions depend heavily on the correct segmentation of the lesions. Traditional convolutional neural networks (CNNs) have demonstrated promising results in skin lesion segmentation, but they are limited in their ability to capture distant connections and intricate features. In addition, current medical image segmentation algorithms rarely consider the distribution of different categories in different regions of the image and do not consider the spatial relationship between pixels. Objectives: This study proposes a self-adaptive position-aware skin lesion segmentation model SapFormer to capture global context and fine-grained detail, better capture spatial relationships, and adapt to different positional characteristics. The SapFormer is a multi-scale dynamic position-aware structure designed to provide a more flexible representation of the relationships between skin lesion characteristics and lesion distribution. Additionally, it increases skin lesion segmentation accuracy and decreases incorrect segmentation of non-lesion areas. Innovations: SapFormer designs multiple hybrid transformers for multi-scale feature encoding of skin images and multi-scale positional feature sensing of the encoded features using a transformer decoder to obtain fine-grained features of the lesion area and optimize the regional feature distribution. The self-adaptive feature framework, built upon the transformer decoder module, dynamically and automatically generates parameterizations with learnable properties at different positions. These parameterizations are derived from the multi-scale encoding characteristics of the input image. Simultaneously, this paper utilizes the cross-attention network to optimize the features of the current region according to the features of other regions, aiming to increase skin lesion segmentation accuracy. Main results: The ISIC-2016, ISIC-2017, and ISIC-2018 datasets for skin lesions are used as the basis for the experiment. On these datasets, the proposed model has accuracy values of 97.9 %, 94.3 %, and 95.7 %, respectively. The proposed model's IOU values are, in order, 93.2 %, 86.4 %, and 89.4 %. The proposed model's DSC values are 96.4 %, 92.6 %, and 94.3 %, respectively. All three metrics surpass the performance of the majority of state-of-the-art (SOTA) models. SapFormer's metrics on these datasets demonstrate that it can precisely segment skin lesions. Notably, our approach exhibits remarkable noise resistance in non-lesion areas, while simultaneously conducting finer-grained regional feature extraction on the skin lesion image. Conclusions: In conclusion, the integration of a transformer-guided position-aware network into semantic skin lesion segmentation results in a notable performance boost. The ability of our proposed network to capture spatial relationships and fine-grained details proves beneficial for effective skin lesion segmentation. By enhancing lesion localization, feature extraction, quantitative analysis, and classification accuracy, the proposed segmentation model improves the diagnostic efficiency of skin lesion analysis on dermoscopic images. It assists dermatologists in making more accurate and efficient diagnoses, ultimately leading to better patient care and outcomes. This research paves the way for advances in diagnosing and treating skin lesions, promoting better understanding and decision-making in the clinical setting. © 2023 The Authors","10.1016/j.compbiomed.2023.107846","Segmentation; Self-adaptive feature extraction; Skin lesion; Vision transformer","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181766722&doi=10.1016%2fj.compbiomed.2023.107846&partnerID=40&md5=eb696c0faba7ca5483e36261e259b63b"
"An aseptic approach towards skin lesion localization and grading using deep learning and harris hawks optimization","Balaha H.M.; Hassan A.E.-S.; El-Gendy E.M.; ZainEldin H.; Saafan M.M.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is the most common form of cancer. It is predicted that the total number of cases of cancer will double in the next fifty years. It is an expensive procedure to discover skin cancer types in the early stages. Additionally, the survival rate reduces as cancer progresses. The current study proposes an aseptic approach toward skin lesion detection, classification, and segmentation using deep learning and Harris Hawks Optimization Algorithm (HHO). The current study utilizes the manual and automatic segmentation approaches. The manual segmentation is used when the dataset has no masks to use while the automatic segmentation approach is used, using U-Net models, to build an adaptive segmentation model. Additionally, the meta-heuristic HHO optimizer is utilized to achieve the optimization of the hyperparameters of 5 pre-trained CNN models, namely VGG16, VGG19, DenseNet169, DenseNet201, and MobileNet. Two datasets are used, namely ""Melanoma Skin Cancer Dataset of 10000 Images"" and ""Skin Cancer ISIC"" dataset from two publicly available sources for variety purpose. For the segmentation, the best-reported scores are 0.15908, 91.95%, 0.08864, 0.04313, 0.02072, 0.20767 in terms of loss, accuracy, Mean Absolute Error, Mean Squared Error, Mean Squared Logarithmic Error, and Root Mean Squared Error, respectively. For the ""Melanoma Skin Cancer Dataset of 10000 Images"" dataset, from the applied experiments, the best reported scores are 97.08%, 98.50%, 95.38%, 98.65%, 96.92% in terms of overall accuracy, precision, sensitivity, specificity, and F1-score, respectively by the DenseNet169 pre-trained model. For the ""Skin Cancer ISIC"" dataset, the best reported scores are 96.06%, 83.05%, 81.05%, 97.93%, 82.03% in terms of overall accuracy, precision, sensitivity, specificity, and F1-score, respectively by the MobileNet pre-trained model. After computing the results, the suggested approach is compared with 9 related studies. The results of comparison proves the efficiency of the proposed framework. © The Author(s) 2023.","10.1007/s11042-023-16201-3","Convolution neural network (CNN); Deep learning (DL); Harris hawks optimization (HHO); Melanoma skin cancer; Meta-heuristic optimization; Skin lesion; Transfer learning","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166027039&doi=10.1007%2fs11042-023-16201-3&partnerID=40&md5=06855992cd6076f09bfb7e63354a7f19"
"Enhance a System for Predicting Skin Lesion Using Hybrid Feature Selection Technique","Singh N.; Kumar S.; Vasudevan S.K.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Visual assessments during medical examinations of skin lesions may be a tough procedure due to the considerable similarity between the lesions. In light of the increasing prevalence of skin cancer and limited clinical competence, it is imperative to develop artificial intelligence (AI)-powered tools for early-stage diagnosis of skin cancer. Given the availability of extensive skin lesion datasets in scientific literature, AI-powered deep learning (DL) models have shown effective in distinguishing between cancerous and benign skin lesions utilizing dermoscopic pictures. Early identification of skin cancer may lead to a lower mortality rate. Dermoscopy is a very efficient method for identifying and categorizing skin cancer. The research used mathematical-based hybrid methodologies to discover essential characteristics. The methodologies used for our investigation include Chi-square, information gain, and principal component analysis. The findings of this investigation are very comprehensive and meticulous. The validation approach included examining the variations in parameter settings across many machine learning algorithms. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024.","10.1007/978-981-97-4149-6_36","Classification; Deep learning algorithm; Feature selection; Healthcare; Skin lesion","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206071301&doi=10.1007%2f978-981-97-4149-6_36&partnerID=40&md5=3bba92462b443127b414d4e18128b181"
"Explainable Skin Lesion Classification with Multitask Learning","Liu C.-H.; Huang S.-L.","2024","1","1","0","0","0","Unique","0","","","","","","","Deep learning models have been proposed to identify skin lesions based on the skin surface images. However, one unresolved issue is their lack of interpretability, which necessitates the development of skin lesion classification models capable of explaining the diagnostic features. Cell nuclei information plays a crucial role in skin lesion classification because it provides valuable insights into the underlying cellular changes associated with various skin diseases, aiding in accurate diagnosis and appropriate management of patients. This paper aims to identify quantitative features with cross-sectional cellular-resolution images to facilitate an interpretable model. We develop a melanin localization model that can be utilized on skin lesions. This model combines skin layers and cell nuclei segmentation models to derive a set of quantitative features. A multitask learning strategy is applied to enhance the segmentation accuracy and benefit from the shared information of these features. Subsequently, a tree-based machine learning model is employed to develop an interpretable classification model using these features. Using eczema as an example, optical coherence tomography that captures cellular structures, including nuclei in vivo, could potentially enhance understanding of pathogenesis and treatment response without requiring invasive biopsies. © 2024 Copyright for this paper by its authors.","","Multitask learning; optical coherence tomography; skin lesions; tree-based interpretable classification model","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85210858400&partnerID=40&md5=d5a0bcecba4b05f070806b90124b1075"
"Explainable Multi-task Learning Approach for Skin Lesion Classification","Patel K.; Mehta N.; Easwaran S.; Walambe R.; Gite S.; Pradhan B.; Saini B.","2024","1","1","0","0","0","First occurrence","0","","","","","","","The early diagnosis of skin cancer has significantly improved with the use of computer-aided techniques and deep learning (DL) models. However, existing methods often struggle with issues of interpretability and adaptability, which are crucial for clinical application. To address these limitations, we employed a Multi-Task Learning (MTL) approach that simultaneously performs classification and segmentation of skin lesions. This approach not only improves the accuracy and robustness of the models but also enhances their interpretability by incorporating Explainable AI (XAI) techniques into the MTL framework. Our convolutional-deconvolutional based MTL model, tested on the HAM-10000 dataset, demonstrated enhanced classification accuracy and interpretability with an Accuracy of 91.56% and IoU Score of 87.98%. The model outperformed state-of-the-art models, showing a marked enhancement in classification accuracy. Importantly, the use of MTL facilitates a reduction in model complexity while achieving superior performance, making this approach both powerful and efficient. From a practical standpoint, the replicability of our MTL framework is a key advantage, providing a scalable model for researchers and clinicians. The methodology’s adaptability to different imaging datasets underscores its potential utility across various dermatological conditions. Future research could leverage this framework to further refine diagnostic accuracy in other complex imaging tasks, enhancing the scope of AI in medical diagnostics. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","10.1007/978-3-031-68602-3_14","Class activation heatmaps; Deep learning; Explainable AI; Healthcare; Multi-task learning; Skin lesion classification","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212517354&doi=10.1007%2f978-3-031-68602-3_14&partnerID=40&md5=1879379e31660e461958112453ef6d4a"
"AI in dermatology: a comprehensive review into skin cancer detection","Behara K.; Bhero E.; Agee J.T.","2024","1","1","0","0","0","Unique","0","","","","","","","Background: Artificial Intelligence (AI) is significantly transforming dermatology, particularly in early skin cancer detection and diagnosis. This technological advancement addresses a crucial public health issue by enhancing diagnostic accuracy, efficiency, and accessibility. AI integration in medical imaging and diagnostic procedures offers promising solutions to the limitations of traditional methods, which often rely on subjective clinical evaluations and histopathological analyses. This study systematically reviews current AI applications in skin cancer classification, providing a comprehensive overview of their advantages, challenges, methodologies, and functionalities. Methodology: In this study, we conducted a comprehensive analysis of artificial intelligence (AI) applications in the classification of skin cancer. We evaluated publications from three prominent journal databases: Scopus, IEEE, and MDPI. We conducted a thorough selection process using the PRISMA guidelines, collecting 1,156 scientific articles. Our methodology included evaluating the titles and abstracts and thoroughly examining the full text to determine their relevance and quality. Consequently, we included a total of 95 publications in the final study. We analyzed and categorized the articles based on four key dimensions: advantages, difficulties, methodologies, and functionalities. Results: AI-based models exhibit remarkable performance in skin cancer detection by leveraging advanced deep learning algorithms, image processing techniques, and feature extraction methods. The advantages of AI integration include significantly improved diagnostic accuracy, faster turnaround times, and increased accessibility to dermatological expertise, particularly benefiting underserved areas. However, several challenges remain, such as concerns over data privacy, complexities in integrating AI systems into existing workflows, and the need for large, high-quality datasets. AI-based methods for skin cancer detection, including CNNs, SVMs, and ensemble learning techniques, aim to improve lesion classification accuracy and increase early detection. AI systems enhance healthcare by enabling remote consultations, continuous patient monitoring, and supporting clinical decision-making, leading to more efficient care and better patient outcomes. Conclusions: This comprehensive review highlights the transformative potential of AI in dermatology, particularly in skin cancer detection and diagnosis. While AI technologies have significantly improved diagnostic accuracy, efficiency, and accessibility, several challenges remain. Future research should focus on ensuring data privacy, developing robust AI systems that can generalize across diverse populations, and creating large, high-quality datasets. Integrating AI tools into clinical workflows is critical to maximizing their utility and effectiveness. Continuous innovation and interdisciplinary collaboration will be essential for fully realizing the benefits of AI in skin cancer detection and diagnosis. 2024 Behara et al.","10.7717/peerj-cs.2530","Artificial intelligence; Classification; Clinical decisions; Deep learning; Dermatology; Image preprocessing; Machine learning; Prediction; Skin cancer","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85211080806&doi=10.7717%2fpeerj-cs.2530&partnerID=40&md5=688f7a920c539a5a1d10f253bb27081d"
"A Study on Skin Lesions Classification Based on Improved EfficientN etB0 Network","Zhao H.; Wu Y.; Lu Y.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is the most common type of cancer, and early detection and treatment of skin lesions can greatly improve the cure rate of skin cancer. However, existing skin disease assisted classification networks have some problems such as large number of parameters and complex operations, which are not convenient for deployment on mobile devices. This paper proposes a lightweight convolution neural network method for skin lesion classification. The method replaces the standard convolution module in the first stage of the network based on the lightweight EfficientN etBO with the Ghost module, and adds the CBAM module between the 1 *1 convolution layer and the pooling layer in the ninth stage, to form an improved EfficientNetBO network model. The accuracy of the improved EfficientN etBO on the ISIC2018 skin disease dataset HAMI0000 is 89.38%, the Macro_P, Macro_R, Fl-Score is 0.88, 0.86, 0.87 respectively. The number of parameters is only 5.19M, while the single inference time is 71ms. All of these indicators make the model suitable for deploying skin disease classification models on mobile devices to assist clinical diagnosis. © 2024 IEEE.","10.1109/NTCI64025.2024.10776178","CBAM attention mechanism; EfficientNetB0; Ghost module; skin lesions classifications","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215122177&doi=10.1109%2fNTCI64025.2024.10776178&partnerID=40&md5=2e1c8b49a27920a11acf920b117f3407"
"Skin cancer classification using vision transformers and explainable artificial intelligence","Dagnaw G.H.; El Mouhtadi M.; Mustapha M.","2024","1","1","0","0","0","Unique","0","","","","","","","Background: Skin cancer diagnosis is a critical aspect of dermatological healthcare, and requires accurate and efficient classification methods. Recently, vision transformers (ViTs) and convolutional neural networks (CNNs) have emerged as promising architectures. However, the interpretability of these models remains a concern, hindering their widespread adoption in the clinical setting. Therefore, the aim of this research is to propose an explainable skin cancer classification using deep learning and explainable artificial intelligence methods. Methods: This study presents skin cancer classification utilizing two pretrained ViTs, three Swin transformers, five pretrained CNNs, and three visual-based explainable artificial intelligence (XAI) models. The ViT-base, ViT-large, Swin-tiny, Swin-base, and Swin-small transformer models and VGG19, ResNet18, ResNet50, MobileNetV2, and DenseNet201 pretrained CNN models are used for classification. For explanation, gradient-weighted class activation mapping (Grad-CAM), Grad-CAM++, and score-weighted class activation mapping (Score-CAM) XAI models were adopted. The study used freely available datasets to train and test the proposed models and adopted synthetic minority oversampling technique (SMOTE) to address class imbalance issues. Results: The performances of the ViT and CNN models were evaluated using five performance metrics: accuracy, precision, F1 score, sensitivity, and specificity. The ViT models demonstrated competitive performance with ViT-large and ViT-base, achieving an accuracy of 88.6%. Swin-base exhibited a balanced sensitivity and specificity. Mainly, ResNet50 outperformed the tested models with an accuracy of 88.8%, precision of 86.9%, sensitivity of 88.6%, F1 score of 87.8%, and specificity of 88.9%. The integration of XAI techniques into the ResNet50 model showed that the model learns from relevant regions of the image to classify a given image as benign or malignant. Conclusions: This study presents ViT and CNN models for skin cancer classification, and the XAI techniques applied to the model contributes to enhancing transparency of the decision-making process of deep learning models. These findings will aid in accurate and trustworthy skin cancer classification and will be vital for clinical adoption in enhancing clinical decision-making in dermatological healthcare. © Journal of Medical Artificial Intelligence. All rights reserved.","10.21037/jmai-24-6","Deep learning; skin cancer classification; Swin transformers; vision transformers (VITs)","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85199005333&doi=10.21037%2fjmai-24-6&partnerID=40&md5=eaf87420b31ebfa65ced7b60abe7f625"
"A Deep Learning Approach to Automate Classification of Arsenic-Affected Skin using EfficientNet-B1","Saha A.; Das U.; Rahman M.S.","2024","1","1","0","0","0","Unique","0","","","","","","","Arsenic contamination in groundwater is a major health hazard, particularly in areas where it causes chronic arsenic poisoning, which is manifested in skin lesions and other dermatological conditions. Early and accurate detection of arsenic-affected skin is important for timely medical intervention. This paper proposes the automatic classification system using EfficientNet-B1, which is one of the most outstanding deep learning models for detecting arsenic-induced skin lesions using clinical images, in the model, training, and testing of a dataset comprising healthy and arsenic-affected skin, with augmentation techniques employed to increase robustness and generalizability from the image dataset. Several deep earning models were compared for this classification task: VGG-19, ResNet-50, MobileNetv2, EfficientNet-B0, and EfficientNetB1. EfficientNet-B1, after fine-tuning, achieved the highest performance of 94.69% accuracy and proved outstanding in this application. These results point out that EfficientNet-B1 is a reliable and scalable tool for automatic diagnosis and can thus assist healthcare providers in resource-constrained settings. Future work will include an extension of the dataset, improving model interpretability, and exploring clinical practical use of the system. © 2024 IEEE.","10.1109/ICCIT64611.2024.11022014","arsenic-affected skin; automated diagnosis; Deep learning; EffficientNet-B1; medical imaging","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009084918&doi=10.1109%2fICCIT64611.2024.11022014&partnerID=40&md5=df843dc7b2bc577ed1c0e00bfbbdd3a0"
"Alleviation of Health Data Poverty for Skin Lesions Using ACGAN: Systematic Review","Ravikumar A.; Sriraman H.; Chadha C.; Kumar Chattu V.","2024","1","1","0","0","0","First occurrence","0","","","","","","","Skin-based infections are one of the primary causes of the global disease burden. Digital Health Technologies powered by data science models have the potential to revolutionize global health care. Health data poverty refers to the failure of individual people, teams, or communities to profit in research or development owing to a deficiency of representative data. Generative Adversarial Network-based synthetic images can be viable solutions to health data poverty since timely detection and frequent monitoring are extremely critical for the survival of the patients. This study aims to investigate the possibility of obtaining photo - realistic dermatoscopic images of Skin Lesions via Generative Adversarial Networks (GAN), followed by distributing the images to augment the existing dataset to further enhance the performance of a Convolutional Neural Network for the task of classification. The medical and technological publications in six databases: PubMed, Web of Science, IEEE Xplore, Science Direct, Scopus, and Google Scholar were investigated. A Deep Learning pipeline has been created and a set of deep learning models such as VGG16 (Visual Geometry Group 16), DenseNet, Xception, and Inception-ResNet v2 have been assembled. We have used condition-based generative adversarial networks (GANs) besides the traditional data augmentation approaches such as rotation and scaling. To highlight the image features that eventually lead to classification are highlighted using a Local Interpretable Model-Agnostic Explanation (LIME) strategy. It was inferred from the results of the classification that DenseNet-201 with GAN Augmentation was the best individual model, with an accuracy of around 82%, while models such as VGG-16 and SVM (Support Vector Machine) were unable to compete. It was also observed that starting with the pre-trained ImageNet weights sped up the convergence and prevented models from over fitting in the absence of the regularization effect of augmented data. However, the exploitation of the data was still not perfectly optimal, as over fitting with data augmentation and early stopping was observed, which can be used by more extensive data augmentation techniques. The GAN augmentation showed to reduce the data imbalance and increase the data percentage of the less representative classes. A data augmentation approach based on synthetic data that has been obtained from GAN helps us to classify images of lesions of the skin with high accuracy. We can also infer from the results obtained that, enriching the data with GAN-produced data samples results in a significant performance increase. In the field of medical imaging, where particularly large training datasets are not available, novel data augmentation and generation procedures can be beneficial.  © 2013 IEEE.","10.1109/ACCESS.2024.3417176","Deep learning; digital health; GAN; health data poverty; machine learning; scalability","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196719891&doi=10.1109%2fACCESS.2024.3417176&partnerID=40&md5=1e99946b9242cb253e3a8e452af8c488"
"Exploring the influence of transformer-based multimodal modeling on clinicians’ diagnosis of skin diseases: A quantitative analysis","Zhang Y.; Hu Y.; Li K.; Pan X.; Mo X.; Zhang H.","2024","1","1","0","0","0","Unique","0","","","","","","","Objectives: The study aimed to propose a multimodal model that incorporates both macroscopic and microscopic images and analyze its influence on clinicians’ decision-making with different levels of experience. Methods: First, we constructed a multimodal dataset for five skin disorders. Next, we trained unimodal models on three different types of images and selected the best-performing models as the base learners. Then, we used a soft voting strategy to create the multimodal model. Finally, 12 clinicians were divided into three groups, with each group including one director dermatologist, one dermatologist-in-charge, one resident dermatologist, and one general practitioner. They were asked to diagnose the skin disorders in four unaided situations (macroscopic images only, dermatopathological images only, macroscopic and dermatopathological images, all images and metadata), and three aided situations (macroscopic images with model 1 aid, dermatopathological images with model 2&3 aid, all images with multimodal model 4 aid). The clinicians’ diagnosis accuracy and time for each diagnosis were recorded. Results: Among the trained models, the vision transformer (ViT) achieved the best performance, with accuracies of 0.8636, 0.9545, 0.9673, and AUCs of 0.9823, 0.9952, 0.9989 on the training set, respectively. However, on the external validation set, they only achieved accuracies of 0.70, 0.90, and 0.94, respectively. The multimodal model performed well compared to the unimodal models, achieving an accuracy of 0.98 on the external validation set. The results of logit regression analysis indicate that all models are helpful to clinicians in making diagnostic decisions [Odds Ratios (OR) > 1], while metadata does not provide assistance to clinicians (OR < 1). Linear analysis results indicate that metadata significantly increases clinicians’ diagnosis time (P < 0.05), while model assistance does not (P > 0.05). Conclusions: The results of this study suggest that the multimodal model effectively improves clinicians’ diagnostic performance without significantly increasing the diagnostic time. However, further large-scale prospective studies are necessary. © The Author(s) 2024.","10.1177/20552076241257087","computer-aided diagnosis; multimodality; quantitative research; Skin disease; soft voting","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193974709&doi=10.1177%2f20552076241257087&partnerID=40&md5=fcf52609787cf5be30bbedc60ccb2bc8"
"Empowering Future Engineers: Educational Journey from AI Fundamentals to Healthcare Innovations","Abdelmawla I.; Elghareeb A.; Gaffar A.; Khokhar A.","2024","0","1","0","0","0","Unique","0","","","","","","","This research-to-practice paper details implementing an educational approach designed to equip undergraduate students with the skills to develop healthcare applications and contribute to this domain using Medical AI by bridging the gap between the AI and the medical domain. This gap includes three challenges: medical data acquisition, strict privacy regulations in handling medical data, and a disconnect between the medical domain's requirements and AI's capabilities. To handle these challenges, We structured our educational approach into three phases: The first one is the educational AI stack which includes five learning stages, the second one is the medical foundation stack, which includes two working stages and the third one is system development, deployment, operation and testing. We implemented this approach within three Senior Design Projects focused on diagnosing skin cancer, breast cancer predication, and assessing allergy risks based on personal and biological data. The approach begins with a foundational AI stack, including Tensor-Flow, Scikit-Learn, and Keras, emphasizing transfer learning and model architecture selection customization. The second phase included the medical data acquisition process, NDA (Non-disclosure agreement) signing by the participants in these projects, and data handling, preprocessing, model selection, and training. The final phase included systems development, Deployment, and operation and testing. Students achieved significant results, including a 94% accuracy rate in skin cancer detection and over 85% precision in breast cancer prediction, including the tumor grade, stage, recurrence, and survival in the second project. And an 88% accuracy in allergy prediction using biological information, including Skin color and skin condition. They integrated the trained models with three developed web applications for the end users. Following the integration process, they deployed these applications into AWS and GCP. They compared the trained models between these two cloud platforms to determine their AI models' most effective deployment environment considering the model accuracy and cost. As a final step, the students tested the deployed applications within five stages, including the Go-live test, system performance, user satisfaction, model accuracy, and security status. Our approach aligns with ABET accreditation standards, focusing on the practical application of medical AI. © 2024 IEEE.","10.1109/FIE61694.2024.10893351","ABET Accreditation; AI Stack; Allergy Detection; AWS; Educational AI; GCP; Healthcare Innovations; HIPAA; Keras; Skin Cancer detection; System Development","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000789185&doi=10.1109%2fFIE61694.2024.10893351&partnerID=40&md5=32d54b0f8e86f8493f76aa32b5da5d95"
"A comparative study of deep learning algorithms for image-based classification of hyperpigmented skin disease","Ranuh I.G.B.R.S.; Sanjoto M.C.; Zakiyyah A.Y.; Meiliana","2024","1","1","0","0","0","Unique","0","","","","","","","There are growing numbers of significant skin disorders, including skin pigmentation. It states that skin color is determined by the amount of melanin produced by the body. The two main categories of skin pigmentation are hyperpigmentation, in which pigment seems to overflow, and hypopigmentation, in which pigment appears to decrease. However, many skin conditions share characteristics, making it difficult for dermatologists to correctly early diagnose their patients. Consequently, the accurate detection of skin disorders and the diagnosis of dermatoscopy pictures can be greatly aided by machine learning and deep learning approaches. The most effective deep learning technique for picture identification was investigated to diagnose hyperpigmented skin diseases. YOLO, DenseNet201, GoogLeNet, InceptionResNetV2, and MobileNet were among the pretrained models used to classify four most common hyperpigmented skin disorders. Using assessment metrics like accuracy and AUC (Area Under Curve), it was determined which deep learning method would work best for creating a clinical diagnostic system. The study analyzed the accuracy rates of five pretrained models, including GoogleNet, MobileNet, DenseNet201, InceptionResNetV2, and YOLO, after 50 iterations. Using metrics such as accuracy and Area Under the Curve (AUC), the study evaluated the models' performance on a small dataset split into 80% for training and 20% for testing. Training accuracy rates were 93.8%, 100%, 100%, 98.77%, and 97.43%, respectively, while test accuracy rates were 87.18%, 79.49%, 87.18%, 89.74%, and 97.56%. DenseNet201 showed strong performance, particularly for cafe-au-lait spots (CS), melasma (ML), and nevi (MN), but struggled with congenital nevus (CN). Despite DenseNet201's strong traditional CNN capabilities and generalization, YOLO emerged as the top model due to its stable accuracy and AUC values, as confirmed by confusion matrices. While these models show promise as diagnostic tools for dermatologists, further research, including expanding the dataset and exploring hybrid models, is needed to enhance their clinical accuracy and effectiveness. © 2024 The Authors.","10.1016/j.procs.2024.10.342","Area Under Curves; Confusion Matrix; Convolutional Neural Network; deep learning; diagnosis; machine learning; pigmentation; skin disorders","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213072018&doi=10.1016%2fj.procs.2024.10.342&partnerID=40&md5=e3a8fe8136926fb8ae7148736911e6fa"
"Explainable Raman Spectral Classification - Towards Clinical Practice of Cancer Diagnosis","Jenila C.; Gurram V.; Aarthi V.P.M.B.; Sangaraju B.; Siva Reddy G.C.; Soddala G.S.","2024","0","1","0","0","0","First occurrence","0","","","","","","","This study explores the potential of explainable Raman spectral classification for cancer diagnosis, with a focus on its transition to clinical practice. By leveraging Convolutional Neural Networks (CNNs) and explainable artificial intelligence (XAI) techniques, we classify Raman spectra obtained from normal skin fibroblasts (HF) and tumor-associated fibroblasts (ZAM). The CNN model, trained on preprocessed and frequency-transformed Raman data, achieves a high classification accuracy of 90.32%, demonstrating its effectiveness in distinguishing between cancerous and non-cancerous samples. Additionally, the use of SHAP (SHapley Additive Explanations) values enhances the model's interpretability by identifying key spectral features that influence its predictions, facilitating clinical understanding. Performance metrics for the model indicate a precision of 0.86, recall of 0.97, and F1-score of 0.91 for Class 0 (normal fibroblasts), and a precision of 0.96, recall of 0.83, and F1-score of 0.89 for Class 1 (tumor-associated fibroblasts). The SHAP values highlight critical features, such as Feature 3 ( 0.035), Feature 9 ( 0.025), and others, that play significant roles in the model's decision-making process. This research underscores the importance of interpretable and accurate spectral analysis tools in advancing Raman spectroscopy-based cancer diagnostics. The findings demonstrate the promise of deep learning-driven spectral classification, with significant implications for improving early cancer detection and diagnosis, marking a crucial step toward personalized cancer treatment in clinical settings. © 2024 IEEE.","10.1109/ICCES63552.2024.10860163","Cancer Diagnostics; Convolutional Neural Networks (CNN); Explainable Artificial Intelligence (XAI); Fibroblasts; Raman Spectroscopy","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218409801&doi=10.1109%2fICCES63552.2024.10860163&partnerID=40&md5=168d2f98ffa41093568431fdcb6e5a88"
"A New Multi-Layer Machine Learning (MLML) Architecture for Non-invasive Skin Cancer Diagnosis on Dermoscopic Images","Keskenler M.F.; Çelik E.; Dal D.","2024","1","1","0","0","0","Unique","0","","","","","","","Artificial intelligence (AI) has significantly impacted the healthcare industry, enabling the development of advanced medical devices and software that provide efficient and precise treatments. Health 4.0, the incorporation of computing and AI technologies into healthcare, is driving the industry's digital transformation and improving the diagnosis and treatment of diseases. AI can help detect diseases such as cancer at an early stage. AI can also lower the healthcare costs by reducing the need for unnecessary biopsies and speeding up the diagnostic process. Machine learning algorithms are commonly utilized in AI-powered healthcare studies and are also used in image-based research to diagnose a variety of diseases since the integration of AI into healthcare holds great potential to improve patient care and reduce costs. In this study, we present a multi-layer machine learning (MLML) method based on the joint use of machine learning algorithms to improve the success of skin cancer diagnosis. In this respect, the MLML method with 3 layers is proposed. In the first layer, decision tree, random forest, neural network, naive bayes, and support vector machine algorithms are used. After executing this layer, 5 different classification results are transferred to the second layer where k-nearest neighbor algorithm is utilized. In the last layer, the results are improved using the linear regression algorithm. Thanks to our method, images in the input dataset are classified into three groups: cancer, not cancer, and early-stage cancer. The multi-layer architecture is utilized to make joint decisions with different machine learning algorithms and remove the limitations of each algorithm so that more accurate decisions can be made. Fourteen feature extraction algorithms that were not previously used in skin cancer images are employed. Inclusion of age, gender, and region of the lesion in the decision-making process in addition to image features also contributes to obtaining better classification results. The performance of the proposed method was evaluated using four metrics. The conducted experiments showed that the MLML technique achieved 88.81% accuracy, 88.89% precision, 99.17% recall, and 93.75% F1-score in classifying skin cancer images. Finally, the results were compared with other relevant studies in the literature to demonstrate the superiority of the proposed method. © The Author(s) under exclusive licence to The Korean Institute of Electrical Engineers 2024.","10.1007/s42835-023-01758-8","Dermoscopic images; Diagnosis; Machine learning; MLML; Multi-layer architecture; Skin cancer","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182172540&doi=10.1007%2fs42835-023-01758-8&partnerID=40&md5=a6b455a4de80ca5919f3b4e18f24219d"
"A Skin Cancer Detection Framework Based on Double-Branch Attention Neural Networks","Wang Y.; Cheng H.; Wan C.; Zhang B.; Shi A.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is a major cancer and has increased rapidly in the past decades. Early detection can significantly increase the cure rate. Recently, deep learning models, especially various convolutional Neural Networks using dermatoscope images (i. e., dermoscopy) have been widely adopted to classify skin lesions. Different from traditional image classification, several challenges in detecting and classifying skin cancers still exist, including imbalanced training data in each skin cancer category, small visual differences between categories, and small area of skin lesion. To solve these challenges, this paper proposed a skin cancer classification framework based on double-branch attention convolutional neural networks (DACNN). First, in data pre-processing, the whole dataset was divided into finer-grained categories according to the natural subclasses in each category to alleviate the imbalanced data. Next, from the viewpoint of neural network structure, attention residual learning (ARL) modules were used as basic blocks in upper-branch, which was able to effectively extract the features of potential sick area, then the lesion location network (LLN) was designed to localize, cut out and zoom-in the sick sub-area, followed by being sent to down-branch with the same neural structure as the upper-branch, for extracting the locally detailed features. Then, the inferred features from both branches were integrated for effective detection and classification. Moreover, to further alleviate the impact of imbalanced categorical data, weighted loss function was utilized in the model training. The proposed DACNN model was implemented in the real dataset consisting of 10015 dermatoscope images and compared with several typical deep learning based skin lesion detection methods. Experimental results showed that the performance metrics of sensitivity, accuracy and Fl_score reached 0.922, 0. 942 and 0.933, respectively. Compared with recurrent attention convolutional neural network (RACNN) detection methods, these three metrics were improved by 3.48%, 2.95% and 3.44% respectively. In summary, our work significantly improved the accuracy of dermoscopy based skin cancer detection through appropriate division of dermatoscope image classes, used the double-branch attention neural networks to firstly localize and enlarge the features of potential sick area, and then further extracted the locally detailed features, which solved the intrinsic issues of dermatoscope images, including imbalanced samples in each skin cancer category, vague visual differences between categories, and small area of skin lesion. © 2024 Chinese Academy of Medical Sciences. All rights reserved.","10.3969/j.issn.0258-8021.2024.02.003","attention mechanism; double-branch neural network; skin cancer; unbalanced data","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85197304099&doi=10.3969%2fj.issn.0258-8021.2024.02.003&partnerID=40&md5=c6548411d8a1c0cf6799844c5cdc1754"
"Optimization of Laser-Based Method to Conduct Skin Ablation in Zebrafish and Development of Deep Learning-Based Method for Skin Wound-Size Measurement","Siregar P.; Liu Y.-S.; Casuga F.P.; Huang C.-Y.; Chen K.H.C.; Huang J.-C.; Hung C.-H.; Lin Y.-K.; Hsiao C.-D.; Lin H.-Y.","2024","1","1","0","0","0","Unique","0","","","","","","","Skin plays an important role as a defense mechanism against environmental pathogens in organisms such as humans or animals. Once the skin integrity is disturbed by a wound, pathogens can penetrate easily into a deeper part of the body to induce disease. By this means, it is important for the skin to regenerate quickly upon injury to regain its protective barrier function. Traditionally, scientists use rodents or mammals as experimental animals to study skin wound healing. However, due to concerns about animal welfare and increasing costs of laboratory animals, such as rodents, scientists have considered alternative methods of implementing replace, reduce, and refine (3Rs) in experimentation. Moreover, several previous studies on skin wound healing in fish used relatively expensive medical-grade lasers with a low calculation efficiency of the wound area, which led to human judgment errors. Thus, this study aimed to develop a new alternative model for skin wound healing by utilizing zebrafish together with a new rapid and efficient method as an alternative in investigating skin wound healing. First, in order to fulfill the 3Rs concept, the pain in the tested zebrafish was evaluated by using a 3D locomotion assay. Afterward, the obtained behavior data were analyzed using the Kruskal–Wallis test, followed by Dunn’s multiple comparisons tests; later, 3 watts was chosen as the power for the laser, since the wound caused by the laser at this power did not significantly alter zebrafish swimming behaviors. Furthermore, we also optimized the experimental conditions of zebrafish skin wound healing using a laser engraving machine, which can create skin wounds with a high reproducibility in size and depth. The wound closure of the tested zebrafish was then analyzed by using a two-way ANOVA, and presented in 25%, 50%, and 75% of wound-closure percentages. After imparting wounds to the skin of the zebrafish, wound images were collected and used for deep-learning training by convolutional neural networks (CNNs), either the Mask-RCNN or U-Net, so that the computer could calculate the area of the skin wounds in an automatic manner. Using ImageJ manual counting as a gold standard, we found that the U-Net performance was better than the Mask RCNN for zebrafish skin wound judgment. For proof-of-concept validation, a U-Net trained model was applied to study and determine the effect of different temperatures and the administration of antioxidants on the skin wound-healing kinetics. Results showed a significant positive correlation between the speed of wound closure and the exposure to different temperatures and administration of antioxidants. Taken together, the laser-based skin ablation and deep learning-based wound-size measurement methods reported in this study provide a faster, reliable, and reduced suffering protocol to conduct skin wound healing in zebrafish for the first time. © 2024 by the authors.","10.3390/inventions9020025","deep learning; ImageJ; laser ablation; medicine; skin; wound healing; zebrafish","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191562454&doi=10.3390%2finventions9020025&partnerID=40&md5=929e1a5a24fab38e77522352399dd678"
"Curriculum-Based Augmented Fourier Domain Adaptation for Robust Medical Image Segmentation","Wang A.; Islam M.; Xu M.; Ren H.","2024","0","1","0","0","0","First occurrence","0","","","","","","","Accurate and robust medical image segmentation is fundamental and crucial for enhancing the autonomy of computer-Aided diagnosis and intervention systems. Medical data collection normally involves different scanners, protocols, and populations, making domain adaptation (DA) a highly demanding research field to alleviate model degradation in the deployment site. To preserve the model performance across multiple testing domains, this work proposes the Curriculum-based Augmented Fourier Domain Adaptation (Curri-AFDA) for robust medical image segmentation. In particular, our curriculum learning strategy is based on the causal relationship of a model under different levels of data shift in the deployment phase, where the higher the shift is, the harder to recognize the variance. Considering this, we progressively introduce more amplitude information from the target domain to the source domain in the frequency space during the curriculum-style training to smoothly schedule the semantic knowledge transfer in an easier-To-harder manner. Besides, we incorporate the training-Time chained augmentation mixing to help expand the data distributions while preserving the domain-invariant semantics, which is beneficial for the acquired model to be more robust and generalize better to unseen domains. Extensive experiments on two segmentation tasks of Retina and Nuclei collected from multiple sites and scanners suggest that our proposed method yields superior adaptation and generalization performance. Meanwhile, our approach proves to be more robust under various corruption types and increasing severity levels. In addition, we show our method is also beneficial in the domain-Adaptive classification task with skin lesion datasets. The code is available at https://github.com/lofrienger/Curri-AFDA. Note to Practitioners-Medical image segmentation is key to improving computer-Assisted diagnosis and intervention autonomy. However, due to domain gaps between different medical sites, deep learning-based segmentation models frequently encounter performance degradation when deployed in a novel domain. Moreover, model robustness is also highly expected to mitigate the effects of data corruption. Considering all these demanding yet practical needs to automate medical applications and benefit healthcare, we propose the Curriculum-based Fourier Domain Adaptation (Curri-AFDA) for medical image segmentation. Extensive experiments on two segmentation tasks with cross-domain datasets show the consistent superiority of our method regarding adaptation and generalization on multiple testing domains and robustness against synthetic corrupted data. Besides, our approach is independent of image modalities because its efficacy does not rely on modality-specific characteristics. In addition, we demonstrate the benefit of our method for image classification besides segmentation in the ablation study. Therefore, our method can potentially be applied in many medical applications and yield improved performance. Future works may be extended by exploring the integration of curriculum learning regime with Fourier domain amplitude fusion in the testing time rather than in the training time like this work and most other existing domain adaptation works.  © 2004-2012 IEEE.","10.1109/TASE.2023.3295600","augmentation mixing; Curriculum learning; domain adaptive medical image segmentation; Fourier transform; robustness","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165898668&doi=10.1109%2fTASE.2023.3295600&partnerID=40&md5=0e459b0eb907ba638edad9fffe62af9d"
"Softflatten-Net: A Deep Convolutional Neural Network Design for Monkeypox Classification From Digital Skin Lesions Images","Karaddi S.H.; Sharma L.D.; Bhattacharya A.","2024","1","1","0","0","0","First occurrence","0","","","","","","","In scenarios where smallpox has been eradicated, the most important human orthopoxvirus pandemic is monkeypox (MP), which is a zoonotic disease that spreads more often. MP is difficult to diagnose, as it has characteristics similar to smallpox, chickenpox (CP), and measles (MS). Polymerase chain reaction (PCR), computerized histopathological nucleic acid amplification test (NAAT), immunohistochemistry, enzyme-linked immunosorbent assay (ELISA), and electron microscope-based skin lesion testing are used for MP detection. But, these tests are time-consuming and may lead to false decision. Therefore, automation in MP detection using skin lesions images is needed for developing affordable and fast solutions. In this work, we have proposed a deep convolutional neural network architecture named the Softflatten network (Softflatten-Net) to identify and categorize MP using skin lesions. This architecture helps to avoid gradient vanishing and overfitting problems. Classification has been conducted in seven distinct scenarios: MP versus normal (NH), MP versus MS, MP versus CP, MP versus other, MP versus CP versus NH, MP versus CP versus MS, and MP versus CP versus MS versus NH. Among binary classification tasks, the Softflatten-Net achieved the maximum classification accuracy (Ac) of 97.25%, a sensitivity (Sen) of 92.94%, and a precision (Pr) of 96.48% in classifying MP from other categories of skin images. In the three-class classification task (MP versus CP versus NH), the proposed network achieved an Ac of 94.71%, a Pr of 92.11%, and a Sen of 87.87%. In the four-class classification scenario, namely, MP versus CP versus MS versus NH, the proposed model achieved the Ac, Pr, and Sen values of 90.73%, 84.21%, and 69.38%, respectively. Furthermore, we have considered 4241 skin images from IEEE data port for nine-class classification and 28771 images from Dermnet for 20-class classification to validate the performance of the proposed Softflatten-Net architecture. The proposed architecture performed satisfactorily on all the three datasets considered in this work and outperformed the existing state-of-the-art models in the literature. A postimage analysis is performed via local interpretable model-agnostic explanations (LIMEs), gradient class activation maps (G-CAMs), and occlusion sensitivity (OS) approaches. In addition, to analyze misclassification, the stochastic neighbor embedding distributed with t (t-SNE) is employed. Visualization techniques aided in identifying the image regions most pertinent for classification. The proposed network can be utilized for early and reliable detection of MP using images of skin lesions.  © 2001-2012 IEEE.","10.1109/JSEN.2024.3445286","Chickenpox (CP); classification; convolutional neural network (CNN); measles (MS); monkeypox (MP); MP skin lesion dataset (MSLD); skin lesions; Softflatten network (Softflatten-Net)","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001207667&doi=10.1109%2fJSEN.2024.3445286&partnerID=40&md5=6941889f353c8d94a4bf783040566685"
"A multiparametric analysis including single-cell and subcellular feature assessment reveals differential behavior of spheroid cultures on distinct ultra-low attachment plate types","Vitacolonna M.; Bruch R.; Agaçi A.; Nürnberg E.; Cesetti T.; Keller F.; Padovani F.; Sauer S.; Schmoller K.M.; Reischl M.; Hafner M.; Rudolf R.","2024","0","1","0","0","0","Unique","0","","","","","","","Spheroids have become principal three-dimensional models to study cancer, developmental processes, and drug efficacy. Single-cell analysis techniques have emerged as ideal tools to gauge the complexity of cellular responses in these models. However, the single-cell quantitative assessment based on 3D-microscopic data of the subcellular distribution of fluorescence markers, such as the nuclear/cytoplasm ratio of transcription factors, has largely remained elusive. For spheroid generation, ultra-low attachment plates are noteworthy due to their simplicity, compatibility with automation, and experimental and commercial accessibility. However, it is unknown whether and to what degree the plate type impacts spheroid formation and biology. This study developed a novel AI-based pipeline for the analysis of 3D-confocal data of optically cleared large spheroids at the wholemount, single-cell, and sub-cellular levels. To identify relevant samples for the pipeline, automated brightfield microscopy was employed to systematically compare the size and eccentricity of spheroids formed in six different plate types using four distinct human cell lines. This showed that all plate types exhibited similar spheroid-forming capabilities and the gross patterns of growth or shrinkage during 4 days after seeding were comparable. Yet, size and eccentricity varied systematically among specific cell lines and plate types. Based on this prescreen, spheroids of HaCaT keratinocytes and HT-29 cancer cells were further assessed. In HaCaT spheroids, the in-depth analysis revealed a correlation between spheroid size, cell proliferation, and the nuclear/cytoplasm ratio of the transcriptional coactivator, YAP1, as well as an inverse correlation with respect to cell differentiation. These findings, yielded with a spheroid model and at a single-cell level, corroborate earlier concepts of the role of YAP1 in cell proliferation and differentiation of keratinocytes in human skin. Further, the results show that the plate type may influence the outcome of experimental campaigns and that it is advisable to scan different plate types for the optimal configuration during a specific investigation. Copyright © 2024 Vitacolonna, Bruch, Agaçi, Nürnberg, Cesetti, Keller, Padovani, Sauer, Schmoller, Reischl, Hafner and Rudolf.","10.3389/fbioe.2024.1422235","CCD-1137Sk; CK14; cytokeratin-14; HaCaT; HT-29; Involucrin; Ki-67; MDA-MB-231","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201563630&doi=10.3389%2ffbioe.2024.1422235&partnerID=40&md5=ff22160b380e376882adf5cdd3e94a8e"
"Correction to: Hierarchical skin lesion image classification with prototypical decision tree (npj Digital Medicine, (2025), 8, 1, (26), 10.1038/s41746-024-01395-z)","Yu Z.; Nguyen T.D.; Ju L.; Gal Y.; Sashindranath M.; Bonnington P.; Zhang L.; Mar V.; Ge Z.","2025","1","1","0","0","0","Unique","0","","","","","","","Correction to: npj Digital Medicinehttps://doi.org/10.1038/s41746-024-01395-z; published online 14 January 2025 This correction pertains to attribution in the “Class Distance Guided Prototype Learning” section, ensuring proper recognition of the work from Landrieu et al. [38], as we adapt it for addressing hierarchical skin tree model optimization. While reference [38] was included, we need to explicitly credit their foundational work in this context. In the first paragraph of the section, add: Revise: “According to [41], the distortion of..” to “According to [41] and following the formulation in [38], the distortion of..” Revise: “Hence, we introduce a scale..” to “Hence, as introduced in [38], we adopt a scale..” The original Article has been corrected. © The Author(s) 2025.","10.1038/s41746-025-01494-5","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218424636&doi=10.1038%2fs41746-025-01494-5&partnerID=40&md5=fdf637a2ae8a93ac529a4ee50788a062"
"Multiclass skin lesion classification and localziation from dermoscopic images using a novel network-level fused deep architecture and explainable artificial intelligence","Arshad M.; Khan M.A.; Almujally N.A.; Alasiry A.; Marzougui M.; Nam Y.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Background and objective: Early detection and classification of skin cancer are critical for improving patient outcomes. Dermoscopic image analysis using Computer-Aided Diagnostics (CAD) is a powerful tool to assist dermatologists in identifying and classifying skin lesions. Traditional machine learning models require extensive feature engineering, which is time-consuming and less effective in handling complex data like skin lesions. This study proposes a deep learning-based network-level fusion architecture that integrates multiple deep models to enhance the classification and localization of skin lesions in dermoscopic images. The goal is to address challenges like irregular lesion shapes, inter-class similarities, and class imbalances while providing explainability through artificial intelligence. Methods: A novel hybrid contrast enhancement technique was applied for pre-processing and dataset augmentation. Two deep learning models, a 5-block inverted residual network and a 6-block inverted bottleneck network, were designed and fused at the network level using a depth concatenation approach. The models were trained using Bayesian optimization for hyperparameter tuning. Feature extraction was performed with a global average pooling layer, and shallow neural networks were used for final classification. Explainable AI techniques, including LIME, were used to interpret model predictions and localize lesion regions. Experiments were conducted on two publicly available datasets, HAM10000 and ISIC2018, which were split into training and testing sets. Results: The proposed fused architecture achieved high classification accuracy, with results of 91.3% and 90.7% on the HAM10000 and ISIC2018 datasets, respectively. Sensitivity, precision, and F1-scores were significantly improved after data augmentation, with precision rates of up to 90.91%. The explainable AI component effectively localized lesion areas with high confidence, enhancing the model’s interpretability. Conclusions: The network-level fusion architecture combined with explainable AI techniques significantly improved the classification and localization of skin lesions. The augmentation and contrast enhancement processes enhanced lesion visibility, while fusion of models optimized classification accuracy. This approach shows potential for implementation in CAD systems for skin cancer diagnosis, although future work is required to address the limitations of computational resource requirements and training time. Clinical trail number: Not applicable. © The Author(s) 2025.","10.1186/s12911-025-03051-2","Deep learning; Dermoscopy; Explainable artificial intelligence; Network-level fusion; Shallow neural network; Skin cancer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010015005&doi=10.1186%2fs12911-025-03051-2&partnerID=40&md5=92ec35a808e4e90caf60ddc93a5cd259"
"Explainable deep learning approaches for skin cancer diagnosis","Alrabai A.; Echtioui A.; Kallel F.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer continues to be a significant worldwide health issue, highlighting the critical need for precise and timely detection to guarantee positive patient outcomes and efficient treatment. While deep learning algorithms have demonstrated significant potential, they are frequently observed as black-box models, posing challenges for dermatologists in interpreting and validating their decisions. This study integrates deep learning with explainable artificial intelligence to address the complexities inherent in skin cancer detection. Four advanced pre-trained models—InceptionV3, Xception, ResNet50V2, and DenseNet121—are employed for the classification of skin lesions. To deal with the class imbalance and to improve the generalisation of the model, image augmentation techniques are considered. Transparency in the decision-making process is achieved through XAI, which is essential in medical contexts where interpretability fosters trust and supports the seamless adoption of AI-driven diagnostic systems in clinical workflows. Extensive evaluation reveals that better performances are obtained when Xception model is considered, achieving an accuracy of 90.15%, precision of 90.02%, recall of 90.25%, and an F1 score of 90.10%. These results highlight the transformative impact of integrating deep learning and XAI in skin cancer diagnosis, laying the foundation for future advancements in medical image analysis. The capacity of these technologies to enable early and accurate detection holds substantial promise for improving patient care, reducing healthcare costs, and increasing survival rates among individuals affected by skin cancer. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2025.","10.1007/s13721-025-00554-w","Classification; Deep Learning; Pre-trained Models; Skin Cancer; XAI","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009625917&doi=10.1007%2fs13721-025-00554-w&partnerID=40&md5=0c3beb1a8439856a6288c957212b3f3d"
"A novel framework of skin cancer detection using Yolo-Unet++ segmentation model with adaptive deep learning-based classification","Jenita Christy S.; Rosline Nesa Kumari G.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancers, particularly melanoma are significant health issues. Early identification of skin disease models is implemented with various dermoscopic examinations, medical screening, and so on. Yet, this traditional model requires assistance from medical experts to offer more accurate diagnosis outcomes. Moreover, identifying skin disease at the starting stages is crucial for enhancing the treatment plans of the victims. Recently, computerized techniques known as computer-aided diagnosis applications have been employed to perform early prognosis of skin diseases. Additionally, artificial intelligence is utilized to carry out automatic detection of skin diseases via skin images. Still, automatic diagnosis of skin cancers at the beginning stages is hard because of the lack of contrast between the moles of melanoma and skin areas as well as the high degree of color similarity among the affected and unaffected regions within the skin. Currently, deep learning techniques provide promising outcomes in medical imaging. Thus, the research work implements a novel skin cancer detection model is presented using deep learning. The standard datasets are considered for data collection process. The total image acquired from both dataset 1 and dataset 2 contains 5000 and 200 images. In dataset 1, the total 5000 image are divided into 3750 for training and 1250 for testing. Likewise, a total 200 images of dataset 2 are divided into 150 images for training and 50 images for testing. Then, the collected skin image is fed to the abnormality segmentation process. Here, the Yolo-Unet++ with LovaszSoftMax with IoU Loss Function (Y-UNet++-LS-IoULF) is utilized for segmenting the images. Later, the segmented images are forwarded to the vision transformer (ViT)-aided Adaptive EfficientB7 network (ViT-AEB7) for detecting skin cancer. Here, an updated random variable-aided Kookaburra optimization algorithm (URV-KOA) is used for tuning the network parameters of the EfficientB7 model. Lastly, an empirical investigation is executed for the suggested system by comparing it with classical-related approaches. The findings of the recommended method attain 89.91% and 84.92% in terms of F1-score and MCC measure. Thus, the significant advancement in the developed model attains better detected outcomes and enables accurate treatment planning. © The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature 2025.","10.1007/s13721-025-00568-4","Adaptive EfficientnetB7 network; IoU Loss Function; Lovasz Softmax; Skin cancer detection; Updated random variable-aided Kookaburra optimization algorithm; Vision transformer; Yolo-Unet++","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012449642&doi=10.1007%2fs13721-025-00568-4&partnerID=40&md5=35c551d1c506dd928634adf60ca866db"
"Melanoma skin cancer detection and classification using cycle-consistent simplicial adversarial attention adaptation networks with banyan tree growth optimization in medical image processing","Shukla A.K.; Agrawal G.; Giri Prasad R.; Bachute M.","2025","1","1","0","0","0","First occurrence","0","","","","","","","The incidence of Skin Cancer (SC) is on the rise globally, making it one of the most common and dangerous types of cancer. To address these disadvantages, this paper introduces a new way of melanoma detection and classification employing Cycle-Consistent Simplicial Adversarial Attention Adaptation Networks (Cy-CSAAANets) optimized by Banyan Tree Growth Optimization (BTGO), in medical image processing. Input images are obtained from four datasets—ISIC2018, ISIC2019, ISIC2020, and HAM10000 originally with noise from various sources. To solve this, Window-Aware Guided Image Filtering (WGIF) is utilized for noise reduction. The skin cancer areas are segmented by the Adaptively Regularized Kernel Fuzzy C-Means (ARKFCM) method, followed by feature extraction using a Modified ResNet-152 (MResNet-152) network. Classification is done through Cy-CSAAANets, which are optimized by BTGO to identify and classify melanoma types and distinguish between normal and abnormal areas. With the python implementation, the proposed Cy-CSAAANets + BTGO approach achieved 99.2 % accuracy, 97.8 % precision, and 99.1 % recall on the ISIC2018 dataset. On the ISIC2019 dataset, the method achieved 98.1 % accuracy, 98.0 % precision, and 95.8 % recall. On the ISIC2020 dataset, it achieved 98.7 % accuracy, 96.5 % precision, and 95.9 % recall, whereas for the HAM10000 dataset, the model achieved 99.1 % accuracy, 96.8 % precision, and 99.1 % recall, beating existing current state-of-the-art methods. The outcomes show that the combination of advanced methods can improve classification precision, and hence this technique is a worthwhile candidate for early melanoma diagnosis in clinical practices, capable of enhancing patient outcomes. © 2025 Elsevier Ltd","10.1016/j.bspc.2025.107914","Adaptively Regularized Kernel Fuzzy C-Means; Banyan Tree Growth Optimization; Cycle-Consistent Simplicial Adversarial Attention Adaptation Networks; Melanoma Skin Cancer Detection; Modified ResNet-152 Network; Window-Aware Guided Image Filtering","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003886259&doi=10.1016%2fj.bspc.2025.107914&partnerID=40&md5=dd39bb39be7ca86bbf710ccde8ee8665"
"Multi-resolution vision transformer model for histopathological skin cancer subtype classification using whole slide images","Chaurasia A.K.; Toohey P.W.; Harris H.C.; Hewitt A.W.","2025","1","1","0","0","0","Unique","0","","","","","","","Background: Digital pathology has significantly advanced cancer diagnosis by enabling high-resolution visualisation and assessment of tissue specimens. However, the manual analysis of these images remains labour-intensive and susceptible to variability, resulting in inconsistencies in diagnosis and treatment decisions. Herein, a multi-resolution model was developed and externally validated for classifying subtypes of skin cancer from whole slide images (WSIs). Methods: A dataset comprising approximately 1.13 million histological patches generated by dividing WSIs from Non-Melanoma Skin Cancer Segmentation (NMSCS) and Heidelberg datasets into non-overlapping tiles. All patches were normalised using the Macenko method prior to training a self-supervised vision transformer (ViT)-based model. The model was designed to classify six classes, including the most common subtypes of skin cancer: basal cell carcinoma (BCC), squamous cell carcinoma (SCC), intraepidermal carcinoma (IEC), Melanoma, Naevi, and Non-cancerous tissue. Multi-resolution data at 10x, 20x, 40x, and 400x magnifications were incorporated to enhance robustness. The model was externally validated on 5,147 slides from 4,066 patients for non-melanoma cancer subtypes. Model performance was evaluated using classification metrics, and the quadratic weighted Cohen's Kappa (κ) score was used to measure the agreement between the model's predictions and the actual labels with a 95 % confidence interval (CI). Results: The multi-resolution model demonstrated strong classification performance across six classes, achieving an overall κ score of 0.859 (95 % CI: 0.851, 0.866) and 0.898 (95 % CI: 0.892, 0.904) on the validation and testing sets, respectively, reflecting robust performance across diverse skin cancer subtypes. The multi-resolution model for non-melanoma skin cancer exhibited superior performance, achieving an overall κ score of 0.919 (95 % CI: 0.914, 0.924) on the validation set. On the testing set, the κ score ranged from 0.996 to 0.889 with magnifications of 10x, 20x, 40x, and 400x. Attention maps highlighted clinically relevant features for cancerous tissue at different magnifications, aiding interpretability. Additionally, the model obtained a κ score of 0.791 (95 % CI: 0.774, 0.808) on the external data at the slide level, indicating substantial agreement between the model's prediction and the actual label of WSIs. Conclusion: The multi-resolution model has the potential to assist anatomical pathologists in automatically detecting, highlighting, and classifying subtypes of melanoma and non-melanoma skin cancer subtypes directly from WSIs. Its capability could lead to more effective clinical decision-making in digital pathology, ultimately improving patient outcomes. For non-melanoma skin cancer, the model could be deployed in resource-limited settings with high incidence of the disease and limited access to experienced dermatopathologists. © 2025 The Authors","10.1016/j.compbiomed.2025.110724","Digital pathology; Melanoma; Multi-resolution model; Non-melanoma skin cancer; Skin cancer classification; Vision transformer; Whole slide images","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009943638&doi=10.1016%2fj.compbiomed.2025.110724&partnerID=40&md5=a7c56f4c062cd96649bfc7390deb3ff0"
"Automated Classification of Skin Diseases Using Microscopic Images: A Machine Learning Approach","Karapinar Senturk Z.; Guler R.; Ozcan Y.; Gamsizkan M.","2025","1","1","0","0","0","Unique","0","","","","","","","This study presents a machine learning-based approach for the automated classification of skin diseases, specifically targeting morphea and lichen sclerosus, using microscopic images. The proposed method involves a systematic workflow, including image preprocessing techniques such as resizing, Reinhard normalization, Gaussian filtering, and CLAHE histogram equalization to enhance image quality. Feature extraction was performed using Gray-Level Co-occurrence Matrix (GLCM) and histogram-based statistical methods, capturing texture and intensity characteristics of skin tissues. Several classification models, including Support Vector Machine (SVM), Artificial Neural Network (ANN), Decision Tree (DT), Random Forest (RF), K-Nearest Neighbors (K-NN), and Logistic Regression (LR), were evaluated using accuracy, precision, recall, and F1 score, with hyperparameter optimization via grid search. The experimental results revealed that the combined feature set (GLCM + Histogram) achieved the highest performance, with the RF and K-NN models yielding a 100% in all performance metrics, including accuracy, sensitivity, recall, and F1-score. The study introduces a novel approach by examining these two diseases simultaneously, offering a reliable tool to support dermatologists with accurate and quick diagnoses. Future work will focus on expanding the dataset, exploring advanced deep learning techniques, and integrating clinical metadata to enhance model generalizability. © 2025 The Author(s). Concurrency and Computation: Practice and Experience published by John Wiley & Sons Ltd.","10.1002/cpe.70220","machine learning; skin disease; whole slide image","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012477232&doi=10.1002%2fcpe.70220&partnerID=40&md5=81172f39d47654e1cbab430138c1b647"
"Explainable clinical diagnosis through unexploited yet optimized fine-tuned ConvNeXt Models for accurate monkeypox disease classification","Waqar M.; Khan Z.A.; Khawaja S.T.; Chaudhary N.I.; Khan S.; Cheema K.M.; Khan M.F.; Ahmed S.S.; Raja M.A.Z.","2025","0","1","0","0","0","Unique","0","","","","","","","Deep learning (DL) has had an incredible influence on many different scientific areas over the past couple of decades. Particularly in the field of healthcare, DL strategies were able to outclass other existing methodologies in image processing. The rapid expansion of the monkeypox endemic to over 40 nations apart from Africa has prompted serious worries in the realm of public health. Given that monkeypox can have symptoms that are akin to both chickenpox and measles, early detection can be difficult. Fortunately, due to the developments in artificial intelligence approaches, it can be implemented to promptly and accurately identify monkeypox disease using visual data information. Many DL driven techniques have already been exploited in the literature for skin related issues, which have provided accurate results to some extent. These models were dependent on extensive computational and time resources due to which the real-time applicability is difficult. Rather of building and training CNNs from scratch, this study uses transfer learning (TL) technique to fine-tune pre-trained networks, particularly exploiting various versions of ConvNeXt, by substituting last layer with additional task specific ones. A number of pre-processing and data augmentation methods have also been assessed and adjusted with regard to computing time and performance. The proposed study performs the binary and multi class monkeypox disease classification task. Promising accurate results of 99.9 % on the benchmark MSLD (binary class) dataset and 94 % on the MSLD v2.0 (multi-class) dataset is obtained by fine-tuned TL-based ConvNeXtSmall and ConvNeXtBase architecture with Adafactor optimization technique, demonstrating the practicality of the suggested framework as a substitute for the current ones. The proposed model is assessed through both standard train-test split and k-fold cross validation techniques. Furthermore, performance of models is also assessed on several other metrics including recall, F1 score, precision and multiple statistical tests incorporated with explainable AI methods for better interpretability of results. The concerns regarding the real-time applicability are tackled by utilizing the less time consuming and computationally efficient networks through the exploitation of transfer learning capabilities. Moreover, the explainable findings of the proposed study will be highly valuable for the healthcare professionals to understand the decisive behavior of the model and make informed clinical decisions. © 2025","10.1016/j.slast.2025.100336","ConvNeXt; Convolutional neural network; Explainable artificial intelligence; Monkeypox disease; Transfer learning","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011835695&doi=10.1016%2fj.slast.2025.100336&partnerID=40&md5=c7284b7aa7c9a2838697049f3cdad720"
"LBO-MPAM: Ladybug Beetle Optimization-based multilayer perceptron attention module for segmenting the skin lesion and automatic localization","V S.; Natrajan K.; S S.P.; K S.K.","2025","1","1","0","0","0","Unique","0","","","","","","","In recent years, skin cancer has been the most dangerous disease noticed among people worldwide. Skin cancer should be identified earlier to reduce the rate of mortality. Employing dermoscopic images can identify and categorise skin cancer effectively. But, the visual evaluation is a complex procedure to be done in the dermoscopic image. However, Deep learning (DL) is an efficient method for skin cancer detection; however, segmenting the skin lesion and automatic localisation in an earlier stage is complicated. In this paper, a novel Ladybug Beetle Optimization-Double Attention Based Multilevel 1-D CNN (LBO-DAM 1-D CNN) technique is proposed to detect and classify skin cancer. To improve skin lesion type discriminability, the two types of attention modules are introduced. The Ultra-Lightweight Subspace Attention Module (ULSAM) is utilised for classifying the feature maps into different stages to validate the frequency from different image samples. However, the multilayer perceptron attention module (MLPAM) is determined to provide information regarding skin cancer classification and diminish the noise and unwanted data. To minimise data loss, it is then combined with hierarchical complementarity during classification. Second, a modified MLPAM is used to extract significant feature spaces for network learning, select the most important information, and reduce feature space redundancy. The Ladybug Beetle Optimization (LBO) algorithm provides the optimal classification solution by minimising the loss rate of DAM 1-D CNN architecture. The experimentation is conducted on three different datasets such as ISIC2020, HAM10000, and the melanoma detection dataset. The experimental results revealed that the proposed method is compared with different existing methods such as IMFO-KELM, Mask RCNN, M-SVM, DCNN-9, and TL-CNN with different datasets. These methods attained 94.56, 92.65, 90.56, 88.65, and 95.5 for the ISIC2020 dataset but the proposed method enhanced the classification performance by attaining 97.02. Also, the validation is based on metrics, namely, accuracy, precision, sensitivity, and F1-score of 97.03%, 97.05%, 97.58%, and 97.27% for a total of 500 epochs. © 2024 Informa UK Limited, trading as Taylor & Francis Group.","10.1080/0952813X.2023.2301374","1-dimensional CNN; attention modules; dermoscopic images; Ladybug Beetle Optimization; Skin cancer disease","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182822722&doi=10.1080%2f0952813X.2023.2301374&partnerID=40&md5=3cc76ff5e1e7db4c4bb9cd5b92515e36"
"Novel vision transformer and data augmentation technique for efficient detection of monkeypox disease","Alarfaj A.A.; Ahmad S.; Hakeem A.M.; Alabdulqader E.A.; PERO C.; Alsubai S.; Innab N.; Ashraf I.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Recent technological advancements have paved the way for the optimization of medical processes, particularly automated disease detection. Moreover, the adoption of machine learning (ML) has greatly helped in automating disease detection. Such approaches can detect various diseases early, enabling timely treatment to save countless lives. Early and accurate diagnosis is very important for diseases like monkeypox, to curb its spread. Monkeypox is a viral disease caused by double-stranded DNA and can be transmitted through close contact with infected humans or animals. It’s early identification and accurate lesion diagnosis are critical to contain the disease. This study proposes an automated approach to optimize the diagnosis of monkeypox disease using a novel vision transformer, which is utilized due to its effectiveness for feature extraction. The Proposed approach’s efficiency and accuracy are tested on a public benchmark dataset comprising a variety of skin lesions of different ages and genders. In addition, data augmentation involves rotation, scaling, and flipping thereby enhancing the density of the training data set for better generalization of ML models. Experiments involve binary, as well as, multi-class classification. For the binary class, the proposed model achieves an accuracy of 97.63%, outperforming traditional ML and deep learning (DL) techniques. In the case of multi-class classification with monkeypox, measles, normal, HFMD, cowpox, and chickenpox classes, the proposed model archives an accuracy of 90.61% while precision, recall, and F1 scores are 91.39%, 89.17%, and 90.28%, respectively. Furthermore, the proposed approach shows average accuracy, precision, recall, and F1 scores of 97.54%, 96.19%, 95.16%, and 95.67%, respectively for five-fold cross-validation. Experiments demonstrate that the combination of data augmentation techniques and the vision transformer model significantly optimizes diagnostic performance. In brief, advanced DL architectures with image data-augmentation strategies can help achieve optimal processes for diagnosing diseases like monkeypox, and avoid widespread outbreaks. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.","10.1007/s11042-024-20456-9","Data augmentation; Medical imaging processing; Monkeypox classification; Optimization of medical process; Vision transformer model","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012432749&doi=10.1007%2fs11042-024-20456-9&partnerID=40&md5=03be75728acd7443c83c6c7cc218b6e6"
"Explainable Liver Segmentation and Volume Assessment Using Parallel Cropping","Satpute N.; Gaikwad N.B.; Khare S.K.; Gómez-Luna J.; Olivares J.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Accurate liver segmentation and volume estimation from CT images are critical for diagnosis, surgical planning, and treatment monitoring. This paper proposes a GPU-accelerated voxel-level cropping method that localizes the liver region in a single pass, significantly reducing unnecessary computation and memory transfers. We integrate this pre-processing step into two segmentation pipelines: a traditional Chan-Vese model and a deep learning U-Net trained on the LiTS dataset. After segmentation, a seeded region growing algorithm is used for 3D liver volume assessment. Our method reduces unnecessary image data by an average of 90%, speeds up segmentation by 1.39× for Chan-Vese, and improves dice scores from 0.938 to 0.960. When integrated into U-Net pipelines, the post-processed dice score rises drastically from 0.521 to 0.956. Additionally, the voxel-based cropping approach achieves a 2.29× acceleration compared to state-of-the-art slice-based methods in 3D volume assessment. Our results demonstrate high segmentation accuracy and precise volume estimates with errors below 2.5%. This proposal offers a scalable, interpretable, efficient liver segmentation and volume assessment solution. It eliminates unwanted artifacts and facilitates real-time deployment in clinical environments where transparency and resource constraints are critical. It is also tested in other anatomical structures such as skin, lungs, and vessels, enabling broader applicability in medical imaging. © 2025 by the authors.","10.3390/app15147807","3D volume assessment; GPU; image segmentation; voxel-based cropping","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011867058&doi=10.3390%2fapp15147807&partnerID=40&md5=7ad74b380d5d73dd1c1d843d07b4a4c1"
"Categorical classification of skin cancer using a weighted ensemble of transfer learning with test time augmentation","Ibrahim A.T.; Abdullahi M.; Kana A.F.D.; Mohammed M.T.; Hassan I.H.","2025","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is the abnormal development of cells on the surface of the skin and is one of the most fatal diseases in humans. It usually appears in locations that are exposed to the sun, but can also appear in areas that are not regularly exposed to the sun. Due to the striking similarities between benign and malignant lesions, skin cancer detection remains a problem, even for expert dermatologists. Considering the inability of dermatologists to diagnose skin cancer accurately, a convolutional neural network (CNN) approach was used for skin cancer diagnosis. However, the CNN model requires a significant number of image datasets for better performance; thus, image augmentation and transfer learning techniques have been used in this study to boost the number of images and the performance of the model, because there are a limited number of medical images. This study proposes an ensemble transfer-learning-based model that can efficiently classify skin lesions into one of seven categories to aid dermatologists in skin cancer detection: (i) actinic keratoses, (ii) basal cell carcinoma, (iii) benign keratosis, (iv) dermatofibroma, (v) melanocytic nevi, (vi) melanoma, and (vii) vascular skin lesions. Five transfer learning models were used as the basis of the ensemble: MobileNet, EfficientNetV2B2, Xception, ResNext101, and DenseNet201. In addition to the stratified 10-fold cross-validation, the results of each individual model were fused to achieve greater classification accuracy. An annealing learning rate scheduler and test time augmentation (TTA) were also used to increase the performance of the model during the training and testing stages. A total of 10,015 publicly available dermoscopy images from the HAM10000 (Human Against Machine) dataset, which contained samples from the seven common skin lesion categories, were used to train and evaluate the models. The proposed technique attained 94.49% accuracy on the dataset. These results suggest that this strategy can be useful for improving the accuracy of skin cancer classification. However, the weighted average of F1-score, recall, and precision were obtained to be 94.68%, 94.49%, and 95.07%, respectively. © 2024 Xi'an Jiaotong University","10.1016/j.dsm.2024.10.002","Annealing learning rate scheduler; Deep convolutional neural network; Dermoscopy; Skin cancer; Test time augmentation; Transfer learning","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007224139&doi=10.1016%2fj.dsm.2024.10.002&partnerID=40&md5=b4abcd11d5b53f8939a02f87d4f02dc8"
"Fractional gannet humming optimization enabled deep convolutional neural network for detection and segmentation of skin cancer","Satish A.R.; Maram B.; Perumalla V.R.; K M.R.","2025","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is a dangerous disorder that is caused by an unchecked proliferation of aberrant skin cells that produce genetic mutations on the skin. When ultraviolet (UV) radiation from sunshine or tanning beds damages the skin cells in a way that leads to their rapid multiplication and formation of malignant tumours. Certain forms of skin cancer metastasize along nerves. This results in tingling, pain, itching, numbness, or a sensation like ants crawling under the skin. Skin cancer spreads to deeper tissues, including cartilage, muscle, and bone. The earlier prediction of skin lesions increases the possibility of survival rate. However, during diagnosis, an anomalous finding is made, and the condition is diagnosed as cancer. To overcome these challenges, a novel deep learning (DL) technique is developed in this research article for categorizing skin cancer by employing the proposed Fractional Gannet Humming Optimization_Deep Convolutional Neural Network (FGHO_DeepCNN). Initially, the input skin cancer image is subjected to the image pre-processing phase. The image pre-processing is done by the bilateral filter. Afterwards, skin lesion segmentation is carried out using an encoder-decoder with Dense-Residual block (DRB), which is trained by the Fractional Gannet optimization algorithm (FGOA). Here, the FGOA is formed by the integration of the Fractional Calculus (FC) concept with the Gannet Optimization Algorithm (GOA). Thereafter, image augmentation is done to enlarge the segmented image using geometric and colour space transformation. After that, a feature extraction process is conducted to obtain the significant features, like Completed Local Binary Pattern (CLBP), Gray Level Co-occurrence Matrix (GLCM), Local Vector Pattern (LVP), Significant Local Binary Pattern (SLBP) and CNN features. Finally, skin cancer detection is done using DeepCNN, which is tuned by the proposed FGHO. Here, the proposed FGHO is formed by the combination of FGOA with the artificial hummingbird algorithm (AHA). The experimental outcomes of the proposed FGHO_DeepCNN approach attained a better Positive Predictive Value (PPV), Negative Predictive Value (NPV), True Positive Rate (TPR), and True Negative Rate (TNR), and accuracy with values of 89.80 %, 89.40 %, 94.50 %, 94.00 % and 93.40 % respectively. The employed FGHO_DeepCNN has acquired excellent performance, thus achieving a PPV of 91.68 %, NPV of 88.46 %, TPR of 91.68 %, TNR of 91.23 % and accuracy of 90.67 % for dataset 2. In dataset 3, the FGHO_DeepCNN obtained superior performance than other techniques with a PPV of 90.56 %, NPV of 90.36 %, TPR of 90.95 %, TNR of 90.87 %, and accuracy of 90.15 %. The practical significance of the proposed FGHO_DeepCNN approach is that it is widely used in dermatology clinics and hospitals to detect skin cancer. © 2025 Elsevier B.V.","10.1016/j.neucom.2025.129816","Artificial Hummingbird Algorithm (AHA); Deep Convolutional Neural Network (DeepCNN); Deep Learning (DL); Fractional Calculus (FC); Gannet Optimization Algorithm (GOA)","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000534518&doi=10.1016%2fj.neucom.2025.129816&partnerID=40&md5=e86a86648617121faae2402a6f506bc0"
"An enhanced harmonic densely connected hybrid transformer network architecture for chronic wound segmentation utilising multi-colour space tensor merging","Cassidy B.; McBride C.; Kendrick C.; Reeves N.D.; Pappachan J.M.; Fernandez C.J.; Chacko E.; Brüngel R.; Friedrich C.M.; Alotaibi M.; AlWabel A.A.; Alderwish M.; Lai K.-Y.; Yap M.H.","2025","0","1","0","0","0","Unique","0","","","","","","","Chronic wounds and associated complications present ever growing burdens for clinics and hospitals world wide. Venous, arterial, diabetic, and pressure wounds are becoming increasingly common globally. These conditions can result in highly debilitating repercussions for those affected, with limb amputations and increased mortality risk resulting from infection becoming more common. New methods to assist clinicians in chronic wound care are therefore vital to maintain high quality care standards. This paper presents an improved HarDNet segmentation architecture which integrates a contrast-eliminating component in the initial layers of the network to enhance feature learning. We also utilise a multi-colour space tensor merging process and adjust the harmonic shape of the convolution blocks to facilitate these additional features. We train our proposed model using wound images from light skinned patients and test the model on two test sets (one set with ground truth, and one without) comprising only darker skinned cases. Subjective ratings are obtained from clinical wound experts with intraclass correlation coefficient used to determine inter-rater reliability. For the dark skin tone test set with ground truth, when comparing the baseline results (DSC=0.6389, IoU=0.5350) with the results for the proposed model (DSC=0.7610, IoU=0.6620) we demonstrate improvements in terms of Dice similarity coefficient (+0.1221) and intersection over union (+0.1270). Measures from the qualitative analysis also indicate improvements in terms of high expert ratings, with improvements of >3% demonstrated when comparing the baseline model with the proposed model. This paper presents the first study to focus on darker skin tones for chronic wound segmentation using models trained only on wound images exhibiting lighter skin. Diabetes is highly prevalent in countries where patients have darker skin tones, highlighting the need for a greater focus on such cases. Additionally, we conduct the largest qualitative study to date for chronic wound segmentation. All source code for this study is available at: https://github.com/mmu-dermatology-research/hardnet-cws. © 2025 The Authors","10.1016/j.compbiomed.2025.110172","Arterial ulcers; Chronic wounds; Deep learning; Diabetic foot ulcers; Hybrid transformer; Pressure ulcers; Segmentation; Synthetic wounds; Venous ulcers; Wounds analysis","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105004042955&doi=10.1016%2fj.compbiomed.2025.110172&partnerID=40&md5=70d484fb26479da97f51853072308421"
"Transformative Advances in AI for Precise Cancer Detection: A Comprehensive Review of Non-Invasive Techniques","Rai H.M.; Yoo J.; Dashkevych S.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Cancer continues to be a primary cause of death worldwide, highlighting the critical need for early diagnosis methods. Automated, quick, and efficient technologies are critical to this endeavor, yet considerable gaps remain in this field. A comprehensive review was undertaken to examine seven cancer types characterized by elevated prevalence and mortality: lung, prostate, brain, skin, breast, leukemia, and colorectal cancer. The study aimed to reveal gaps in the existing research and compare traditional machine learning (TML) with deep learning (DL) methodologies, since such contrasts have been not much explored. A total of 320 publications were carefully chosen for study, including 150 that focused on TML methods and 170 that address DL techniques for the classification of cancer. Diverse parameters were used to assess these investigations, encompassing publication year, employed databases, data sample, classifier, modalities, and evaluation metrics. Separate evaluations were conducted for each cancer type and methodology, yielding 14 unique review tables. The assessment of each cancer type using ML/DL independently relied on four standard criteria: High performance (> 99%), Limited performance (< 85%), key findings, and key challenges. These studies were accompanied by a brief descriptive outline of the features, classifiers, public databases, and evaluation metrics that were utilized in the review process. The study concluded by offering general conclusions that highlighted the overall findings, overall challenges observed during the investigation. This thorough review seeks to improve clinical applications and guide future research initiatives in the persistent fight against cancer. © The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE) 2025.","10.1007/s11831-024-10219-y","","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217228092&doi=10.1007%2fs11831-024-10219-y&partnerID=40&md5=057ac563d3e27c79b2fad88d20c74266"
"PMFSNet: Polarized multi-scale feature self-attention network for lightweight medical image segmentation","Zhong J.; Tian W.; Xie Y.; Liu Z.; Ou J.; Tian T.; Zhang L.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Background and objectives: Current state-of-the-art medical image segmentation methods prioritize precision but often at the expense of increased computational demands and larger model sizes. Applying these large-scale models to the relatively limited scale of medical image datasets tends to induce redundant computation, complicating the process without the necessary benefits. These approaches increase complexity and pose challenges for integrating and deploying lightweight models on edge devices. For instance, recent transformer-based models have excelled in 2D and 3D medical image segmentation due to their extensive receptive fields and high parameter count. However, their effectiveness comes with the risk of overfitting when applied to small datasets. It often neglects the vital inductive biases of Convolutional Neural Networks (CNNs), essential for local feature representation. Methods: In this work, we propose PMFSNet, a novel medical imaging segmentation model that effectively balances global and local feature processing while avoiding the computational redundancy typical of larger models. PMFSNet streamlines the UNet-based hierarchical structure and simplifies the self-attention mechanism's computational complexity, making it suitable for lightweight applications. It incorporates a plug-and-play PMFS block, a multi-scale feature enhancement module based on attention mechanisms, to capture long-term dependencies. Results: The extensive comprehensive results demonstrate that our method achieves superior performance in various segmentation tasks on different data scales even with fewer than a million parameters. Results reveal that our PMFSNet achieves IoU of 84.68%, 82.02%, 78.82%, and 76.48% on public datasets of 3D CBCT Tooth, ovarian tumors ultrasound (MMOTU), skin lesions dermoscopy (ISIC 2018), and gastrointestinal polyp (Kvasir SEG), and yields DSC of 78.29%, 77.45%, and 78.04% on three retinal vessel segmentation datasets, DRIVE, STARE, and CHASE-DB1, respectively. Conclusion: Our proposed model exhibits competitive performance across various datasets, accomplishing this with significantly fewer model parameters and inference time, demonstrating its value in model integration and deployment. It strikes an optimal compromise between efficiency and performance and can be a highly efficient solution for medical image analysis in resource-constrained clinical environments. The source code is available at https://github.com/yykzjh/PMFSNet. © 2025 Elsevier B.V.","10.1016/j.cmpb.2025.108611","Attention mechanism; Lightweight neural network; Medical image segmentation; Multi-scale feature fusion","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85216564615&doi=10.1016%2fj.cmpb.2025.108611&partnerID=40&md5=05777e6d7a3c586be5cde3d9a75e105d"
"Design of an Efficient UNet-Based Transfer Learning Model for Enhancing Skin Cancer Segmentation and Classification Performance","Verma N.; Mishra P.K.","2025","1","1","0","0","0","Unique","0","","","","","","","Accurate and efficient segmentation and classification are indispensable for the early diagnosis and treatment of skin cancer, a common and potentially fatal condition. Combining the UNet architecture with Auto Encoders for robust skin cancer segmentation, followed by binary cascade Convolutional Neural Networks (CNNs). In this text, we present a novel method for accurately classifying melanoma and basal cell carcinoma. Existing models are limited in their ability to achieve high precision, accuracy, and recall rates while maintaining a high Peak Signal-to-Noise Ratio (PSNR) for accurate image reconstruction, which necessitates this research. Our proposed model overcomes these limitations and performs exceptionally well on datasets: ISIC, HAM10000, PH2 Dataset, and Dermofit Image Libraries. When UNet and Auto Encoders are used, the advantages of both architectures are combined. The UNet architecture, renowned for its superior performance in image segmentation tasks, provides a solid foundation for separating skin cancer regions from surrounding tissue. The Auto Encoder component simultaneously facilitates feature extraction and image reconstruction, leading to improved representation learning and segmentation results. Utilizing the complementary capabilities of these models, our method improves the accuracy and efficiency of skin cancer segmentations. Using binary cascade CNNs for classification also improves our model's performance. The binary cascade architecture employs a hierarchical classification method that iteratively improves classification choices at each stage. This facilitates the differentiation between basal cell carcinoma, melanoma, and melanocytic nevi, resulting in highly accurate and trustworthy predictions. Extensive experiments were conducted on the ISIC, HAM10000, PH2 Dataset, and Dermofit Image Library to evaluate the performance of our proposed model. The achieved precision of 99.2%, accuracy of 98.3%, recall of 98.9%, and PSNR greater than 42dB demonstrate the superior functionality and effectiveness of our strategy. These results suggest that our model has a great deal of potential for assisting dermatologists in the early identification and classification of skin cancer, ultimately leading to improved patient outcomes. The combination of UNet with Auto Encoders and binary cascade CNNs has proven effective for segmenting and classifying skin cancer. Our proposed model outperforms current methods in terms of precision, accuracy, recall, and PSNR, demonstrating its potential to have a significant impact on the field of dermatology and aid in the early detection and treatment of skin cancers. © 2025, Modern Education and Computer Science Press. All rights reserved.","10.5815/ijigsp.2025.02.05","Auto Encoders; Binary Cascade CNN; CNN; Skin Cancer; UNet","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002072131&doi=10.5815%2fijigsp.2025.02.05&partnerID=40&md5=a67ca9f085776cc12a4401982cf7f016"
"YOLOSkin: A fusion framework for improved skin cancer diagnosis using YOLO detectors on Nvidia Jetson Nano","Aishwarya N.; Yaythish Kannaa G.S.; Seemakurthy K.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer stands out as one of the most common and lethal forms of cancer, characterized by the rapid and uncontrolled division of the cells comprising the skin layer. Early diagnosis is crucial for reducing fatality rates. However, accurately identifying different types of tumorous cells poses challenges, leading to potential misdiagnoses by physicians. To aid clinicians in precise cancer identification, this study proposes a comprehensive framework incorporating the latest compact versions of YOLO, including YOLOv3tiny, YOLOv4tiny, YOLOv5s, YOLOv7tiny, and YOLOv8s. The research focuses on detecting and classifying nine types of skin cancer using ISIC datasets: Actinic Keratosis, Basal Cell Carcinoma, Dermatofibroma, Melanoma, Nevus, Pigmented Benign Keratosis, Seborrheic Keratosis, Squamous Cell Carcinoma, and Vascular Lesion. Results indicate that YOLOv5s performs well for certain cancer classes, while YOLOv8s excels in others. To enhance the overall performance, a fusion strategy is employed, integrating predictions from both YOLOv5s and YOLOv8s based on confidence scores. The experiments demonstrate that the overall detection accuracy improves from 91.5 % to 94.3 % in terms of mean average precision (mAP@0.5) and 89.6 % to 97.87 % in terms of precision. To implement an embedded deep skin cancer detection system (ESCDS), the suggested framework utilizes the edge computing device, Nvidia Jetson Nano to assess real-time performance and efficiency of lightweight YOLO detectors. For single-image prediction, the approximate inference time on the edge computing device is 106.5 ms for YOLOv3tiny, 125.6 ms for YOLOv4tiny, 142.5 ms for YOLOv5s, 13.9 ms for YOLOv7tiny, and 35.4 ms for YOLOv8s, respectively. © 2024 Elsevier Ltd","10.1016/j.bspc.2024.107093","Convolutional neural network; Deep learning; Fusion; Image classification; Nvidia Jetson Nano; Skin disease diagnosis; YOLO","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85206945570&doi=10.1016%2fj.bspc.2024.107093&partnerID=40&md5=04450b21c5e96eb4a0b4b4d69e14d089"
"Hypergraph convolutional neural networks for clinical diagnosis of monkeypox infections using skin virological images","Hussain S.; Songhua X.; Aslam M.U.; Waqas M.; Hussain F.","2025","1","0","0","0","0","Unique","0","","","","","","","The Monkeypox virus (Mpoxv), characterized by its distinct vesiculopustular rash, has re-emerged as a significant zoonotic pathogen, posing severe public health risks and potential bioterrorism threats. Although less virulent than smallpox, the persistence of Mpoxv infections necessitates advanced diagnostic tools and proactive mitigation strategies. Dermatological virological imaging is significant for automatic Mpoxv detection and classification, yet its fidelity is often compromised by low-resolution data, mainly in the incipient stages of the infection. Conventional deep learning models mostly struggle to capture higher-order dependencies and complex feature interactions within virological images, leading to suboptimal outcomes. An advanced hybrid hypergraph convolutional neural networks (HGCNs) architecture is introduced in response. In this architecture hypergraph effectively models intricate correlations and enables the detect subtle patterns. At the same time, CNN components contribute robust feature extraction, refined through relational modeling, leads optimal detection and classification of Mpoxv infection. The HGCNs were trained and validated using two different validation approaches, including the Holdout method (HM) and a stratified 3-fold cross-validation (3-FCV), yielding HM accuracy of 0.9888, precision of 0.9813, recall of 0.9958, F1 Score of 0.9885, specificity of 0.9890, Micro AUC of 0.9892, and an average time per epoch of 0.5512 s, while 3-FCV achieved an average accuracy of 0.9917, precision of 0.9931, recall of 0.9912, F1 score of 0.9922, specificity of 0.9941, Micro AUC of 0.9903, and an average time per epoch of 0.6151 s. Furthermore, the use of Grad-CAM facilitates precise localization of infected regions within the images. The performance highlights the proposed model's effectiveness as a powerful tool in computational virology, delivering high accuracy and interpretable diagnostics for Mpoxv infections. Data availability: The dataset is freely available online. © 2024 Elsevier B.V.","10.1016/j.asoc.2024.112673","Clinical infections; Deep learning; Dermatological lesion; Hypergraph Convolutional Neural networks; Machine Learning; Monkeypox","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85213826596&doi=10.1016%2fj.asoc.2024.112673&partnerID=40&md5=c73ebc8d3fba5d06253452b99f0e2e0b"
"Optimized Dynamic Graph-Based Framework for Skin Lesion Classification in Dermoscopic Images","Deepa J.; Madhavan P.","2025","1","1","0","0","0","Unique","0","","","","","","","Early and accurate classification of skin lesions is critical for effective skin cancer diagnosis and treatment. However, the visual similarity of lesions in their early stages often leads to misdiagnoses and delayed interventions. This lack of transparency makes it challenging for dermatologists to interpret with validate decisions made by such methods, reducing their trust in the system. To overcome these complications, Skin Lesions Classification in Dermoscopic Images using Optimized Dynamic Graph Convolutional Recurrent Imputation Network (SLCDI-DGCRIN-RBBMOA) is proposed. The input image is pre-processed utilizing Confidence Partitioning Sampling Filtering (CPSF) to remove noise, resize, and enhance image quality. By using the Hybrid Dual Attention-guided Efficient Transformer and UNet 3+ (HDAETUNet3+) it segment ROI region of the preprocessed dermoscopic images. Finally, segmented images are fed to Dynamic Graph Convolutional Recurrent Imputation Network (DGCRIN) for classifying skin lesion as actinic keratosis, dermatofibroma, basal cell carcinoma, squamous cell carcinoma, benign keratosis, vascular lesion, melanocytic nevus, and melanoma. Generally, DGCRIN does not express any adaption of optimization strategies for determining optimal parameters to exact skin lesion classification. Hence, Red Billed Blue Magpie Optimization Algorithm (RBBMOA) is proposed to enhance DGCRIN that can exactly classify type of skin lesion. The proposed SLCDI-DGCRIN-RBBMOA technique attains 26.36%, 20.69% and 30.29% higher accuracy, 19.12%, 28.32%, and 27.84% higher precision, 12.04%, 13.45% and 22.80% higher recall and 20.47%, 16.34%, and 20.50% higher specificity compared with existing methods such as a deep learning method dependent on explainable artificial intelligence for skin lesion classification (DNN-EAI-SLC), multiclass skin lesion classification utilizing deep learning networks optimal information fusion (MSLC-CNN-OIF), and classification of skin cancer from dermoscopic images utilizing deep neural network architectures (CSC-DI-DCNN) respectively. © (2025), (Science and Information Organization). All rights reserved.","10.14569/IJACSA.2025.01602115","Confidence partitioning sampling filtering; dynamic graph convolutional recurrent imputation network; hybrid dual attention-guided efficient transformer and UNet 3+; ISIC-2019 skin disease dataset; red billed blue magpie optimization algorithm","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-86000313799&doi=10.14569%2fIJACSA.2025.01602115&partnerID=40&md5=6b22e0c7c348f85c91620263a27a4e96"
"Design an Ensemble Pretrained Deep Learning Model for Classification of Melanoma Skin Cancer Images","Albashish D.; Almansour N.; Abdullah A.; Mustafa H.M.J.; Alsayyed M.R.; Alrashdan O.","2025","1","1","0","0","0","Unique","0","","","","","","","The recent global outbreak of melanoma skin cancer has prompted increased attention toward computer-aided diagnosis systems (CAD). The primary application of CAD in melanoma is to discriminate between benign and malignant skin medical images. Additionally, the survival rate improves with early cancer detection. This study presents a CAD diagnosis method for melanoma skin diagnosis, which utilizes deep learning and medical image processing. To elaborate, the proposed model comprises two stages: Firstly, fine-tune three powerful pretrained models, namely VGG16, NASNetMobile, and ResNet50V2, to address the current task. Secondly, we leverage their decisions through ensemble learning and hard voting techniques. The evaluation of the proposed ensemble model on a skin cancer dataset yielded a notable accuracy of 91.5% on a test portion of the dataset. This method reveals the potential for diagnosing melanoma while also illustrating the efficacy of the ensemble model in augmenting diagnostic performance.  © 2025 IEEE.","10.1109/ICCIAA65327.2025.11013331","Classification; Ensemble Pretrained Deep Learning; Melanoma Skin Cancer Image","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009967460&doi=10.1109%2fICCIAA65327.2025.11013331&partnerID=40&md5=294bc2b0e4f64ccb57e7df4ea390c6b8"
"Dual-stage segmentation and classification framework for skin lesion analysis using deep neural network","Manzoor K.; Gilal N.U.; Agus M.; Schneider J.","2025","1","1","0","0","0","Unique","0","","","","","","","ObjectiveSkin diseases, caused by various pathogens including bacteria, viruses, and fungi, are prevalent globally and significantly affect patients’ physical, emotional, and social well-being. Early and accurate detection of such conditions is critical to prevent progression, especially in cases of malignant skin lesions. This study aims to develop a dual-stage deep learning framework for the segmentation and classification of skin lesions, addressing challenges such as imbalanced data, lesion variability, and low contrast.MethodsWe propose a two-phase framework: (i) Precise instance segmentation using U-Net with a Visual Geometry Group (VGG16 encoder) to isolate skin lesions and (ii) classification using EfficientFormer and SwiftFormer networks to evaluate performance on both balanced and imbalanced datasets. Experiments were conducted on three benchmark datasets: Human against machine with 10,000 training images (HAM10000), International Skin Imaging Collaboration (ISIC) 2018, and the newly released ISIC 2024 SLICE-3D dataset. For SLICE-3D, we evaluated both tabular-only and image + metadata fusion approaches using XGBoost classifier and ResNet-based classifier, respectively.ResultsOn the balanced HAM10000 dataset, EfficientFormerV2 achieved 97.11% accuracy, a 97.14% F1-score, 96.85% sensitivity, and 96.70% specificity. On the ISIC 2018 dataset, the segmentation model achieved 97.59% accuracy, 89.12% Jaccard index, and 94.24% Dice similarity coefficient. For the ISIC 2024 SLICE-3D challenge, the tabular-only XGBoost classifier achieved a partial area under the receiver operating characteristic curve score of 0.16752, while the image + tabular fusion model achieved a score of 0.15792 using ResNet, demonstrating competitive performance in a highly imbalanced and clinically realistic setting.ConclusionThe proposed dual-stage deep learning framework demonstrates high accuracy and robustness across segmentation and classification tasks on diverse datasets. Its adaptability to large-scale, non-dermoscopic data such as SLICE-3D confirms its potential for deployment in real-world skin cancer triage and teledermatology applications. © The Author(s) 2025. This article is distributed under the terms of the Creative Commons Attribution 4.0 License (https://creativecommons.org/licenses/by/4.0/) which permits any use, reproduction and distribution of the work without further permission provided the original work is attributed as specified on the SAGE and Open Access page (https://us.sagepub.com/en-us/nam/open-access-at-sage).","10.1177/20552076251351858","deep learning; image augmentation; skin cancer; skin disease classification; Skin lesion segmentation","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012501581&doi=10.1177%2f20552076251351858&partnerID=40&md5=94a8339ba5834a6d40a42826e65fcd4e"
"Melanoma Detection Using Egret Search Golden Optimization Tuned Distributed Pooling-Based Fused Deep BiLSTM Model","Fatima S.; Akther S.","2025","0","1","0","0","0","Unique","0","","","","","","","Melanoma, the most lethal type of skin cancer, remains a formidable obstacle in the quest for early detection. Despite significant strides in the field, numerous enduring challenges obstruct the effectiveness of current models and techniques. A critical disadvantage of current melanoma detection models is their limited sensitivity, particularly in identifying early-stage melanomas. However, there exists the complexity in visual features of skin lesion images due to the nonhomogenous features and fuzzy boundaries that limit their performance. Additionally, the reliability of melanoma detection is degraded by the noise, shadows, and artifacts present in the medical images. This research addresses these challenges and introduces a novel melanoma detection model that ultimately enhances diagnostic accuracy and enables timely intervention. This research aims to develop an Egret Search Golden (ESG) optimization tuned distributed pooling-based BiLSTM model (ESG-distributed pooling-based BiLSTM) model to enhance the detection accuracy of melanoma through an innovative approach. It begins by gathering data and then employs a series of advanced techniques. The adaptive optimized Otsu thresholding process provides more precise, segmenting of the melanoma lesions accurately from the background, and feature extraction encompassing texture patterns, deep features, and hybrid structural features that provide effective melanoma detection. The extracted features utilizing the Gray Level Co-occurrence Matrix are then fed into a sophisticated distributed pooling-based fused deep BiLSTM model, further fine-tuned using the ESG approach, to overcome all the challenges of the early detection of melanoma, a unique optimization method provides high convergence rate, effective handling of high-dimensional problems, robustness, and reduced the time complexity. To extract the features, the encoder and decoder subnetworks are associated in a sequence pathway to bring the semantic features and provide efficiency in melanoma detection in the initial stage. Specifically, for the Melanoma Skin Cancer Dataset, the model yielded impressive TP 90 values, showcasing metrics at 96.13%, 96.06%, and 96.10%, respectively. Moreover, when adopting the k-fold 6 approach, the model’s performance elevated to even higher levels, delivering better measures of 96.40%, 96.27%, and 96.05% for accuracy, sensitivity, and specificity. © 2027 World Scientific Publishing Company.","10.1142/S021946782750046X","distributed pooling-based fused deep BiLSTM; Egret search golden optimization; gray level co-occurrence matrix; optimized Otsu thresholding; RESNET 101","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009416860&doi=10.1142%2fS021946782750046X&partnerID=40&md5=3a55790fb4a441a877319d516f4feba5"
"D2U-Net: a dual-path hybrid UNet architecture for precise medical image segmentation","Ahmed N.; Tan X.; Ma L.","2025","0","1","0","0","0","Unique","0","","","","","","","Many:medical image segmentation tasks heavily depend on U-shaped encoder–decoder networks like as U-Net, which have shown remarkable success. Nevertheless, these designs may have difficulties in effectively capturing inter-dependencies within hierarchical layers, resulting in problematic segmentation of lesions with irregular borders and intricate morphologies. In order to tackle these difficulties, we present D2U-Net, an innovative dual-path Hybrid U-Net architecture specifically implemented to improve feature interactions and optimize the use of multi-scale information. In order to capture both global and local features through deeper hierarchical interactions, the design utilizes dual encoder–decoder paths. Moreover, we present a Contextual-Spectral Fusion Module (CSFM) specifically developed as a comprehensive information fusion and enhancement approach for encoder–decoder approaches. This module facilitates the smooth and extensive sharing of information across multiple stages and paths. The proposed model integrates fine-grained texture details with high-level semantic features to improve segmentation accuracy. The D2U-Net is further enhanced by integrating global context and local edge extraction in a hybrid fashion through two encoder–decoder paths. The precise integration of multi-scale features is ensured by lightweight operations and efficient merging, resulting in the accurate segmentation of complex medical images. We assess the performance of D2U-Net on four intricate medical image segmentation tasks: the segmentation of skin lesions datasets, including the ISIC2018 and PH2 datasets, polyps, and brain tumor datasets. The proposed approach is consistently superior to state-of-the-art approaches, as evidenced by experimental results. This approach provides greater accuracy and generalization across an extensive variety of datasets. Our method achieved an IoU of 90.5% and a Dice coefficient of 95.0% on the ISIC2018 dataset. On the PH2 dataset, it achieved an IoU of 84.0% and a Dice coefficient of 91.2%. On the polyp dataset, it achieved an average IoU of 96.15% and a Dice coefficient of 97.95%. On the brain tumor dataset, it achieved an IoU of 89.4% and a Dice coefficient of 94.3%. It is anticipated that this method could be used in clinical practice in the future owing to its consistent accuracy throughout a wide range of datasets covering various medical conditions. The code is available in the GitHub repository at https://github.com/nooriahmed/D2U-Net-Hybrid-U-Net. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.","10.1007/s00371-025-04044-y","Lesions; Multi-Scale; Polyp; Segmentation; Skin Cancer","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009412558&doi=10.1007%2fs00371-025-04044-y&partnerID=40&md5=e5d45cbeb0e98a7db5cb9b3b8f8b0228"
"A Multi-Location Enhanced Multi-Scale Feature Fusion Skin Lesion Segmentation Network Based on U-shaped Network","Wu R.; Song H.; Zeng X.; Li G.; Zhang J.; He F.","2025","1","1","0","0","0","First occurrence","0","","","","","","","With the continuous development of deep learning, computer-aided diagnosis reduces the pressure of dermatologists. Although many U-Net-based convolution neural networks can achieve effective segmentation, they still need to be improved when faced with challenging skin lesion segmentation tasks such as fuzzy and irregular lesions. To this end, we propose a skin lesion segmentation algorithm (MSMF-Net) based on U-shaped network architecture, which mainly includes the following aspects: the feature distribution of different encoding stages are fused and fully utilised by the EFM module. To better handle high-level semantic features, multiscale and attention parallel module (MS-AM) is introduced at the bottleneck of the network. Attention bridging module (ABM) is used at skip connection, which enables effective local and global information fusion and provides rich feature information for decoding. The decoding fusion module (DFM) processes the decoded information by flexibly embedding channel and spatial attention into the fusion process, so that the information is more concentrated in the lesion area. Our network is compared with other state-of-the-art methods on two public datasets and obtains the best results. © 2025 SPIE.","10.1117/12.3059230","convolution neural network; deep learning; feature fusion; segmentation; skin lesion","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000825493&doi=10.1117%2f12.3059230&partnerID=40&md5=4bd6e02a1a591af69b37f16b8259cb8e"
"Skin cancer detection using optimized mask R-CNN and two-fold-deep-learning-classifier framework","Reddy A.S.; M.P G.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancer is a serious and potentially life-threatening condition caused by DNA damage in the skin cells, leading to genetic mutations and abnormal cell growth. These mutations can cause the cells to divide and grow uncontrollably, forming a tumor on the skin. To prevent skin cancer from spreading and potentially leading to serious complications, it's critical to identify and treat it as early as possible. An innovative two-fold deep learning based skin cancer detection model is presented in this research work. Five main stages make up the proposed model: Preprocessing, segmentation, feature extraction, feature selection, and skin cancer detection. Initially, the Min–max contrast stretching and median filtering used to pre-process the collected raw image. From the pre-processed image, the Region of Intertest (ROI) is identified via optimized mask Region-based Convolutional Neural Network (R-CNN). Then, from the identified ROI areas, the texture features like Illumination-invariant Binary Gabor Pattern (II-BGP), Local Binary Pattern (LBP), Gray-Level Co-occurrence Matrix (GLCM), Color feature such as Color Correlogram and Histogram Intersection, and Shape feature including Moments, Area, Perimeter, Eccentricity, Average bending energy are extracted. To choose the optimal features from the extracted ones, the Golden Eagle Mutated Leader Optimization (GEMLO) is used. The proposed Golden Eagle Mutated Leader Optimization (GEMLO) is the conceptual amalgamation of the standard Mutated Leader Algorithm (MLA) and Golden Eagle Optimizer are used to select best features (GEO). The skin cancer detection is accomplished via two-fold-deep-learning-classifiers, that includes the Fully Convolutional Neural Networks (FCNs) and Multi-Layer Perception (MLP). The final outcome is the combination of the outcomes acquired from Fully Convolutional Neural Networks (FCNs) and Multi-Layer Perception (MLP). The PYTHON platform is being used to implement the suggested model. Using the current models, the findings are assessed for accuracy, sensitivity, precision, FPR, FNR, and other metrics. The proposed model has a 92% detection accuracy rating, which is the highest. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","10.1007/s11042-024-20377-7","Fully convolutional neural network; Golden eagle optimizer; Mask R-CNN; Multi-layer perception; Mutated leader algorithm; Skin cancer detection","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85217989249&doi=10.1007%2fs11042-024-20377-7&partnerID=40&md5=cd45da7a4f011205e1fca3cf54f43f27"
"9th International Skin Imaging Collaboration Workshop, ISIC 2024, 7th International Workshop on Interpretability of Machine Intelligence in Medical Image Computing, iMIMIC 2024, Embodied AI and Robotics for HealTHcare Workshop, EARTH 2024 and 5th MICCAI Workshop on Distributed, Collaborative and Federated Learning, DeCaF 2024 held at 27th International conference on Medical Image Computing and Computer Assisted Intervention, MICCAI 2024","","2025","1","1","0","0","0","Unique","0","","","","","","","The proceedings contain 23 papers. The special focus in this conference is on Skin Imaging Collaboration, Interpretability of Machine Intelligence in Medical Image Computing, Embodied AI and Robotics for HealTHcare Workshop and MICCAI Workshop on Distributed, Collaborative and Federated Learning. The topics include: DeCaF 2024 Preface; i2M2Net: Inter/Intra-modal Feature Masking Self-distillation for Incomplete Multimodal Skin Lesion Diagnosis; from Majority to Minority: A Diffusion-Based Augmentation for Underrepresented Groups in Skin Lesion Analysis; segmentation Style Discovery: Application to Skin Lesion Images; a Vision Transformer with Adaptive Cross-Image and Cross-Resolution Attention; lesion Elevation Prediction from Skin Images Improves Diagnosis; DWARF: Disease-Weighted Network for Attention Map Refinement; PIPNet3D: Interpretable Detection of Alzheimer in MRI Scans; Detecting Unforeseen Data Properties with Diffusion Autoencoder Embeddings Using Spine MRI Data; interpretability of Uncertainty: Exploring Cortical Lesion Segmentation in Multiple Sclerosis; TextCAVs: Debugging Vision Models Using Text; evaluating Visual Explanations of Attention Maps for Transformer-Based Medical Imaging; Exploiting XAI Maps to Improve MS Lesion Segmentation and Detection in MRI; EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian Splatting; VISAGE: Video Synthesis Using Action Graphs for Surgery; a Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery; SurgTrack: CAD-Free 3D Tracking of Real-World Surgical Instruments; MUTUAL: Towards Holistic Sensing and Inference in the Operating Room; Complex-Valued Federated Learning with Differential Privacy and MRI Applications; enhancing Privacy in Federated Learning: Secure Aggregation for Real-World Healthcare Applications; federated Impression for Learning with Distributed Heterogeneous Data; A Federated Learning-Friendly Approach for Parameter-Efficient Fine-Tuning of SAM in 3D Segmentation; probing the Efficacy of Federated Parameter-Efficient Fine-Tuning of Vision Transformers for Medical Image Classification; FedGS: Federated Gradient Scaling for Heterogeneous Medical Image Segmentation.","","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218707937&partnerID=40&md5=74a8b017b112b80a6cfd4e25bc162373"
"Explainable Pre-Trained Models for Skin Cancer Classification","Alrabai A.; Echtioui A.; Kallel F.","2025","1","1","0","0","0","First occurrence","0","","","","","","","Skin cancers represent a significant health issue, posing considerable challenges in early detection and accurate classification. This study investigated the effectiveness of Deep Learning (DL) models for skin cancers classification using dermoscopic images. Two pre-trained models, DenseNet169 and VGG16, were employed. The DenseNet169 model delivered the highest accuracy, reaching 90.61% while, the VGG16 model achieved an accuracy of 84.24%. The two models were assessed using key performance metrics to conduct a comprehensive evaluation of their effectiveness and overall performance. This analysis provided significant insights on various aspects of each model, allowing a detailed comparison. It also provided a clearer comprehension of the strengths and weaknesses of both models, aiding in informed decision-making and driving further optimization. In addition, three explainable AI (XAI) techniques were applied to deliver perceptions into the decision-making process. These techniques enhance model interpretability by emphasizing the regions of the input images that contribute most to the classification decision, thus improving the transparency and reliability of the model's predictions. Reliable skin cancer classification has the potential to assist healthcare specialists, particularly dermatologists, in making early diagnoses, facilitating timely interventions, and ultimately improving patient outcomes. This study offers valuable contributions for enhancing skin cancer detection and classification, supporting accurate diagnosis and treatment development for medical practitioners.  © 2025 IEEE.","10.1109/SSD64182.2025.10989876","Classification; DL; Skin cancer; XAI","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007283186&doi=10.1109%2fSSD64182.2025.10989876&partnerID=40&md5=05eabc33988b75abfaddd8c311c1da09"
"TlED-Net: optimizing semantic segmentation via triple-loop encoder-decoder architecture with dense skip connections","Wei Y.; Wang Y.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Semantic segmentation is crucial for efficient medical image analysis, enhancing diagnostic accuracy and efficiency. However, existing networks often suffer from limited feature interaction, inadequate attention mechanism integration, and isolated deep-level semantic information. To address these issues, we propose TlED-Net, a novel semantic segmentation network based on a triple-loop encoder-decoder architecture (TlEDA) with dense skip connections. TlED-Net employs different representation strategies across its network depth, optimizes different shallow architectures, and introduces innovative modules such as dense atrous spatial pyramid pooling (DASPP) and channel space mixed attention. Experimental results on six medical datasets demonstrate that TlED-Net outperforms 17 state-of-the-art models, achieving Dice coefficients of up to 97.145% on the LUNG dataset and 93.176% on the skin lesion dataset. These findings highlight the effectiveness of TlED-Net in extracting and interpreting complex semantic information, positioning it as a valuable tool for medical image analysis. © The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature 2025.","10.1007/s00371-025-04063-9","Medical image; Multi-loop encoder-decoder architecture; Semantic segmentation","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105009411172&doi=10.1007%2fs00371-025-04063-9&partnerID=40&md5=f3287e323fa83b0073ed3e67f6d89761"
"Computer Vision for Disease Detection - An Overview of How Computer Vision Techniques Can Be Used to Detect Diseases in Medical Images, Such as X-Rays and MRIs","Sharma R.; Kumar N.; Sharma V.","2025","0","1","0","0","0","Unique","0","","","","","","","The utility of computer imaginative and predictive strategies to medical imaging for disease detection represents a big advancement in healthcare diagnostics. This explores a complete methodology that integrates progressive approaches in deep learning, data augmentation, explainable artificial intelligence (AI), real-time processing, and multimodal records fusion. The primary goal is to enhance the accuracy, efficiency, and transparency of diagnostic strategies throughout various clinical situations consisting of Alzheimer’s disease, cardiovascular disorders, and pores and skin conditions. The approach begins with the compilation and preprocessing of diverse scientific datasets, including X-rays, MRIs, CT scans, ECGs, and fundus images. This phase involves close collaboration with medical experts for accurate annotation and the application of advanced preprocessing techniques to ensure high-quality input data. In the final phase, a hybrid deep learning architecture is employed, combining Convolutional Neural Networks (CNNs) for spatial feature extraction and transformers for better capturing long-range dependencies and enhancing the model’s predictive performance.This architecture is in addition reinforced through transfer getting to know, leveraging pre-educated models and best-tuning them on precise medical datasets to enhance performance. Information augmentation strategies and generative hostile networks (GANs) are applied to mitigate records scarcity by creating synthetic scientific pictures, thereby enhancing the version’s robustness. The education section consists of both supervised and semi-supervised studying strategies, with cross-validation to make certain model generalizability. Explainable AI strategies, consisting of Grad-CAM, are included to offer visible insights into the model’s decision-making process, fostering consider and interpretability. Actual-time processing talents are finished through model optimization techniques like pruning and quantization, and deployment on aspect devices to ensure on the spot diagnostic comments. Seamless integration with clinical workflows is prioritized, with the improvement of consumer-friendly interfaces and dashboards that gift diagnostic effects and recommendations comprehensively. A key innovation is the multimodal data integration, combining clinical pictures with EHRs and genetic data. This holistic technique permits for a more comprehensive evaluation and personalized diagnostic insights. Continuous learning frameworks also are carried out, enabling the models to conform and improve with new information and feedback, ensuring they remain current with today’s clinical advancements. The results demonstrate extensive enhancements in diagnostic accuracy and performance, with real-time systems imparting instant and dependable comments. The mixing of explainable AI strategies guarantees transparency and fosters greater acceptance amongst healthcare experts. The future scope of this studies consists of further enhancements in multimodal data integration, using federated learning to ensure records privacy, and the incorporation of augmented and virtual reality for interactive diagnostics. Continuous development and real-world validation via clinical trials might be essential in solidifying the role of AI-pushed diagnostics in healthcare. Thus, this looks at providing a robust and innovative framework for disease detection the use of computer imaginative and predictive, highlighting its potential to transform healthcare diagnostics by means of supplying precise, green, and transparent solutions. As the sphere progresses, those AI-driven equipment are poised to come to be integral in clinical exercise, riding ahead the competencies of precision remedy and personalized care. © 2025 by The Institute of Electrical and Electronics Engineers, Inc. All rights reserved.","10.1002/9781394278695.ch4","computer vision techniques; continuous learning; data augmentation; deep learning; diagnostic accuracy; disease detection; electronic health records; explainable AI; generative adversarial networks; medical imaging; model optimization; multimodal data fusion; personalized diagnostics; precision medicine; real-time processing; semi-supervised learning; supervised learning","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85215974839&doi=10.1002%2f9781394278695.ch4&partnerID=40&md5=50c4ac45a656091afb30d240ac715064"
"Image-based quantification of Scale Loss in Fish using Machine Learning and Computer Vision","Fjorden T.; Høgstedt E.B.; Schellewald C.; Mester R.; Remen M.; Nytrø A.V.; Stahl A.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Ensuring fish welfare is vital for sustainable salmon farming, with skin condition being a key health indicator. Despite stringent regulations, the salmon farming industry is still experiencing high mortality rates, emphasizing the need for innovative solutions to measure and improve fish welfare along with operational efficiency. This paper proposes a system for detecting and quantifying scale loss in fish by leveraging state-of-the-art artificial intelligence methods. The system first detects the fish in an image using a state-of-the-art object detection model. Further, the fish's skin and scale loss are segmented using two instance segmentation models. Finally, the relative skin area of the scale loss is determined, effectively quantifying the amount of scale loss. To achieve this, machine-learning models were trained with a specialized new dataset. In the test data set, the fish detection model achieved a mean average precision (mAP)50-95 of 87.5% with 24.69 frames per second (FPS) on a NVIDIA 1660Ti GPU. The skin segmentation model achieved a mAP50-95 of 98.0% with 24.15 FPS. The scale segmentation model achieved an F1 score of 76.3% with 5.94 FPS, resulting in an estimated scale loss percentage with a mean square error (MSE) of 0.278 on 10 test images. This resulted in a workflow that analyzes four fish per second on a low-budget GPU with high accuracy. Furthermore, threshold values mapping scale loss percentage to a discrete LAKSVEL welfare score was established, achieving a welfare score accuracy of 85.7% over 140 test images. The test data used to evaluate these models originated from a controlled setup above water. Additional tests were performed using real-world underwater images, understandingly showing that the models were not immediately applicable to the new data. However, the models showed a remarkable ability to be fine-tuned with as few as 18 images, to result in F1 scores of 88.2%, 98.9%, and 61.5% for fish detection, skin segmentation, and scale loss segmentation, respectively. A user-friendly application, ScaleGuard, was also developed to make these advanced models accessible to non-technical personnel. © 2025 SPIE.","10.1117/12.3055209","Aquaculture; Artificial Intelligence; Fish Skin; Fish Welfare; instance segmentation; Machine Learning","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000136975&doi=10.1117%2f12.3055209&partnerID=40&md5=1b97f4eaddc6fac4f8d2e1da2bf2b20a"
"A Novel Deep Learning Based Melanoma Detection Technique Using Skin Lesion","Hussain S.A.; Ahmed G.; Siddiqui S.; Malick R.; Akhunzada A.","2025","1","1","0","0","0","First occurrence","0","","","","","","","A 'Melanoma'is a type of skin cancer with the highest fatal rate. It accounts for up to 1.7% of cancer diagnoses globally. Early detection of the disease has many advantages, it significantly reduces the chances of morbidity. The therapies involved are very critical and has an adverse effect on patients mental and physical condition, usually biopsy is advised for detection. The procedure itself is very hard and gives difficult stretch to a patient. In addition, the patient needs to wait for around 3-12 weeks to get results. The procedures used in the earliest form of melanoma detection are believed to be expensive and somewhat inaccurate. In this article, a novel deep learning-based approach is proposed to diagnose the disease in a timely manner. Currently due to less fragmented data the existing solutions has not been adopted by a practical industry. During the exploratory data analysis, it has been found that data is highly imbalance. To mitigate this lack of proportion the calculated weightage for each class has been utilised. In order to oversee the impact of this weightage we have reduced the count of classes which have higher data to not more than 4-folds when comparing to classes containing lower data count. In the combination we have used EfficientNet-B1 for classification along with more hidden and fully connected layers. The proposed models have been evaluated through industry practices. The 86% accuracy was acheived with the process. Cross validation on the model approach has been applied to find unbiased result set of accuracy, precision, recall and F1-score. To be more discrete, 10-folds are used on the proposed deep learning model approach. Previously an accuracy of 81% (Res-Net) and 82% (Dense-Net) were reported for the same models. The dermoscopic images are highly complacent, after shedding some more light it was evident that the inner attributes of the image play a vital role and provide valuable insights to the model. The rotation range, zoom range and rescale is being focused as they sharpen the object edges and help to identify the sheer pointers prominently. The complete data is provided to the models which significantly improves the results. © 2025 IEEE.","10.1109/ICCIT63348.2025.10989443","Cancer; Convolutional Neural Network; Deep learning; DenseNet; EfficientNet; Medical imaging; Melonoma; ResNet; Skin Cancer; Skin Science","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105007547911&doi=10.1109%2fICCIT63348.2025.10989443&partnerID=40&md5=e41101896ee1b554a372d46b477d8a19"
"Skin Cancer Classification Using PCNN-LinkNet Model Trained with Monotonic and Deep Feature Set","Vinay Kumar Y.B.; Vimala H.S.","2025","1","1","0","0","0","Unique","0","","","","","","","Skin cancer is among the most rapidly spreading types of cancer affecting humans. Traditional diagnosis relies on expert dermatologists and specialized equipment, which can be both time-consuming and costly. To reduce diagnostic expenses, deep learning has offered state-of-the-art solutions for early and accurate detection of skin cancer. Nevertheless, analyzing skin lesion images is challenging due to lighting, color, and shape variations. These difficulties make reliable automated recognition of skin cancer essential for enhancing the efficiency of early diagnosis. In response to these challenges, this research introduces a novel approach called customized batch normalization-assisted parallel convolutional neural network for skin cancer classification (CBNPC-SCC), specifically designed to overcome these obstacles and improve diagnostic performance. The proposed approach encompasses four primary stages such as preprocessing, segmentation, feature extraction, and classification phases. Initially, the new pixel estimation-based Wiener filter (NPE-WF) approach is employed for preprocessing the input image, in which the modifications are carried out in the mean, variance, and new pixel computations. This improved version minimizes the noise while enhancing the quality of the image. Subsequently, a channel attention module updated SegNet (CAMSgN) is proposed using the CAM method and is employed for segmenting the preprocessed image, which enhances the capability of isolating the region of interest, enhancing the segmentation of cancer regions. Then, the appropriate features such as local Gabor transitional pattern (LGTrP), multi-texton (MTH) features, threshold-based smoother function in local monotonic pattern (TS-LMP), and deep features are extracted from the segmented outcome. These features effectively capture the transition patterns between pixels and the fine-grained patterns, crucial for differentiating the types of skin lesions. Finally, the obtained features are given to the classification phase, where a hybrid combination of LinkNet and weighted quadratic mean-customized batch normalization-parallel convolutional neural network (WQM-CBN-PCNN) models is proposed for classification. Averaging the outcomes of both classifiers shows the final classification outcomes. Furthermore, the proposed model achieved a higher accuracy of 0.975, F-measure of 0.914, and specificity of 0.986, which surpasses the results of the traditional methods. Overall, the proposed model presents an effective, reliable, and cost-efficient solution for automated skin cancer classification. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","10.1007/s44174-025-00467-2","Deep learning; Feature extraction; Image processing; Segmentation; Skin cancer classification","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105012186071&doi=10.1007%2fs44174-025-00467-2&partnerID=40&md5=9f14ad511f6a72e289b8728d0302c1c9"
"IMAGE PROCESSING FOR DETECTING MELANOMA SKIN CANCER USING AN OPTIMIZED ROTATION-INVARIANT COORDINATE CONVOLUTIONAL NEURAL NETWORK","Perumal Sankar S.; Vinu R.; Sreelekshmi S.; George D.E.","2025","1","1","0","0","0","Unique","0","","","","","","","Cancer is a dangerous disease caused by the unregulated proliferation of bodily cells. Cancer can grow in any part of the human body and may contain billions of cells. Skin cancer is a common type of cancer that originates on the top layer of the skin. Melanoma is one category of skin cancer, this is the most dangerous disease and leads to mortality. Generally, the machine learning approaches are mostly employed to diagnose skin cancer using protein sequences and other forms of imaging. But some disadvantages occur while using the machine learning systems, like requiring human-engineered features, difficulty, and being time-consuming. In this manuscript, Image Processing for detecting Melanoma Skin Cancer using an optimized Rotation-Invariant Coordinate Convolutional Neural Network (IP-MSCD-RICCNN) is proposed for skin cancer detection. The input images are gathered through the International Skin Imaging Collaboration (ISIC) dataset. Then the images are pre-processed using the Gillijn De Moor Filter (GDMF) to remove the noise. The pre-processed image is given to the Generalized Intuitionistic Fuzzy C-Means Clustering (GIFCMC) for segmenting the Region of Interest (ROI) from the skin image. The segmented image is supplied to the Newton Time-Extracting Wavelet Transform (NTEWT) to extract Gray Level Co-occurrence Matrix (GLCM) features such as Contrast, Correlation, Entropy, and Inverse. After feature extraction, the Rotation-Invariant Coordinate Convolutional Neural Network (RICCNN) is used to classify the skin images as malignant melanoma and benign melanoma. RICCNN generally does not adapt any optimization approaches to determine the optimal parameters for exact prediction. Therefore, the Mutated Coati Optimization Algorithm (MCOA) is employed to enhance the RICCNN for skin image classification. The proposed IP-MSCD-RICCNN method is implemented in Python. The proposed method achieves 26.36%, 20.69%, and 35.29% higher accuracy, 32.85%, 19.63%, and 25.47% higher precision, and 16.57%, 21.47%, and 30.56% greater sensitivity when compared to the existing models: skin cancer diagnosis from dermoscopy imageries utilizing deep learning with fuzzy k-means clustering (DI-SCD-DL-FKC), automatic detecting skin cancer by consolidating texture features with convolutional neural networks in dermoscopy images (DI-SCD-CNN-TF), and detection of skin cancer utilizing combined decision of deep learners (SCD-CD-DCNN) respectively.  © 2025 World Scientific Publishing Company.","10.1142/S0219519425500058","Generalized intuitionistic fuzzy C-means clustering; Gillijn De Moor filter; ISIC dataset; mutated coati optimization algorithm; Newton time-extracting wavelet transform; rotation-invariant coordinate convolutional neural network","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002795160&doi=10.1142%2fS0219519425500058&partnerID=40&md5=59c127ce9f382be0821e85ea80de9a84"
"MixFormer: A Mixed CNN-Transformer Backbone for Medical Image Segmentation","Liu J.; Li K.; Huang C.; Dong H.; Song Y.; Li R.","2025","0","1","0","0","0","Unique","0","","","","","","","Transformers using self-attention mechanisms have recently advanced medical imaging by modeling long-range semantic dependencies, though they lack the ability of convolutional neural networks (CNNs) to capture local spatial details. This study introduced a novel segmentation (SEG) network derived from a mixed CNN-Transformer (MixFormer) feature extraction backbone to enhance medical image segmentation. The MixFormer network seamlessly integrates global and local information from Transformer and CNN architectures during the downsampling process. To comprehensively capture the interscale perspective, we introduced a multiscale spatial-aware fusion (MSAF) module, enabling effective interaction between coarse and fine feature representations. In addition, we proposed a mixed multibranch dilated attention (MMDA) module to bridge the semantic gap between encoding and decoding stages while emphasizing specific regions. Finally, we implemented a CNN-based upsampling approach to recover low-level features, substantially improving segmentation accuracy. Experimental validations on prevalent medical image datasets demonstrated the superior performance of MixFormer. On the Synapse dataset, our approach achieved a mean Dice similarity coefficient (DSC) of 82.64% and a mean Hausdorff distance (HD) of 12.67 mm. On the automated cardiac diagnosis challenge (ACDC) dataset, the DSC was 91.01%. On the international skin imaging collaboration (ISIC) 2018 dataset, the model achieved a mean intersection over union (mIoU) of 0.841, an accuracy of 0.958, a precision of 0.910, a recall of 0.934, and an F1 score of 0.913. For the Kvasir-SEG dataset, we recorded a mean Dice of 0.9247, an mIoU of 0.8615, a precision of 0.9181, and a recall of 0.9463. On the computer vision center (CVC)-ClinicDB dataset, the results were a mean Dice of 0.9441, an mIoU of 0.8922, a precision of 0.9437, and a recall of 0.9458. These findings underscore the superior segmentation performance of MixFormer compared to most mainstream segmentation networks such as CNNs and other Transformer-based structures.  © 1963-2012 IEEE.","10.1109/TIM.2024.3497060","Medical image segmentation (SEG); mixed convolutional neural network (CNN)-Transformer backbone; mixed multibranch dilated attention (MMDA); multiscale spatial-aware fusion (MSAF)","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209255807&doi=10.1109%2fTIM.2024.3497060&partnerID=40&md5=dc18907209ddc3a78d3da899dd5607c4"
"Deep Learning-Based Lung Cancer Histopathology Classification: Emphasizing Accuracy and Customisation","Kashyap S.; Shukla A.K.; Naim I.; Pal S.","2025","0","1","0","0","0","First occurrence","0","","","","","","","Lung cancer is a dangerous disease that can be fatal, and a correct diagnosis is essential for figuring out the best way to treat it. The optimum treatment for people with lung cancer requires the classification of the disease into its histological types, such as adenocarcinoma (ADC), small cell lung cancer (SCLC), and squamous cell carcinoma (SCC). Each histological subtype has its features and may react differently to different types of medicine. So, knowing the exact subtype helps guide treatment choices and improve the patient's outcome. Lung cancer subtypes are necessary for personalized treatment. It helps doctors choose tumor-specific treatments such as surgery, radiation, chemotherapy, targeted drugs, and immunotherapies. Precise categorization improves prognosis, avoids needless medicines, and lets patients participate in clinical studies targeting their cancer subtype. Precision medicine improves lung cancer outcomes with accurate categorization. The current algorithms in this domain have shown deficiencies in performance criteria such as specificity, F-score, sensitivity, and precision in recognition. These limitations may stem from challenges such as the complexity and heterogeneity of histopathological images, variations in staining techniques, and the presence of confounding factors. Deep learning methods have made it easier to look at histopathology slides of cancer and see what's going on. Several studies have shown that convolutional neural networks (CNN) are essential for classifying histopathological pictures of different kinds of cancer, like brain, skin, breast, lung, and colon cancer. This study divides lung cancer images into three groups: normal, adenocarcinoma, and squamous cell carcinoma. We have been training deep learning algorithms to identify lung cancer in histopathology slides better, and utilizing deep learning strategies and cutting-edge algorithms such as VGG-19, ResNet-50 v2, EfficientNetB1, and others indicates a comprehensive approach to addressing the problem. Deep learning models, notably CNNs and pre-trained models, possess exceptional performance in computer vision tasks including detecting instances, semantic separation, recognizing objects, and image classification. These models are accurate and effective in pixel-level object segmentation, picture classification, object detection, image synthesis, and image captioning. Non-small cell carcinomas such as ADC, SCC, and SCLC are particularly interesting in this study because of the potential for improved diagnostic accuracy and stability. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2025.","10.1007/s44174-025-00335-z","And images from histopathology; Classification; Deep learning; EfficientNetB1; Lung cancer; ResNet-50 v2; VGG-19","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003433723&doi=10.1007%2fs44174-025-00335-z&partnerID=40&md5=0b5fd2f8132a0253ddc4d45512442832"
"Class-Aware Cartilage Segmentation for Autonomous US-CT Registration in Robotic Intercostal Ultrasound Imaging","Jiang Z.; Kang Y.; Bi Y.; Li X.; Li C.; Navab N.","2025","0","1","0","0","0","Unique","0","","","","","","","Ultrasound imaging has been widely used in clinical examinations owing to the advantages of being portable, real-time, and radiation-free. Considering the potential of extensive deployment of autonomous examination systems in hospitals, robotic US imaging has attracted increased attention. However, due to the inter-patient variations, it is still challenging to have an optimal path for each patient, particularly for thoracic applications with limited acoustic windows, e.g., intercostal liver imaging. To address this problem, a class-aware cartilage bone segmentation network with geometry-constraint post-processing is presented to capture patient-specific rib skeletons. Then, a dense skeleton graph-based non-rigid registration is presented to map the intercostal scanning path from a generic template to individual patients. By explicitly considering the high-acoustic impedance bone structures, the transferred scanning path can be precisely located in the intercostal space, enhancing the visibility of internal organs by reducing the acoustic shadow. To evaluate the proposed approach, the final path mapping performance is validated on five distinct CTs and two volunteer US data, resulting in ten pairs of CT-US combinations. Results demonstrate that the proposed graph-based registration method can robustly and precisely map the path from CT template to individual patients (Euclidean error:2.21pm 1.11mm). Note to Practitioners - The precise mapping of trajectories has been a bottleneck in developing autonomous intercostal intervention within limited acoustic space. Existing methods, based on external features such as the skin surface or passive markers, fail to capture the acoustic properties of local tissues, leading to significant shadowing when ribs are involved. The proposed method begins by utilizing distinctive anatomical features to extract cartilage bones and stiff ribs through a class-aware segmentation network. To ensure the segmentation accuracy of the shape of the anatomy of interest, a VAE-based boundary-constraint post-processing in manifold space is developed. Subsequently, a dense skeleton graph-based registration is developed to explicitly consider the subcutaneous bone structure, allowing for the precise mapping of intercostal paths from generic templates to individual patients. Results from ten randomly paired CT and US datasets show that the proposed method accurately maps the intercostal path from the template to individual patients, significantly improving accuracy and robustness over previous methods. We believe that the proposed method can further pave the way for autonomous robotic US imaging.  © 2024 IEEE.","10.1109/TASE.2024.3411784","intercostal ultrasound scanning; robotic ultrasound; ultrasound segmentation; US bone segmentation","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196719979&doi=10.1109%2fTASE.2024.3411784&partnerID=40&md5=065e84c33b9119844a6c7a9310608dde"
"Classification of Skin Lesion With Features Extraction Using Quantum Chebyshev Polynomials and Autoencoder From Wavelet-Transformed Images","Farhatullah; X. Chen; D. Zeng; J. Xu; R. Nawaz; R. Ullah","2024","1","1","0","0","0","Unique","0","Accepted","Long Short-Term Memory (LSTM) with advanced feature extraction","For efficient classification of skin lesions using an innovative automated system.","ISIC2017 and HAM10000","The model uses preprocessing with wavelet transformations. Features are extracted using Quantum Chebyshev polynomials, refined by an Autoencoder, and then classified using an LSTM.","On HAM10000: Accuracy: 99.58%, Precision: 97.84%, Recall: 97.49%, F1-score: 97.66%. On ISIC2017: Accuracy: 98.87%.","Skin, a vital organ acting as a protective barrier to the external environment, plays a pivotal role in overall human health. Early detection of skin diseases is essential, as untreated conditions can escalate to serious issues such as skin cancer. This study presents an innovative automated system designed for efficient classification of skin lesions, addressing the growing demand for advanced biomedical image analysis. Leveraging the power of Deep Learning, the proposed model incorporates several pre-processing techniques such as wavelet transformations, pooling methods, and normalization to enhance image clarity and remove extraneous artifacts. Two distinct feature extractors are used to extract key features: Quantum Chebyshev polynomials for initial feature extraction, followed by an Autoencoder (AE) for feature refinement and dimensionality reduction. These optimized features are classified using Long Short-Term Memory (LSTM). The experimental evaluation of the proposed model includes analysis with five different optimizers: Adam, RMSprop, SGD, Adadelta, and Adagrad, accross two widely recognized datasets, ISIC2017 and HAM10000. The resutlts reveals that the Adam optimizer consistently yields the highest scores across multiple evaluation matrices. For the ISIC2017 dataset, the model achieves 98.87% accuracy, 98.23% precision, 98.26% recall, F1-score 98.24%, and 98.16% specificity. The HAM10000 dataset exhibits even more remarkable metrics, with 99.58% accuracy, 97.84% precision, 97.49% recall, 97.66% F1-score, and 97.74% specificity. The proposed model surpasses the current state-of-the-art in skin lesion classification and holds the potential to serve as a valuable tool for medical professionals, aiding in the automated classification of skin cancer.","10.1109/ACCESS.2024.3502513","Skin lesion classification;deep learning;medical diagnosis;biomedical image analysis;wavelet transformations;quantum Chebyshev polynomials;autoencoders;feature extraction","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10757392"
"Explainable Deep Learning for Breast Cancer Classification and Localization","Di Giammarco, Marcello and Vitulli, Camilla and Cirnelli, Simone and Masone, Benedetta and Santone, Antonella and Cesarelli, Mario and Martinelli, Fabio and Mercaldo, Francesco","2025","0","1","0","0","0","Unique","0","","","","","","","Breast cancer is a kind of cancer that forms in the cells of the breasts. After skin cancer, breast cancer represents the most common cancer diagnosed in women in the United States. As a matter of fact, in January 2022, there are more than 3.8 million women with a history of breast cancer in the United States, this is the reason why there is a need for novel methods for automatic breast cancer screening, with the aim of starting any therapy as quickly as possible to try to limit the proliferation of the disease. In this article, we propose a method aimed at detecting breast cancer through a deep learning network developed by authors. Moreover, the proposed method is able to provide prediction explainability by means of class activation mapping, aimed to automatically highlight the suspicious area on the image. We take into account a way to understand whether the cancer prediction and localization can be considered robust by analyzing the output of two different class activation mapping algorithms. We evaluate the effectiveness of the proposed method by using a dataset composed of 9,016 images obtaining an accuracy equal to 93.5%, thus showing the effectiveness of the proposed network for breast cancer detection and localization.","10.1145/3702237","","","https://doi.org/10.1145/3702237"
"Nuclear shape descriptors by automated morphometry may distinguish aggressive variants of squamous cell carcinoma from relatively benign skin proliferative lesions: a pilot study","Yang, Weixi; Tian, Rong; Xue, Tongqing","2015","1","1","0","0","0","Unique","0","","","","","","","We evaluated whether degrees of dysplasia may be consistently accessed in an automatic fashion, using different kinds of non-melanoma skin cancer (NMSC) as a validatory model. Namely, we compared Bowen disease, actinic keratosis, basal cell carcinoma, low-grade squamous cell carcinoma, and invasive squamous cell carcinoma. We hypothesized that characterizing the shape of nuclei may be important to consistently diagnose the aggressiveness of a skin tumor. While basal cell carcinoma is comparatively relatively benign, management of squamous cell carcinoma is controversial because of its potential to recur and intraoperative dilemma regarding choice of the margin or the depth for the excision. We provide evidence here that progressive nuclear dysplasia may be automatically estimated through the thresholded images of skin cancer and quantitative parameters estimated to provide a quasi-quantitative data, which can thenceforth guide the management of the particular cancer. For circularity, averaging more than 2500 nuclei in each group estimated the means ± SD as 0.8 ± 0.007 vs. 0.78 ± 0.0063 vs. 0.42 ± 0.014 vs. 0.63 ± 0.02 vs. 0.51 ± 0.02 ( F  = 318063.56, p  < 0.0001, one-way analyses of variance). The mean aspect ratios were (means ± SD) 0.97 ± 0.0014 vs. 0.95 ± 0.002 vs. 0.38 ± 0.018 vs. 0.84 ± 0.0035 vs. 0.74 ± 0.019 ( F  = 1022631.931, p  < 0.0001, one-way analyses of variance). The Feret diameters averaged over 2500 nuclei in each group were the following: 1 ± 0.0001 vs. 0.9 ± 0.002 vs. 5 ± 0.031 vs. 1.5 ± 0.01 vs. 1.9 ± 0.004 ( F  = 33105614.194, p  < 0.0001, one-way analyses of variance). Multivariate analyses of composite parameters potentially detect aggressive variants of squamous cell carcinoma as the most dysplastic form, in comparison to locally occurring squamous cell carcinoma and basal cell carcinoma, or benign skin lesions.","10.1007/s13277-015-3294-5","Nuclear shape; Aspect ratio; Circularity; Skin cancer; Metastasis","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s13277-015-3294-5"
"Skin Microstructure is a Key Contributor to Its Friction Behaviour","Leyva-Mendivil, Maria F.; Lengiewicz, Jakub; Page, Anton; Bressloff, Neil W.; Limbert, Georges","2016","1","1","0","0","0","Unique","0","","","","","","","Due to its multifactorial nature, skin friction remains a multiphysics and multiscale phenomenon poorly understood despite its relevance for many biomedical and engineering applications (from superficial pressure ulcers, through shaving and cosmetics, to automotive safety and sports equipment). For example, it is unclear whether, and in which measure, the skin microscopic surface topography, internal microstructure and associated nonlinear mechanics can condition and modulate skin friction. This study addressed this question through the development of a parametric finite element contact homogenisation procedure which was used to study and quantify the effect of the skin microstructure on the macroscopic skin frictional response. An anatomically realistic two-dimensional image-based multilayer finite element model of human skin was used to simulate the sliding of rigid indenters of various sizes over the skin surface. A corresponding structurally idealised multilayer skin model was also built for comparison purposes. Microscopic friction specified at skin asperity or microrelief level was an input to the finite element computations. From the contact reaction force measured at the sliding indenter, a homogenised (or apparent) macroscopic friction was calculated. Results demonstrated that the naturally complex geometry of the skin microstructure and surface topography alone can play as significant role in modulating the deformation component of macroscopic friction and can significantly increase it. This effect is further amplified as the ground-state Young’s modulus of the stratum corneum is increased (for example, as a result of a dryer environment). In these conditions, the skin microstructure is a dominant factor in the deformation component of macroscopic friction, regardless of indenter size or specified local friction properties. When the skin is assumed to be an assembly of nominally flat layers, the resulting global coefficient of friction is reduced with respect to the local one. This seemingly counter-intuitive effect had already been demonstrated in a recent computational study found in the literature. Results also suggest that care should be taken when assigning a coefficient of friction in computer simulations, as it might not reflect the conditions of microscopic and macroscopic friction one intends to represent. The modelling methodology and simulation tools developed in this study go beyond what current analytical models of skin friction can offer: the ability to accommodate arbitrary kinematics (i.e. finite deformations), nonlinear constitutive properties and the complex geometry of the skin microstructural constituents. It was demonstrated how this approach offered a new level of mechanistic insight into plausible friction mechanisms associated with purely structural effects operating at the microscopic scale; the methodology should be viewed as complementary to physical experimental protocols characterising skin friction as it may facilitate the interpretation of observations and measurements and/or could also assist in the design of new experimental quantitative assays.","10.1007/s11249-016-0794-4","Skin; Friction mechanisms; Contact mechanics; Microstructure; Finite element; Image-based modelling; Material properties","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s11249-016-0794-4"
"Künstliche Intelligenz und Smartphone-Programm-Applikationen (Apps)","Blum, A.; Bosch, S.; Haenssle, H. A.; Fink, C.; Hofmann-Wellenhof, R.; Zalaudek, I.; Kittler, H.; Tschandl, P.","2020","0","1","0","0","0","Unique","0","","","","","","","Vorteile der künstlichen Intelligenz (KI) Durch einen verantwortungsvollen, sicheren und erfolgreichen Einsatz der künstlichen Intelligenz (KI) im dermato-onkologischen Bereich können mögliche Vorteile entstehen: (1) die ärztlich-medizinische Arbeit kann sich auf Hautkrebspatienten fokussieren, (2) Patienten können rascher und effizienter versorgt werden bei zunehmender Hautkrebsinzidenz und parallel abnehmenden Zahlen beruflich aktiver Hautarzte, und (3) Anwender können von den KI-Ergebnissen lernen. Potenzielle Nachteile und Gefahren des KI-Einsatzes (1) Ein mangelndes Vertrauensverhältnis bei fehlendem Patienten-Arzt-Kontakt kann sich entwickeln, (2) ein zusätzlicher zeitlicher Aufwand kann durch die zeitnahe ärztliche Kontrolle von der KI als benigne eingestuften Hautläsionen entstehen, (3) ausreichende ärztliche Erfahrungen zum Erkennen und korrigieren fehlerhafter KI-Entscheidungen können fehlen, und (4) bei fehlerhafter KI-Entscheidung ist eine erneute Kontaktaufnahme zum Patienten zur zeitnahen Vorstellung notwendig. Ungeklärt sind bisher bei der KI-Anwendung die medizinisch-rechtliche Situation sowie die finanzielle Vergütung. Apps mit KI erbringen auf Basis von klinischen Bildern von Hauttumoren aktuell keine ausreichende diagnostische Hilfe. Voraussetzungen und möglicher Nutzen von Smartphone-Programm-Applikationen Smartphone-Programm-Applikationen (Apps) können verantwortungsvoll erfolgreich eingesetzt werden, wenn die Bildqualität gut ist, anamnestische Angaben unkompliziert eingegeben werden können, die Bild- und Befundübermittlung gesichert ist und medizinrechtliche sowie finanzielle Fragen geklärt sind. Apps können als krankheitsspezifisches Informationsmaterial eingesetzt werden und in der Teledermatologie die Patientenversorgung optimieren. Advantages of artificial intelligence (AI) With responsible, safe and successful use of artificial intelligence (AI), possible advantages in the field of dermato-oncology include the following: (1) medical work can focus on skin cancer patients, (2) patients can be more quickly and effectively treated despite the increasing incidence of skin cancer and the decreasing number of actively working dermatologists and (3) users can learn from the AI results. Potential disadvantages and risks of AI use (1) Lack of mutual trust can develop due to the decreased patient–physician contact, (2) additional time effort will be necessary to promptly evaluate the AI-classified benign lesions, (3) lack of adequate medical experience to recognize misclassified AI decisions and (4) recontacting a patient in due time in the case of incorrect AI classifications. Still problematic in the use of AI are the medicolegal situation and remuneration. Apps using AI currently cannot provide sufficient assistance based on clinical images of skin cancer. Requirements and possible use of smartphone program applications Smartphone program applications (apps) can be implemented responsibly when the image quality is good, the patient’s history can be entered easily, transmission of the image and results are assured and medicolegal aspects as well as remuneration are clarified. Apps can be used for disease-specific information material and can optimize patient care by using teledermatology.","10.1007/s00105-020-04658-4","Vorteile; Nachteile; Patientenversorgung; Bildqualität ; Teledermatologie ; Advantages; Disadvantages; Patient care; Image quality; Teledermatology","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s00105-020-04658-4"
"Computerassistierte Hautkrebsdiagnose","Brinker, T. J.; Schlager, G.; French, L. E.; Jutzi, T.; Kittler, H.","2020","0","1","0","0","0","Unique","0","","","","","","","Background Artificial intelligence (AI) is increasingly being used in medical practice. Especially in the image-based diagnosis of skin cancer, AI shows great potential. However, there is a significant discrepancy between expectations and true relevance of AI in current dermatological practice. Objectives This article summarizes promising study results of skin cancer diagnosis by computer-based diagnostic systems and discusses their significance for daily practice. We hereby focus on the analysis of dermoscopic images of pigmented and unpigmented skin lesions. Materials and methods A selective literature search for recent relevant trials was conducted. The included studies used machine learning, and in particular “convolutional neural networks”, which have been shown to be particularly effective for the classification of image data. Results and conclusions In numerous studies, computer algorithms were able to detect pigmented and nonpigmented neoplasms of the skin with high precision, comparable to that of dermatologists. The combination of the physician’s assessment and AI showed the best results. Computer-based diagnostic systems are widely accepted among patients and physicians. However, they are still not applicable in daily practice, since computer-based diagnostic systems have only been tested in an experimental environment. In addition, many digital diagnostic criteria that help AI to classify skin lesions remain unclear. This lack of transparency still needs to be addressed. Moreover, clinical studies on the use of AI-based assistance systems are needed in order to prove its applicability in daily dermatologic practice. Hintergrund Die künstliche Intelligenz (KI) hält zunehmend Einzug im Gesundheitswesen. Auffallend ist jedoch eine gewisse Diskrepanz zwischen den hohen, zum Teil unrealistischen Erwartungen besonders im Bereich der bildbasierten Hautkrebsdiagnostik und dem tatsächlichen Stellenwert in der Praxis. Ziel der Arbeit (Fragestellung) In diesem Beitrag werden Ergebnisse relevanter Studien zum Einsatz computerbasierter Assistenzsysteme in der Hautkrebsdiagnose vorgestellt und diskutiert mit Fokus auf der Auswertung von dermatoskopischen Bildern pigmentierter sowie nicht pigmentierter Läsionen. Material und Methoden Die Studienauswahl basiert auf einer selektiven Literaturrecherche relevanter Studien der vergangenen Jahre zum Einsatz von maschinellem Lernen, insbesondere sog. „convolutional neural networks“, die sich als besonders effektiv für die Klassifizierung von Bilddaten erwiesen haben. Ergebnisse und Diskussion In zahlreichen Studien konnte gezeigt werden, dass Computeralgorithmen anhand eines digitalen Bildes pigmentierte und nicht pigmentierte Neoplasien der Haut mit vergleichbarer Präzision wie Dermatologen erkennen. Die besten Ergebnisse wurden erzielt, wenn KI mit der ärztlichen Einschätzung einer suspekten Läsion kombiniert wurde. Insgesamt stoßen computergestützte Assistenzsysteme auf breite Akzeptanz sowohl unter Patienten als auch Ärzten. Doch sind die vielversprechenden Ergebnisse noch nicht auf den Klinik- oder Praxisalltag übertragbar, da sie unter experimentellen Bedingungen erzielt wurden, zudem ist eine lückenlose Transparenz der KI kaum erreichbar. Klinische Studien zum flächendeckenden Einsatz KI-basierter Assistenzsysteme sind unabdingbar, um den Nutzen und die grundsätzliche Anwendbarkeit computerassistierter Diagnostik in der Praxis zu belegen.","10.1007/s00105-020-04662-8","Maschinelles Lernen; Dermatoskopische Bilder; Computerbasierte Assistenzsysteme ; Suspekte Läsionen; Computeralgorithmen; Machine learning; Dermoscopy; Computer-assisted diagnosis; Suspicious lesions; Computer algorithms","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s00105-020-04662-8"
"Digitalisierte Bildverarbeitung: künstliche Intelligenz im diagnostischen Einsatz","Winkler, J. K.; Sies, K.; Fink, C.; Toberer, F.; Enk, A.; Haenssle, H. A.","2020","0","1","0","0","0","Unique","0","","","","","","","Digital image processing has made much progress during recent years. First systems for automated image analysis were based on multistage machine learning algorithms and required extensive preprocessing and segmentation using handcrafted filters. Meanwhile, current systems are based on deep-learning convolutional neuronal networks (CNNs). Such artificial intelligence (AI) based systems are increasingly being established for use in clinical medicine. Dermatology is especially suitable for digital image analysis since human skin can easily be accessed. Due to continuously increasing incidence rates the diagnosis of skin cancer is of special interest in dermatology. Dermoscopic images of pigmented and non-pigmented skin lesions have been collected in large publicly available databases and may be used for training of automated diagnostic systems. Previous clinical studies have shown that CNNs are capable of attaining a dermatologist level diagnostic accuracy in the detection of skin cancer. A first CNN has recently been approved as a medical device for the European market (Moleanalyzer pro®, FotoFinder Systems GmbH). Moreover, deep-learning CNNs have also been trained and tested in the field of dermatohistopathology. Despite the euphoria about the high-level diagnostic performance, it is crucial to carefully evaluate the limitations of AI-based diagnostic systems. Image artefacts, such as color markings and superimposed scales that are included in the image may severely impair the diagnostic accuracy. Investigating such limitations is relevant to determine the indications and contraindications and to further improve the performance of AI-based diagnostic systems. Importantly, AI-based diagnostic devices are intended as assistance systems and the results should be carefully integrated into the decision-making progress by clinicians, who remain fully responsible for the management decisions. Die digitalisierte Bildverarbeitung hat in den vergangenen Jahren große Fortschritte erzielt. Erste Systeme zur automatisierten Bildanalyse basierten auf mehrstufigen Algorithmen des Maschinenlernens. Diese wurden inzwischen zu selbstlernenden neuronalen Netzwerken weiterentwickelt. Solche auf künstlicher Intelligenz (KI) basierenden Systeme werden zunehmend auch in der Medizin eingesetzt. Die Dermatologie bietet sich durch die einfache Zugänglichkeit der Hautoberfläche für eine KI-basierte Diagnosestellung durch Bildanalyse an. Innerhalb der Dermatologie nimmt die Hautkrebsdiagnostik einen zentralen Stellenwert ein. Dermatoskopische Aufnahmen von pigmentierten und nichtpigmentierten Hautveränderungen sind in großen öffentlichen Datenbanken verfügbar und können so zum Training automatisierter Diagnosesysteme eingesetzt werden. In Studien zur Hautkrebsdiagnostik erzielten verschiedene KI-basierte Systeme eine diagnostische Genauigkeit, welche mit der von erfahrenen Dermatologen vergleichbar war. Ein erstes neuronales Netzwerk zur Hautkrebsdiagnostik anhand von dermatoskopischen Bildern ist zwischenzeitlich für den europäischen Markt zugelassen worden (Moleanalyzer pro®, FotoFinder Systems GmbH). Auch in der Dermatohistopathologie wurden KI-basierte Diagnosesysteme entwickelt und in Studien getestet. Trotz aller Euphorie über die hohe Leistungsfähigkeit ist es jedoch von essenzieller Bedeutung, auch Limitierungen KI-basierter Diagnosesysteme aufzuzeigen. So beeinflussen Bildartefakte wie farbliche Markierungen oder eingeblendete Lineale innerhalb dermatoskopischer Aufnahmen die Bildbeurteilung durch neuronale Netzwerke. Genaue Kenntnisse über die Limitierungen der neuen Technologie sind entscheidend, um Indikationen und Kontraindikationen von KI-basierten Diagnosesystemen zu bestimmen und die Leistungsfähigkeit weiter zu verbessern. Auch wenn neuronale Netzwerke bereits als Assistenzsysteme für die Hautkrebserkennung eingesetzt werden, liegen der verantwortungsvolle Umgang mit den Ergebnissen und die abschließende Therapieentscheidung weiterhin beim klinisch tätigen Arzt.","10.1007/s12312-019-00729-3","Dermatologie; Automatisierte Bildanalyse; Deep Learning; Neuronales Netzwerk; Hautkrebs; Dermatology; Automated image analysis; Deep learning; Neuronal network; Skin neoplasms","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s12312-019-00729-3"
"Handheld versus mounted laser speckle contrast perfusion imaging demonstrated in psoriasis lesions","Chizari, Ata; Schaap, Mirjam J.; Knop, Tom; Boink, Yoeri E.; Seyger, Marieke M. B.; Steenbergen, Wiendelt","2021","0","1","0","0","0","Unique","0","","","","","","","Enabling handheld perfusion imaging would drastically improve the feasibility of perfusion imaging in clinical practice. Therefore, we examine the performance of handheld laser speckle contrast imaging (LSCI) measurements compared to mounted measurements, demonstrated in psoriatic skin. A pipeline is introduced to process, analyze and compare data of 11 measurement pairs (mounted-handheld LSCI modes) operated on 5 patients and various skin locations. The on-surface speeds (i.e. speed of light beam movements on the surface) are quantified employing mean separation (MS) segmentation and enhanced correlation coefficient maximization (ECC). The average on-surface speeds are found to be 8.5 times greater in handheld mode compared to mounted mode. Frame alignment sharpens temporally averaged perfusion maps, especially in the handheld case. The results show that after proper post-processing, the handheld measurements are in agreement with the corresponding mounted measurements on a visual basis. The absolute movement-induced difference between mounted-handheld pairs after the background correction is $$16.4\pm 9.3~\%$$ 16.4 ± 9.3 % (mean ± std, $$n=11$$ n = 11 ), with an absolute median difference of $$23.8\%$$ 23.8 % . Realization of handheld LSCI facilitates measurements on a wide range of skin areas bringing more convenience for both patients and medical staff.","10.1038/s41598-021-96218-6","","","https://www.nature.com/articles/s41598-021-96218-6.pdf"
"Efficacy and safety of neuromuscular electrical stimulation in the prevention of pressure injuries in critically ill patients: a randomized controlled trial","Baron, Miriam Viviane; Silva, Paulo Eugênio; Koepp, Janine; Urbanetto, Janete de Souza; Santamaria, Andres Felipe Mantilla; Santos, Michele Paula; Mello Pinto, Marcus Vinicius; Brandenburg, Cristine; Reinheimer, Isabel Cristina; Carvalho, Sonia; Wagner, Mário Bernardes; Miliou, Thomas; Poli-de-Figueiredo, Carlos Eduardo; Pinheiro da Costa, Bartira Ercília","2022","0","1","0","0","0","Unique","0","","","","","","","Background Pressure injuries (PIs), especially in the sacral region are frequent, costly, and increase morbidity and mortality of patients in an intensive care unit (ICU). These injuries can occur as a result of prolonged pressure and/or shear forces. Neuromuscular electrical stimulation (NMES) can increase muscle mass and improve local circulation, potentially reducing the incidence of PI. Methods We performed a randomized controlled trial to assess the efficacy and safety of NMES in preventing PI in critically ill patients. We included patients with a period of less than 48 h in the ICU, aged ≥ 18 years. Participants were randomly selected (1:1 ratio) to receive NMES and usual care (NMES group) or only usual care (control group—CG) until discharge, death, or onset of a PI. To assess the effectiveness of NMES, we calculated the relative risk (RR) and number needed to treat (NNT). We assessed the muscle thickness of the gluteus maximus by ultrasonography. To assess safety, we analyzed the effects of NMES on vital signs and checked for the presence of skin burns in the stimulated areas. Clinical outcomes were assessed by time on mechanical ventilation, ICU mortality rate, and length of stay in the ICU. Results We enrolled 149 participants, 76 in the NMES group. PIs were present in 26 (35.6%) patients in the CG and 4 (5.3%) in the NMES group ( p  ˂ 0.001). The NMES group had an RR = 0.15 (95% CI 0.05–0.40) to develop a PI, NNT = 3.3 (95% CI 2.3–5.9). Moreover, the NMES group presented a shorter length of stay in the ICU: Δ = − 1.8 ± 1.2 days, p = 0.04. There was no significant difference in gluteus maximus thickness between groups (CG: Δ = − 0.37 ± 1.2 cm vs. NMES group: Δ = 0 ± 0.98 cm, p  = 0.33). NMES did not promote deleterious changes in vital signs and we did not detect skin burns. Conclusions NMES is an effective and safe therapy for the prevention of PI in critically ill patients and may reduce length of stay in the ICU. Trial registration RBR-8nt9m4. Registered prospectively on July 20th, 2018, https://ensaiosclinicos.gov.br/rg/RBR-8nt9m4","10.1186/s13613-022-01029-1","Controlled clinical trial; Decubitus ulcer; Electrical stimulation; Electrical stimulation therapy; Intensive care units; Neuromuscular electrical stimulation; Pressure ulcer; Preventive therapy","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s13613-022-01029-1"
"Optical scan and 3D printing guided radiation therapy – an application and provincial experience in cutaneous nasal carcinoma","Cheng, Jui Chih; Dubey, Arbind; Beck, James; Sasaki, David; Leylek, Ahmet; Rathod, Shrinivas","2022","0","1","0","0","0","Unique","0","","","","","","","Background Single field Orthovoltage radiation is an acceptable modality used for the treatment of nasal cutaneous cancer. However, this technique has dosimetric pitfalls and unnecessary excessive exposure of radiation to organs at risk (OAR). We present the clinical outcome of a case series of cutaneous nasal tumours using a novel technique incorporating an optical scanner and a 3-dimensional (3D) printer to deliver treatments using parallel opposed (POP) fields. Materials and methods The POP delivery method was validated using ion chamber and phantom measurements before implementation. A retrospective chart review of 26 patients treated with this technique between 2015 and 2019 was conducted. Patients’ demographics and treatment outcomes were gathered and tabulated. These patients first underwent an optical scan of their faces to collect topographical data. The data were then transcribed into 3D printing algorithms, and positive impressions of the faces were printed. Custom nose block bolus was made with wax encased in an acrylic shell; 4 cm thick using the printed face models. Custom lead shielding was also generated. Treatments were delivered using 250 KeV photons POP arrangement with 4 cm diameter circle applicator cone and prescribed to the midplane. Dose and fractionation were as per physician discretion. Results Phantom measurements at mid-plane were found to match the prescribed dose within ±0.5%. For the 26 cases in this review, the median age was 78.5 years, with 15 females and 11 males. 85% of cases had Basal cell carcinoma (BCC); 1 had squamous cell carcinoma (SCC), one had synchronous BCC + SCC, and 1 had Merkel cell carcinoma. Twenty-one cases had T1N0 disease, 4 had T2N0, and 1 had T3N0. Dose and fractionation delivered were 40Gy in 10 fractions for the majority of cases. The complete response rate at a median follow-up of 6 months was 88%; 1 patient had a refractory tumour, and one patient had a recurrence. Toxicities were minor with 81% with no reported side effects. Three patients experienced grade 3 skin toxicity. Conclusions Utilization of optic scanner and 3D printing technology, with the innovative approach of using POP orthovoltage beams, allows an effective and efficient way of treatment carcinomas of the nose with a high control rate and low toxicity profiles.","10.1186/s41205-022-00136-w","Optical scan; 3D printing; Skin cancers","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s41205-022-00136-w"
"Spatial proteomics identifies JAKi as treatment for a lethal skin disease","Nordmann, Thierry M.; Anderton, Holly; Hasegawa, Akito; Schweizer, Lisa; Zhang, Peng; Stadler, Pia-Charlotte; Sinha, Ankit; Metousis, Andreas; Rosenberger, Florian A.; Zwiebel, Maximilian; Satoh, Takashi K.; Anzengruber, Florian; Strauss, Maximilian T.; Tanzer, Maria C.; Saito, Yuki; Gong, Ting; Thielert, Marvin; Kimura, Haruna; Silke, Natasha; Rodriguez, Edwin H.; Restivo, Gaetana; Nguyen, Hong Ha; Gross, Annette; Feldmeyer, Laurence; Joerg, Lukas; Levesque, Mitchell P.; Murray, Peter J.; Ingen-Housz-Oro, Saskia; Mund, Andreas; Abe, Riichiro; Silke, John; Ji, Chao; French, Lars E.; Mann, Matthias","2024","1","1","0","0","0","Unique","0","","","","","","","Toxic epidermal necrolysis (TEN) is a fatal drug-induced skin reaction triggered by common medications and is an emerging public health issue^ 1 – 3 . Patients with TEN undergo severe and sudden epidermal detachment caused by keratinocyte cell death. Although molecular mechanisms that drive keratinocyte cell death have been proposed, the main drivers remain unknown, and there is no effective therapy for TEN^ 4 – 6 . Here, to systematically map molecular changes that are associated with TEN and identify potential druggable targets, we utilized deep visual proteomics, which provides single-cell-based, cell-type-resolution proteomics^ 7 , 8 . We analysed formalin-fixed, paraffin-embedded archived skin tissue biopsies of three types of cutaneous drug reactions with varying severity and quantified more than 5,000 proteins in keratinocytes and skin-infiltrating immune cells. This revealed a marked enrichment of type I and type II interferon signatures in the immune cell and keratinocyte compartment of patients with TEN, as well as phosphorylated STAT1 activation. Targeted inhibition with the pan-JAK inhibitor tofacitinib in vitro reduced keratinocyte-directed cytotoxicity. In vivo oral administration of tofacitinib, baricitinib or the JAK1-specific inhibitors abrocitinib or upadacitinib ameliorated clinical and histological disease severity in two distinct mouse models of TEN. Crucially, treatment with JAK inhibitors (JAKi) was safe and associated with rapid cutaneous re-epithelialization and recovery in seven patients with TEN. This study uncovers the JAK/STAT and interferon signalling pathways as key pathogenic drivers of TEN and demonstrates the potential of targeted JAKi as a curative therapy. Cell-type-resolved spatial proteomics of the skin from patients with toxic epidermal necrolysis reveals that it is driven by JAK/STAT signaling, leading to successful treatment of this potentially fatal condition in patients using JAK inhibitors.","10.1038/s41586-024-08061-0","","","https://www.nature.com/articles/s41586-024-08061-0.pdf"
"Evaluation of artificial intelligence-powered screening for sexually transmitted infections-related skin lesions using clinical images and metadata","Soe, Nyi N.; Yu, Zhen; Latt, Phyu M.; Lee, David; Ong, Jason J.; Ge, Zongyuan; Fairley, Christopher K.; Zhang, Lei","2024","1","1","0","0","0","Unique","0","","","","","","","Background Sexually transmitted infections (STIs) pose a significant global public health challenge. Early diagnosis and treatment reduce STI transmission, but rely on recognising symptoms and care-seeking behaviour of the individual. Digital health software that distinguishes STI skin conditions could improve health-seeking behaviour. We developed and evaluated a deep learning model to differentiate STIs from non-STIs based on clinical images and symptoms. Methods We used 4913 clinical images of genital lesions and metadata from the Melbourne Sexual Health Centre collected during 2010–2023. We developed two binary classification models to distinguish STIs from non-STIs: (1) a convolutional neural network (CNN) using images only and (2) an integrated model combining both CNN and fully connected neural network (FCN) using images and metadata. We evaluated the model performance by the area under the ROC curve (AUC) and assessed metadata contributions to the Image-only model. Results Our study included 1583 STI and 3330 non-STI images. Common STI diagnoses were syphilis (34.6%), genital warts (24.5%) and herpes (19.4%), while most non-STIs (80.3%) were conditions such as dermatitis, lichen sclerosis and balanitis. In both STI and non-STI groups, the most frequently observed groups were 25–34 years (48.6% and 38.2%, respectively) and heterosexual males (60.3% and 45.9%, respectively). The Image-only model showed a reasonable performance with an AUC of 0.859 (SD 0.013). The Image + Metadata model achieved a significantly higher AUC of 0.893 (SD 0.018) compared to the Image-only model ( p  < 0.01). Out of 21 metadata, the integration of demographic and dermatological metadata led to the most significant improvement in model performance, increasing AUC by 6.7% compared to the baseline Image-only model. Conclusions The Image + Metadata model outperformed the Image-only model in distinguishing STIs from other skin conditions. Using it as a screening tool in a clinical setting may require further development and evaluation with larger datasets.","10.1186/s12916-024-03512-x","A; r; t; i; f; i; c; i; a; l;  ; i; n; t; e; l; l; i; g; e; n; c; e; ,;  ; S; e; x; u; a; l; l; y;  ; t; r; a; n; s; m; i; t; t; e; d;  ; i; n; f; e; c; t; i; o; n; s; ,;  ; C; l; i; n; i; c; a; l;  ; i; m; a; g; e;  ; c; l; a; s; s; i; f; i; c; a; t; i; o; n","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s12916-024-03512-x"
"Explainable AI Case Studies in Healthcare","Vakulabharanam, Vijaya Kumar; Mandhula, Trupthi; Kothapalli, Swathi","2024","0","1","0","0","0","Unique","0","","","","","","","This book chapter explores the concept of explainable artificial intelligence (Explainable AI) in healthcare, emphasizing its importance in readdressing the obstacles presented by black box AI models. The introduction highlights the adaptation of AI in healthcare is on the rise and the need for transparency and interpretability. The overview of Explainable AI provides insights into the methods and approaches employed to achieve explainability in AI models. The demonstrated importance of Explainable AI in healthcare highlights its potential to enhance patient outcomes and provide valuable support in clinical decision-making and ensure compliance with ethical and regulatory requirements. The methodology and approach for implementing Explainable AI in healthcare settings, including data collection and preprocessing, are discussed. Various Explainable AI techniques and models used in healthcare, their strengths, limitations, and interpretability levels are examined. Two case studies on retinopathy, skin cancer, and ICU mortality prediction diagnosis showcase the practical application of Explainable AI, illustrating how it enhances diagnostic accuracy, provides transparent insights into AI predictions, and fosters collaboration between clinicians and AI systems. In conclusion, Explainable AI plays a pivotal role in healthcare by ensuring transparency, interpretability, and trust in AI models, promoting responsible adoption, and improving patient care outcomes.","10.1007/978-981-97-3705-5_12","Explainable artificial intelligence; Transparency; Retinopathy; Skin cancer; ICU mortality prediction","","http://link.springer.com/openurl/pdf?id=doi:10.1007/978-981-97-3705-5_12"
"Design a System to Automatically Detect Common Skin Diseases Using Deep Learning and Web Application","Nguyen, Phuc Hoang; Ngo, Hoan Thanh; Ngo, Lua Thi","2024","1","1","0","0","0","Unique","0","","","","","","","Skin and subcutaneous conditions affect nearly 1.9 billion people at any given time and are the fourth leading cause of the burden of non-fatal disease worldwide. Accurate diagnosis of skin diseases is also a challenge for dermatologists because several skin diseases have similar morphological features, extensive areas of inflammation, the complexity of skin color, and disease color change in hairy areas. Furthermore, the shortage and unequal distribution of qualified dermatologists significantly influences the diagnostic outcome. Based on a deep learning platform, this study developed an automated system to classify four common inflammatory skin diseases with similar morphological and color features including eczema, psoriasis, actinic keratosis, and severe acne. The study used 5433 images for four diseases taken from the SD-260 and Dermnet datasets. The deep learning model developed in this study was a customized ResNet152 model. The deep learning model developed in this study gave better than 89% accuracy results. Finally, we built a simple web application using Flask-Ngrok that automatically classified skin diseases with the developed built-in model. This study shows that deep learning has great potential for the accurate classification of skin diseases and strongly develops teledermatology to aid in disease diagnosis. Besides, it also provides a simple way to build web apps on Google Colaboratory, which anyone can access for free to build and run AI models independent of computer hardware.","10.1007/978-3-031-44630-6_41","Deep Learning; Skin Diseases Detection; SD-260 Dataset; Dermnet Dataset; Web Application","","http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-031-44630-6_41"
"A comprehensive analysis of recent advancements in cancer detection using machine learning and deep learning models for improved diagnostics","Rai, Hari Mohan; Yoo, Joon","2023","0","1","0","0","0","Unique","0","","","","","","","Purpose There are millions of people who lose their life due to several types of fatal diseases. Cancer is one of the most fatal diseases which may be due to obesity, alcohol consumption, infections, ultraviolet radiation, smoking, and unhealthy lifestyles. Cancer is abnormal and uncontrolled tissue growth inside the body which may be spread to other body parts other than where it has originated. Hence it is very much required to diagnose the cancer at an early stage to provide correct and timely treatment. Also, manual diagnosis and diagnostic error may cause of the death of many patients hence much research are going on for the automatic and accurate detection of cancer at early stage. Methods In this paper, we have done the comparative analysis of the diagnosis and recent advancement for the detection of various cancer types using traditional machine learning (ML) and deep learning (DL) models. In this study, we have included four types of cancers, brain, lung, skin, and breast and their detection using ML and DL techniques. In extensive review we have included a total of 130 pieces of literature among which 56 are of ML-based and 74 are from DL-based cancer detection techniques. Only the peer reviewed research papers published in the recent 5-year span (2018–2023) have been included for the analysis based on the parameters, year of publication, feature utilized, best model, dataset/images utilized, and best accuracy. We have reviewed ML and DL-based techniques for cancer detection separately and included accuracy as the performance evaluation metrics to maintain the homogeneity while verifying the classifier efficiency. Results Among all the reviewed literatures, DL techniques achieved the highest accuracy of 100%, while ML techniques achieved 99.89%. The lowest accuracy achieved using DL and ML approaches were 70% and 75.48%, respectively. The difference in accuracy between the highest and lowest performing models is about 28.8% for skin cancer detection. In addition, the key findings, and challenges for each type of cancer detection using ML and DL techniques have been presented. The comparative analysis between the best performing and worst performing models, along with overall key findings and challenges, has been provided for future research purposes. Although the analysis is based on accuracy as the performance metric and various parameters, the results demonstrate a significant scope for improvement in classification efficiency. Conclusion The paper concludes that both ML and DL techniques hold promise in the early detection of various cancer types. However, the study identifies specific challenges that need to be addressed for the widespread implementation of these techniques in clinical settings. The presented results offer valuable guidance for future research in cancer detection, emphasizing the need for continued advancements in ML and DL-based approaches to improve diagnostic accuracy and ultimately save more lives.","10.1007/s00432-023-05216-w","Cancer detection; Machine learning; Deep learning; Feature extraction; State-of-art analysis; Brain tumor; Lung cancer; Skin cancer; Breast cancer; GLCM; CNN; SVM","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s00432-023-05216-w"
"Monkeypox detection using deep neural networks","Sorayaie Azar, Amir; Naemi, Amin; Babaei Rikan, Samin; Bagherzadeh Mohasefi, Jamshid; Pirnejad, Habibollah; Wiil, Uffe Kock","2023","0","1","0","0","0","Unique","0","","","","","","","Background In May 2022, the World Health Organization (WHO) European Region announced an atypical Monkeypox epidemic in response to reports of numerous cases in some member countries unrelated to those where the illness is endemic. This issue has raised concerns about the widespread nature of this disease around the world. The experience with Coronavirus Disease 2019 (COVID-19) has increased awareness about pandemics among researchers and health authorities. Methods Deep Neural Networks (DNNs) have shown promising performance in detecting COVID-19 and predicting its outcomes. As a result, researchers have begun applying similar methods to detect Monkeypox disease. In this study, we utilize a dataset comprising skin images of three diseases: Monkeypox, Chickenpox, Measles, and Normal cases. We develop seven DNN models to identify Monkeypox from these images. Two scenarios of including two classes and four classes are implemented. Results The results show that our proposed DenseNet201-based architecture has the best performance, with Accuracy = 97.63%, F1-Score = 90.51%, and Area Under Curve (AUC) = 94.27% in two-class scenario; and Accuracy = 95.18%, F1-Score = 89.61%, AUC = 92.06% for four-class scenario. Comparing our study with previous studies with similar scenarios, shows that our proposed model demonstrates superior performance, particularly in terms of the F1-Score metric. For the sake of transparency and explainability, Local Interpretable Model-Agnostic Explanations (LIME) and Gradient-weighted Class Activation Mapping (Grad-Cam) were developed to interpret the results. These techniques aim to provide insights into the decision-making process, thereby increasing the trust of clinicians. Conclusion The DenseNet201 model outperforms the other models in terms of the confusion metrics, regardless of the scenario. One significant accomplishment of this study is the utilization of LIME and Grad-Cam to identify the affected areas and assess their significance in diagnosing diseases based on skin images. By incorporating these techniques, we enhance our understanding of the infected regions and their relevance in distinguishing Monkeypox from other similar diseases. Our proposed model can serve as a valuable auxiliary tool for diagnosing Monkeypox and distinguishing it from other related conditions.","10.1186/s12879-023-08408-4","Monkeypox; Epidemic; Artificial Intelligence; Deep learning; Explainable Artificial Intelligence; LIME; Grad-cam","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s12879-023-08408-4"
"Exploring the potential of artificial intelligence in improving skin lesion diagnosis in primary care","Escalé-Besa, Anna; Yélamos, Oriol; Vidal-Alaball, Josep; Fuster-Casanovas, Aïna; Miró Catalina, Queralt; Börve, Alexander; Ander-Egg Aguilar, Ricardo; Fustà-Novell, Xavier; Cubiró, Xavier; Rafat, Mireia Esquius; López-Sanchez, Cristina; Marin-Gomez, Francesc X.","2023","1","1","0","0","0","Unique","0","","","","","","","Dermatological conditions are a relevant health problem. Machine learning (ML) models are increasingly being applied to dermatology as a diagnostic decision support tool using image analysis, especially for skin cancer detection and disease classification. The objective of this study was to perform a prospective validation of an image analysis ML model, which is capable of screening 44 skin diseases, comparing its diagnostic accuracy with that of General Practitioners (GPs) and teledermatology (TD) dermatologists in a real-life setting. Prospective, diagnostic accuracy study including 100 consecutive patients with a skin problem who visited a participating GP in central Catalonia, Spain, between June 2021 and October 2021. The skin issue was first assessed by the GPs. Then an anonymised skin disease picture was taken and uploaded to the ML application, which returned a list with the Top-5 possible diagnosis in order of probability. The same image was then sent to a dermatologist via TD for diagnosis, as per clinical practice. The GPs Top-3, ML model’s Top-5 and dermatologist’s Top-3 assessments were compared to calculate the accuracy, sensitivity, specificity and diagnostic accuracy of the ML models. The overall Top-1 accuracy of the ML model (39%) was lower than that of GPs (64%) and dermatologists (72%). When the analysis was limited to the diagnoses on which the algorithm had been explicitly trained (n = 82), the balanced Top-1 accuracy of the ML model increased (48%) and in the Top-3 (75%) was comparable to the GPs Top-3 accuracy (76%). The Top-5 accuracy of the ML model (89%) was comparable to the dermatologist Top-3 accuracy (90%). For the different diseases, the sensitivity of the model (Top-3 87% and Top-5 96%) is higher than that of the clinicians (Top-3 GPs 76% and Top-3 dermatologists 84%) only in the benign tumour pathology group, being on the other hand the most prevalent category (n = 53). About the satisfaction of professionals, 92% of the GPs considered it as a useful diagnostic support tool (DST) for the differential diagnosis and in 60% of the cases as an aid in the final diagnosis of the skin lesion. The overall diagnostic accuracy of the model in this study, under real-life conditions, is lower than that of both GPs and dermatologists. This result aligns with the findings of few existing prospective studies conducted under real-life conditions. The outcomes emphasize the significance of involving clinicians in the training of the model and the capability of ML models to assist GPs, particularly in differential diagnosis. Nevertheless, external testing in real-life conditions is crucial for data validation and regulation of these AI diagnostic models before they can be used in primary care.","10.1038/s41598-023-31340-1","","","https://www.nature.com/articles/s41598-023-31340-1.pdf"
"Design strategy of green intelligent building using deep belief network","Yu, Ting; Yang, Xiao; Sang, Peidong","2023","0","1","0","0","0","Unique","0","","","","","","","The purpose is to study and discuss the design of green intelligent buildings based on biophysical design concepts under the background of the Internet of Things (IoT). Firstly, the biomimetic concept is applied to the design and exploration of children's medical building space, and the biophilia color design method is proposed for children's medical building space and green building lighting system based on the IoT. Meanwhile, the influencing factors of biophilia skin color are studied on children's psychological stress relief. The children's medical building designed with Deep Belief Network in the Deep Learning field can effectively detect human motion in various areas, and the lighting system can be automatically activated by passers-by. Then, Questionnaire Survey method is used to understand the practicability and preference of users and verify the effect of biophilia color design. Consequently, an intelligent dimming lighting system is designed by ZigBee technology. The experiment results indicate that the illuminance error is small for the proposed medical green intelligent building lighting system based on the IoT. Therefore, the implantation of biophilia color in children's s medical building space can promote children’s physical and mental health recovery and have a positive impact on people's s emotions, thereby achieving the role of environmental therapy. Moreover, the proposed medical green intelligent building lighting system based on the IoT can detect the position illuminance of children's s medical buildings and realize intelligent dimming.","10.1007/s13198-021-01513-0","Internet of Things; Biophilia; Green intelligent building; Deep belief network; ZigBee technology","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s13198-021-01513-0"
"Design and Analysis of CNN-Based Skin Disease Detection System with Preliminary Diagnosis","Reddy, T. Vasudeva; Reddy, R. Anirudh; Prasanna, K. Sai; Teja, C. S. Bhanu; Reddy, N. Sai Chara n; Rao, N. Hima Chandra Sekhar","2023","1","1","0","0","0","Unique","0","","","","","","","Over the past few decades, the occurrence of skin diseases has increased, putting a significant strain on healthcare systems worldwide. These skin diseases can be cancerous (e.g., basal and squamous cell carcinoma, melanoma) and non- cancerous (e.g., acne, vitiligo and eczema). Skin problems can be detrimental to physical health and can cause psychological problems, usually in patients whose face is disfigured or damaged due to skin problems. These dermatologic disorders worsen the situation as time progresses, but the survival rates are high if detected and diagnosed early. This article provides a comprehensive overview of the methods used to classify and detect skin disorders as well as diagnostic methods using naturopathic methods. This paper likewise briefs about the openly available image pre-processing mechanisms and classification algorithms based on the relevant works performed by researchers across the world, and suggests the most suitable technique for each process involved in the skin disease system with appropriate results.","10.2991/978-94-6463-252-1_37","CNN architecture; Python; Skin disease; Deep learning; Image processing; Naturopathy","","http://link.springer.com/openurl/pdf?id=doi:10.2991/978-94-6463-252-1_37"
"Computational Tools for Drug Discovery of Anticancer Therapy","Saikia, Surovi; V., Vijaya Padma; Prajapati, Bhupendra G.; Prajapati, Jigna; Parihar, Akshay; Malviya, Rishabha","2023","0","1","0","0","0","Unique","0","","","","","","","The use of computers Computer in cancer Cancer diagnosis Diagnosis is increasing continually powered by new modalities of imaging and advanced image processing methods. The use of bioinformatics, chemoinformatics, mathematical models and artificial intelligence Artificial Intelligence is some of the critical fields but not limited to employing computers Computer for drug design. Medical imagining remains to be notable factor in the management of cancer Cancer in patient which is currently at the rise with the clinicians due to an increase in cancer Cancer incidences. This increase availability of data with the medical practitioners has the potential for using computers Computer or algorithms Algorithm for cancer Cancer diagnosis Diagnosis for elevation of patient outcomes, reduction in toxicity and lowering the clinical burdens. Cancer Cancer data used for diagnosis Diagnosis and treatment will always be on the rise due to the advanced treatment options such as adaptive radiotherapy (RT) planning and image mining of quantitative nature. Machine learning Machine Learning particularly is used to build statistical models based on the past data sets collected having the self-learning knowledge from collected data sets for making future predictions. It has been shown that a trained deep neural network is able to detect malignant cancer Cancer lesions Lesions form skin lesions Lesions photographs giving a rivalry to well-trained dermatologists. Mammographic Mammography lesions Lesions can also be detected by trained deep neural networks producing rivals to certified radiologist which shows the immense accuracy and profitable use of computer Computer -aided methods in cancer Cancer diagnosis Diagnosis . Designing new anti- cancer Cancer therapeutic or helping in the process of developing such therapies to decrease the failure rate and approval time is the most exciting potential of CAD in cancer Cancer such as autoencoders. Precise clinical and preclinical, which lead increased likelihood for clinical success, may be achieved by using computational methods to predict correct mechanism of action. Effective combinations of drugs, which remain to be a combinatorial, can also be solved using AI-based methods as the number of anti- cancer Cancer drugs is increasing day by day. An increase in the growth of complex experimental data sets has lead researchers today to welcome sophisticated algorithmic and computational tools to analyse the results and study basic biological questions including oncology Oncology . Recently, some of the prominent applications of CAD in cancer Cancer diagnosis Diagnosis include tumour detection through medical image analysis, histopathological characterization and quantifications Quantification , clinical diagnosis Diagnosis through computers Computer , selection of treatments, planning treatment and using multimodal clinical data, anti- cancer Cancer drug development and surveillance of cancer Cancer in populations. These steps collectively help to achieve precision oncology Oncology which may increase the efficiency of early screening and improve treatment levels. In this chapter, computer Computer methods employed in most of the critical fields of oncology Oncology such as radiation oncology Oncology , diagnosis Diagnosis and detection of cancer Cancer and optimization of cancer Cancer treatment are discussed.","10.1007/978-981-19-9786-0_25","Cancer; Diagnosis; Metabolism; Computer; Machine learning; Artificial intelligence","","http://link.springer.com/openurl/pdf?id=doi:10.1007/978-981-19-9786-0_25"
"Explainable deep learning approaches for high precision early melanoma detection using dermoscopic images","Mahmud, Md Abdullah All; Afrin, Sadia; Mridha, M. F.; Alfarhood, Sultan; Che, Dunren; Safran, Mejdl","2025","0","1","0","0","0","Unique","0","","","","","","","Detecting skin melanoma in the early stage using dermoscopic images presents a complex challenge due to the inherent variability in images. Utilizing dermatology datasets, the study aimed to develop Automated Diagnostic Systems for early skin cancer detection. Existing methods often struggle with diverse skin types, cancer stages, and imaging conditions, highlighting a critical gap in reliability and explainability. The novel approach proposed through this research addresses this gap by utilizing a proposed model with advanced layers, including Global Average Pooling, Batch Normalization, Dropout, and dense layers with ReLU and Swish activations to improve model performance. The proposed model achieved accuracies of 95.23% and 96.48% for the two different datasets, demonstrating its robustness, reliability, and strong performance across other performance metrics. Explainable AI techniques such as Gradient-weighted Class Activation Mapping and Saliency Maps offered insights into the model’s decision- making process. These advancements enhance skin cancer diagnostics, provide medical experts with resources for early detection, improve clinical outcomes, and increase acceptance of Deep Learning-based diagnostics in healthcare.","10.1038/s41598-025-09938-4","Dermoscopic images; Deep learning; Early-stage melanoma detection; Explainable AI; Swish activation","","https://www.nature.com/articles/s41598-025-09938-4.pdf"
"JuryFusionNet: a Condorcet’s jury theorem-based CNN ensemble for enhanced monkeypox detection from skin lesion images","Asif, Sohaib; Hadi, Fazal","2025","1","1","0","0","0","Unique","0","","","","","","","Accurate and timely diagnosis of monkeypox (Mpox) is vital for outbreak control and effective treatment. However, its skin lesions often resemble those of chickenpox and measles, making visual diagnosis challenging, even for experts. This issue is worsened by the limited availability of large, labeled multiclass datasets, reducing the effectiveness of deep learning models in real-world scenarios. Moreover, single CNN models often suffer from overfitting and bias, leading to suboptimal performance. While combining multiple models can enhance results, simply increasing the voter pool does not ensure higher accuracy. Including multiple models in the voter pool does not always improve accuracy, as traditional ensembles like stacking and majority voting depend on meta-learners or equal weighting, leading to instability and higher computational cost when classifier performance varies. To address these challenges, our study introduces JuryFusionNet, an innovative approach leveraging Condorcet's jury theorem (CDJT) to fuse CNN models. This enhances accuracy while reducing computational complexity by eliminating the need for a meta-learner. First, we leverage transfer learning (TL) and employ four base models, namely DenseNet169, DenseNet201, MobileNet, and ResNet50V2, to extract deep features from skin lesion images. To enhance the performance of these models, we integrate squeeze and excitation (SE) block, which dynamically recalibrate the feature maps to emphasize informative channels and improve model convergence. Furthermore, we explore the power of ensemble learning by applying CDJT, which leverages the collective decision-making of multiple models to improve prediction accuracy through majority voting. Leveraging CDJT, we established that the inclusion of a model in the voter pool enhances the probability of an accurate majority vote, particularly when the model surpasses the accuracy of the other models. We validate our model's effectiveness using two Mpox datasets. The MSID dataset consists of 770 images divided into four classes: chickenpox, measles, Mpox, and normal skin, while the MSLD dataset includes 3192 samples classified as Mpox or non-Mpox. We use Grad-CAM analysis to visualize each base model's attention, confirming the SE block's role in accurately highlighting infected regions. The experimental results demonstrate remarkable accuracy rates of 91.55% on the MSID dataset and 100% on the MSLD dataset, outperforming various pre-trained models, traditional ensemble methods, and existing approaches. Specifically, JuryFusionNet surpasses standard CNN models by 1.94–5.19% and improves upon prior ensemble methods by 0.65–2.59%, underscoring its superior effectiveness in Mpox detection.","10.1007/s13755-025-00355-5","Mpox detection; Attention block; Majority voting; Condorcet's jury theorem; Deep learning","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1007/s13755-025-00355-5"
"Auto-Segmentation via deep-learning approaches for the assessment of flap volume after reconstructive surgery or radiotherapy in head and neck cancer","Thariat, Juliette; Mesbah, Zacharia; Chahir, Youssef; Beddok, Arnaud; Blache, Alice; Bourhis, Jean; Fatallah, Abir; Hatt, Mathieu; Modzelewski, Romain","2025","0","1","0","0","0","Unique","0","","","","","","","Reconstructive flap surgery aims to restore the substance and function losses associated with tumor resection. Automatic flap segmentation could allow quantification of flap volume and correlations with functional outcomes after surgery or post-operative RT (poRT). Flaps being ectopic tissues of various components (fat, skin, fascia, muscle, bone) of various volume, shape and texture, the anatomical modifications, inflammation and edema of the postoperative bed make the segmentation task challenging. We built a artificial intelligence-enabled automatic soft-tissue flap segmentation method from CT scans of Head and Neck Cancer (HNC) patients. Ground-truth flap segmentation masks were delineated by two experts on postoperative CT scans of 148 HNC patients undergoing poRT. All CTs and flaps (free or pedicled, soft tissue only or bone) were kept, including those with artefacts, to ensure generalizability. A deep-learning nnUNetv2 framework was built using Hounsfield Units (HU) windowing to mimic radiological assessment. A transformer-based 2D “Segment Anything Model” (MedSAM) was also built and fine-tuned to medical CTs. Models were compared with the Dice Similarity Coefficient (DSC) and Hausdorff Distance 95th percentile (HD95) metrics. Flaps were in the oral cavity ( N  = 102), oropharynx ( N  = 26) or larynx/hypopharynx ( N  = 20). There were free flaps ( N  = 137), pedicled flaps ( N  = 11), of soft tissue flap-only ( N  = 92), reconstructed bone ( N  = 42), or bone resected without reconstruction ( N  = 40). The nnUNet-windowing model outperformed the nnUNetv2 and MedSam models. It achieved mean DSCs of 0.69 and HD95 of 25.6 mm using 5-fold cross-validation. Segmentation performed better in the absence of artifacts, and rare situations such as pedicled flaps, laryngeal primaries and resected bone without bone reconstruction ( p  < 0.01). Automatic flap segmentation demonstrates clinical performances that allow to quantify spontaneous and radiation-induced volume shrinkage of flaps. Free flaps achieved excellent performances; rare situations will be addressed by fine-tuning the network.","10.1038/s41598-025-08073-4","Head and neck neoplasms; Surgery; Flap; Volume; Neural networks (Computer)","","https://www.nature.com/articles/s41598-025-08073-4.pdf"
"Enhancing basal cell carcinoma classification in preoperative biopsies via transfer learning with weakly supervised graph transformers","Björkman, Johan; Lagerroth, Sigrid; Siarov, Jan; Yacob, Filmon; Neittaanmäki, Noora","2025","0","1","0","0","0","Unique","0","","","","","","","Background Basal cell carcinoma (BCC) is the most common skin cancer, placing a significant burden on healthcare systems globally. Developing high-precision automated diagnostics requires large annotated datasets, which are costly and difficult to obtain. This study aimed to fine-tune a weakly supervised machine learning model to classify BCC in preoperative punch biopsies using transfer learning. By addressing challenges of scalability and variability, this approach seeks to enhance generalizability and diagnostic accuracy. Methods The Basal Cell Classification (BCCC) dataset included 514 WSIs of punch biopsies (261 with BCC and 253 tumor-free slides), divided into training (70%), validation (15%), and test sets (15%). WSIs were split into patches, and features were extracted using a pretrained simCLR model trained on 1,435 WSIs from BCC excisions. Features were formed into graphs for spatial information and the processed by a Vision Transformer. Testing included finetuned and non-finetuned pre-trained models as well as a model trained from the scratch, evaluated on 78 WSIs from the BCCC dataset. The COBRA dataset of 3,588 WSIs (1,794 with BCC and 1,794 without) was used for external validation. Models classified no-tumor vs. tumor (two classes), no-tumor vs. low-risk vs. high-risk tumors (three classes), and no-tumor vs. four BCC subtypes (five classes). Results The fine-tuned model significantly outperformed the non-fine-tuned pretrained model and the model trained from the scratch with accuracies of 91.7%, 82.1%, and 75.3% and with AUCs of 0.98, 0.95–0.98, and 0.91–0.97 for two, three, and five-class classification. On the external validation, accuracies were 84.9% and 70.5%, with AUCs of 0.92 and 0.89–0.91 for two and three-class classification, respectively. The ablation study revealed that the fine-tuned model outperformed the model trained from scratch, improving mean accuracy by 10.6%, 11.7%, and 13.1% on the BCCC dataset, as well as by 29.6% and 19.2% on the COBRA dataset. Conclusions The results suggest that transfer learning not only enhances model performance on small datasets but also supports robust feature extraction in complex histopathology tasks. These findings reinforce the utility of pre-trained models in computational pathology, where access to large, labeled datasets is often limited, and task-specific challenges require nuanced understanding of the visual data.","10.1186/s12880-025-01710-4","Basal cell carcinoma; Digital pathology; Deep learning; Weakly supervised; Graph transformer; Graph convolutional network; Transfer learning","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s12880-025-01710-4"
"Neural network analysis as a novel skin outcome in a trial of belumosudil in patients with systemic sclerosis","Gunes, Ilayda; Bernstein, Elana J.; Cowper, Shawn E.; Panse, Gauri; Pradhan, Niki; Camacho, Lucy Duran; Page, Nicolas; Bundschuh, Elizabeth; Williams, Alyssa; Carns, Mary; Aren, Kathleen; Fantus, Sarah; Volkmann, Elizabeth R.; Bukiri, Heather; Correia, Chase; Kolachalama, Vijaya B.; Wilson, F. Perry; Mawe, Seamus; Mahoney, J. Matthew; Hinchcliff, Monique","2025","1","1","0","0","0","Unique","0","","","","","","","Background The modified Rodnan skin score (mRSS), a measure of systemic sclerosis (SSc) skin thickness, is agnostic to inflammation and vasculopathy. Previously, we demonstrated the potential of neural network-based digital pathology applied to SSc skin biopsies as a quantitative outcome. Here, we leverage deep learning and histologic analyses of clinical trial biopsies to decipher SSc skin features ‘seen’ by artificial intelligence (AI). Methods Adults with diffuse cutaneous SSc ≤ 6 years were enrolled in an open-label trial of belumosudil [a Rho-associated coiled-coil containing protein kinase 2 (ROCK2) inhibitor]. Participants underwent serial mRSS and arm biopsies at week (W) 0, 24 and 52. Two blinded dermatopathologists scored stained sections (e.g., Masson’s trichrome, hematoxylin and eosin, CD3, α-smooth muscle actin) for 16 published SSc dermal pathological parameters. We applied our deep learning model to generate QIF signatures/biopsy and obtain ‘Fibrosis Scores’. Associations between Fibrosis Score and mRSS (Spearman correlation), and between Fibrosis Score and mRSS versus histologic parameters [odds ratios (OR)], were determined. Results Only ten patients were enrolled due to early study termination, and of those, five had available biopsies due to fixation issues. Median, interquartile range (IQR) for mRSS change (0–52 W) for the ten participants was -2 (-9—7.5) and for the five with biopsies was -2.5 (-11—7.5). The correlation between Fibrosis Score and mRSS was R = 0.3; p  = 0.674. Per 1-unit mRSS change (0–52 W), histologic parameters with the greatest associated changes were (OR, 95% CI, p -value): telangiectasia (2.01, [(1.31—3.07], 0.001), perivascular CD3 + (0.99, [0.97—1.02], 0.015), and % of CD8 + among CD3 + (0.95, [0.89—1.01], 0.031). Likewise, per 1-unit Fibrosis Score change, parameters with greatest changes were (OR, p -value): hyalinized collagen (1.1, [1.04 – 1.16], < 0.001), subcutaneous (SC) fat loss (1.47, [1.19—1.81], < 0.001), thickened intima (1.21, [1.06—1.38], 0.005), and eccrine entrapment (1.14, [1—1.31], 0.046). Conclusions Belumosudil was associated with non-clinically meaningful mRSS improvement. The histologic features that significantly correlated with Fibrosis Score changes ( e.g., hyalinized collagen, SC fat loss) were distinct from those associated with mRSS changes ( e.g., telangiectasia and perivascular CD3 +). These data suggest that AI applied to SSc biopsies may be useful for quantifying pathologic features of SSc beyond skin thickness.","10.1186/s13075-025-03508-9","Systemic sclerosis; Scleroderma; Modified Rodnan skin score; Deep Neural Network; AlexNet; Belumosudil; Outcome measure; Outcomes; Artificial intelligence; Dermal fibrosis; Skin fibrosis","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s13075-025-03508-9"
"Computational histology reveals that concomitant application of insect repellent with sunscreen impairs UV protection in an ex vivo human skin model","Charrasse, Sophie; Poquillon, Titouan; Saint-Omer, Charlotte; Schunemann, Audrey; Weill, Mylène; Racine, Victor; Aouacheria, Abdel","2025","1","1","0","0","0","Unique","0","","","","","","","Background Histological alterations such as nuclear abnormalities are sensitive biomarkers associated with diseases, tissue injury and environmental insults. While visual inspection and human interpretation of histology images are useful for initial characterization, such low-throughput procedures suffer from inherent limitations in terms of reliability, objectivity and reproducibility. Artificial intelligence and digital morphometry offer unprecedented opportunities to quickly and accurately assess nuclear morphotypes in relation to tissue damage including skin injury. Methods In this work, we designed NoxiScore, a pipeline providing an integrated, deep learning-based software solution for fully automated and quantitative analysis of nucleus-related features in histological sections of human skin biopsies. We used this pipeline to evaluate the efficacy and safety of three dermato-cosmetic products massively sold at the time of the study in the Montpellier area (South of France): a sunscreen containing UV filters, a mosquito repellent (with synthetic active ingredient IR3535) and a product combining a natural insect repellent plus a sunscreen. Hematoxylin and eosin or hematoxylin-eosin saffron staining was performed to assess skin structure before morphometric parameter computation. Results We report the identification of a specific nuclear feature based on variation in texture information that can be used to assess skin tissue damage after oxidative stress or UV exposure. Our data show that application of the commercial sun cream provided efficient protection against UV effects in our ex vivo skin model, whereas application of the mosquito repellent as a single product exerted no protective or toxic effect. Notably, we found that concurrent application of the insect repellent with the sunscreen significantly decreased the UVB protective effect of the sunscreen. Last, histometric analysis of human skin biopsies from multiple donors indicates that the sunscreen-insect repellent combo displayed variable levels of protection against UV irradiation. Conclusions To our knowledge, our study is the first to evaluate the potential toxicity of combining real-life sunscreen and insect repellent products using ex vivo human skin samples, which most closely imitate the cutaneous physiology. The NoxiScore wet-plus-dry methodology has the potential to provide information about the pharmaco-toxicological profile of topically applied formulations and may also be useful for diagnostic purposes and evaluation of the skin exposome including pesticide exposure, air pollution and water contaminants. Graphical Abstract","10.1186/s13071-025-06712-3","Insect repellent; Organelle biology; Image analysis; Sunscreen; Morphometry; Histology; Exposome; Toxicology","","https://www.biomedcentral.com/openurl/pdf?id=doi:10.1186/s13071-025-06712-3"
"Extracorporeal photopheresis induces the release of anti-inflammatory fatty acids and oxylipins and suppresses pro-inflammatory sphingosine-1-phosphate","Hagn, Gerhard; Cho, Ara; Zila, Nina; Sterniczky, Barbara; Jantschitsch, Christian; Dong, Dexin; Bileck, Andrea; Koren, Mariia; Paulitschke, Philipp; Mohr, Thomas; Knobler, Robert; Weninger, Wolfgang Peter; Gerner, Christopher; Paulitschke, Verena","2025","0","1","0","0","0","Unique","0","","","","","","","Aims Extracorporeal photopheresis (ECP) is a UVA-based phototherapy of whole blood and well established as a first line or combination therapy for the treatment of cutaneous T-cell lymphoma, systemic sclerosis, graft-versus-host disease and is used to control organ transplant rejection. While the proapoptotic activity on activated T-cells is evident, the clinical efficacy of this treatment also appears to be based on other yet unknown mechanisms. In this study, we aimed to identify novel mechanisms of ECP regardless of the patient’s background situation. Main methods To better understand the immediate consequences of ECP, we analyzed blood plasma of patients with different ECP indications immediately before and after treatment with regard to proteins and lipid mediators. Key findings While proteome profiling identified substantial inter-individual differences in the protein composition, no significant alteration was detectable upon treatment. In contrast, several fatty acids and lipid mediators were found to be significantly altered by ECP. Remarkably, upregulated lipid mediators including polyunsaturated fatty acids, 12-HEPE and 13-OxoODE have been described to be anti-inflammatory, while the downregulated molecules sphingosine-1-phosphate (S1P) and stearic acid are potent pro-inflammatory mediators. A selective sphingosine-1-phosphate-1 receptor (S1P1) modulator AUY954, which decreases S1P1 and experimentally reduces transplant rejection in vivo, showed greater anti-proliferative activity in human lung fibroblasts from COPD patients compared to normal lung fibroblasts, confirming that this pathway may be important in ECP and its mode of action. Significance and outlook In conclusion, we suggest that the ECP-induced changes in lipid mediators may contribute to the remarkable anti-inflammatory effects of the treatment. Depending on their lipid status, patients may benefit from novel treatment regimens combining ECP with lipid modulators. This could be used for the prevention of transplant organ rejection, the treatment of acute or chronic GvHD or transplant organ rejection and the long-term treatment of various skin diseases. This study uncovers novel mechanisms of ECP, that can be used to establish clinically relevant lipid profiles of patients to support patient stratification, predictive or prognostic purposes and thus personalized medical care in the framework of PPPM practice. A combination with S1P modulators may therefore have beneficial effects.","10.1007/s00011-025-02007-6","Photopheresis; ECP; UVA; Lipidomics; Proteomics; Molecules sphingosine-1-phosphate; PPPM; Clinically relevant lipid profiles; Individual lipidomics; New therapeutic concepts; Inflammation","","http://link.springer.com/openurl/pdf?id=doi:10.1007/s00011-025-02007-6"
"Explainable Cubic Attention-Based Autoencoder for Skin Cancer Classification","Nasir, Inzamam Mashood; Tehsin, Sara; Damaševičius, Robertas; Zielonka, Adam; Woźniak, Marcin","2025","1","1","0","0","0","Unique","0","","","","","","","In this article, Explainable Cubic Attention-based Autoencoder (ECAbA) model is proposed for skin cancer classification. The proposed model has a data augmentation module, which transform an imbalanced HAM10000 dataset into a refined and balanced form. In the second step, Cubic Attention-based Autoencoder (CAbA) model is proposed, which has 5 3D convolutional layers, 2 attention modules and one flatten, dense and neural successive cancellation layers. The trained model is then used for three explainable models, i.e., LIME, Grad-CAM, and Kernel SHAP, to make the explanations of predicted images. The outputs of these explainer modules make the predictions of CAbA model explainable and reliable. For fostering confidence among medical practitioners, the proposed model can assist in the adoption of AI approaches.","10.1007/978-3-031-81596-6_11","Skin Cancer; Explainable AI; Attention Model","","http://link.springer.com/openurl/pdf?id=doi:10.1007/978-3-031-81596-6_11"
"D-LEMA: Deep Learning Ensembles from Multiple Annotations -- Application to Skin Lesion Segmentation","Zahra Mirikharaji, Kumar Abhishek, Saeed Izadi, Ghassan Hamarneh","2020","1","0","0","0","0","Unique","0","Accepted","An ensemble of Bayesian fully convolutional networks (FCNs)","handle annotators' disagreements when training a deep model for medical image segmentation.","ISIC Archive, PH2, and DermoFit","The approach considers handling contradictory annotations and improving confidence calibration through the fusion of base models' predictions.","The abstract states the approach demonstrates ""superior performance"" on the ISIC Archive and explores generalization on the other datasets.","Medical image segmentation annotations suffer from inter- and intra-observer variations even among experts due to intrinsic differences in human annotators and ambiguous boundaries. Leveraging a collection of annotators' opinions for an image is an interesting way of estimating a gold standard. Although training deep models in a supervised setting with a single annotation per image has been extensively studied, generalizing their training to work with datasets containing multiple annotations per image remains a fairly unexplored problem. In this paper, we propose an approach to handle annotators' disagreements when training a deep model. To this end, we propose an ensemble of Bayesian fully convolutional networks (FCNs) for the segmentation task by considering two major factors in the aggregation of multiple ground truth annotations: (1) handling contradictory annotations in the training data originating from inter-annotator disagreements and (2) improving confidence calibration through the fusion of base models' predictions. We demonstrate the superior performance of our approach on the ISIC Archive and explore the generalization performance of our proposed method by cross-dataset evaluation on the PH2 and DermoFit datasets.","https://doi.org/10.48550/arXiv.2012.07206","","","https://arxiv.org/abs/2012.07206"
